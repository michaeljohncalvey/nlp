---
aliases: ["Life"]
---
# 000 - Life MOC
#MOC 
### [[My Motto]]: "Quam Et Quare?"
Life is perceived [[000 Reality MOC|Reality]]. This page is my [[Narrative Instinct|Narrative Self]]'s attempt to categorize and organize life. It acts as an aid to and influence on my [[Remembering Self]]. 

Life is fundamentally the combination of [[020 Timeline MOC|History]] and the [[Future]]. [[010 Me MOC|I]] am the present.

Life is in a way [[God]].

## WIP
[[Technology]]

## Index Categories
- 000s - [[000 Reality MOC|Reality]], [[Existence]] and [[Time]]
	- [[000 Life MOC | Life]] | [[001 Meta MOC|Meta]] | [[020 Timeline MOC|Timeline]] | [[003 Process MOC|Process]] | [[005 Active MOC|Active]] | [[006 School MOC|School]] | [[007 Work MOC|Work]]
- 010s - [[010 Me MOC|Me]] and my [[010 Mind MOC | Mind]]
	- [[014 Habits MOC]]
	-  [[015 Goals MOC|Goals]]
	-  [[011 Mental Models MOC|Mental Models]]
- 020s - [[020 Timeline MOC|Timeline]]
- 030s - [[030 People MOC|People]] | [[035 Places MOC|Places]]
- **040s - [[040 Interests MOC|Interests]]**
- **050s - [[050 Concepts MOC|Concepts]]**
	- [[Quantum Mechanics TOC|Quantum Mechanics]]
	- [[Machine Learning TOC|Machine Learning]]
	- [[056 Psychology TOC|Psychology]] | [[054 Neuroscience MOC|Neuroscience]]
	- [[Linguistics MOC|Linguistics]]
- 060s - Concepts contd
- 070s - [[070 Finance MOC|Finance]] | [[075 Quantitative Finance MOC|Quant Finance]] -|- [[071 XCAP MOC|XCAP]] | [[076 Algo MOC|ALGO]]
- 080s - [[080 Personal Finance MOC | Personal Finance]]
- 090s - [[090 Lists MOC|Lists]] | [[095 Journals MOC|Journals]]
- 100s - [[100 Ideas MOC|Ideas]] | [[100 Projects MOC|Projects]]
- 200s - [[200 Outputs MOC|Outputs]]
- 300s - [[300 Inputs MOC|Inputs]]

## Index Keywords
- 000s - #MOC
- 010s - #my/mind #concepts #concepts/mental_models
- 020s - #my/body #my/mind
- 030s - #people #places 
- 040s - #outputs/code #inputs/movies
- 050s - #concepts


This page should be a little more personal. Neat and tidy is for losers. Honestly this whole project should be a little more personal. These are all just notes for me. These notes are a [[001 Meta MOC]] realization and analysis of my life. My attempt to categorize my thoughts.

Honestly this note should be way longer. I should think about what life is. Break it down. Truly use these notes as a way to understand [[000 Reality MOC|Reality]].

[[Heuristics and Biases]] are partially responsible for the difference between life and reality, as well as [[Complexity]].

- [[Time]]
- [[Existence]]

Technology is another important category of life I haven't really delved into.

My main [[Map of Content]]

* How to style reference sheets for quicker, intuitive use: [[Layout Doc]] [[Style Guide]]


[[Notes to make]]---
aliases: ["Reality"]
---
[[000 Life MOC|Life]]
# 000 - Reality MOC
#🌲 

Reality is the most basic and fundamental level of being. This is the first reference point for everything. It is the most intrinsic of the [[Orders of Reality]]. [[Existence]] is when our [[010 Mind MOC|Mind]] places us within reality. The relation of the internal to the external, which is a crazy thing.

Existence is matter. Existence is [[Forces]].
The fundamental explanation for this level of being lies in [[Quantum Mechanics TOC|Quantum Mechanics]].

Existence is almost too incomprehensible for people through the nature of our [[054 Biology MOC|Biology]], so we instead turn to [[000 Life MOC|Life]] for a lens through which to percieve reality. To us, reality is subjective, which is impossible to truly understand.

[[Time]] is a critical aspect of existence I haven't thought about nearly enough.

---
aliases: ["Meta"]
---
[[000 Life MOC]] | [[010 Mind MOC]] | [[050 Concepts MOC]]
# 001 - Meta MOC
#outputs/thoughts #MOC

[[2021-03-22]]
Being able to think about things on a meta level provides unparalleled power. This is an application of [[Abstraction]] to [[000 Life MOC]]. I need to think about this concept more. It's tough to write sometimes, but as with [[000 Life MOC|Life]], I need to make this more personal. Meta is my way of exploring existence on a higher level, and organizing as well as categorizing it.

Much of this originally arose from my thoughts in [[Thoughts on note taking]]

I should compile some quick thoughts here on link-based note taking. It'd be interesting to revisit in a little while, and see if the ideas hold as strongly as they do now.

Meta is really a higher [[Orders of Reality]]. This is like accessing a higher order of [[Ontology]].r4

[[003 Process MOC]]
[[Concept Notes]]

[[My Forest]]
[[Evergreen Notes]]:
* [[Seedling Notes]]
* [[Bridge Notes]]
* [[Table of Content|TOC]]
![[Table of Content#TOC Definition]]
* [[Map of Content|MOC]]
![[Map of Content#MOC Definition]]
* [[Timeline]]
* [[003 Process MOC]]

**Useful searches for graphs:**
* `tag:#MOC OR tag:#TOC OR tag:#concepts OR tag:#class OR tag:#bridge OR tag:#todo`
* `tag:#MOC OR tag:#TOC OR tag:#outputs/log OR tag:#outputs/journal`
* `tag:#MOC OR tag:#TOC OR file:TOC OR tag:#outputs/log OR file:Todo OR tag:#concepts OR tag:#todo OR tag:#🌱 OR tag:#docs OR tag:#projects/algo OR file:Docs OR tag:#class OR tag:#bridge OR tag:#projects/old/xcap`
[[000 Life MOC]] | [[005 Active MOC]]
# 003 - Process MOC
#MOC 

This will be the home of my tag-based workflow system.

I think I'm going to base it around my [[Evergreen Notes]] system. That would probably be the post intuitive and help build the metaphor.

## Process
This is the approximate process notes should go through. 
* #🌱  [[Seedling Notes]]
* #🌲  [[Evergreen Notes]]

[[000 Life MOC]]
# 005 - Active MOC
#school #MOC
#🌲

# Projects
[[My Tag System]] - I need to rethink my tag system! It's way too messy rn. I think more nested tags would be a great idea. Start from the top and eliminate.

## People
[[030 People MOC]]

## Timeline
[[020 Timeline MOC|Timeline]]

## Mind
[[010 Mind MOC]]

## XCAP
[[071 XCAP MOC]]

## Algo
[[076 Algo MOC]]

## Mental Models
[[011 Mental Models MOC]]

## Language
[[Linguistics MOC]]


## Stubs
![[Stubs]]

## Major Refactoring
#todo 
* [x] [[Workflow]]
* [ ] 
* I need to think consciously about my workflow, from the initial moments of knowledge seeking through knowledge consumption, digestion and creation. I should link up the process, track things I read, and begin compiling. Maybe I could transcribe my notes in the little reading book?
* [Zotero](https://www.youtube.com/watch?v=XbGJH08ZfCs)? Zotflow? Maybe not worth it for my use case. Don't overdo it!


# Work
[[007 Work MOC]]
[[072 MS TOC]]


# Other important
- [[Notes to make]][[000 Life MOC]] | [[005 Active MOC]]
# 006 - School MOC
#MOC



## Current Classes
### [[RELI254 Islam in South Asia]]
#todo 
- [x] Interpretation Paper!
- [x] FORMAL RESPOS!!!!! 
- [x] INFORMAL RESPOS!!!!

### [[CS451 Machine Learning]]
#todo 
- [x] [[CS451 Project Feature Analysis]]

### [[CS333 Quantum Computing]]
#todo 
- [x] PS8: APR 26

### [[PHYS106 Physics for Educated Citizens]]
#todo 
- [x] Assignment 11a [[2021-05-07]][[000 Life MOC]][[005 Active MOC]]
# 007 - Work MOC
#MOC

[[MS Getting Started]]

[[MS Recruiting]] [[MS Recruiting Efforts]]---
aliases: ["Me", "I"]
---
[[000 Reality MOC]] | [[000 Life MOC]]
# 010 - Me MOC
#MOC #🌲 

### Mission: "Quam Et Quare?"
### "An Unexamined Life Is Not Worth Living"

[[010 Mind MOC|Mind]]

Me. This is one of the [[Orders of Reality]]. It goes below [[000 Life MOC|Life]] 

My fundamental quality is [[Self Awareness]], and the ability 

This could be a good place for me to flesh out my identity.

There are a bunch of parts to me. My body as a physical entity exists, and my mind is aware of it. That is the fundamental basis of bein, which is what this order of reality really entails.

[[010 Mind MOC|Mind]] [[011 Mental Models MOC|Mental Models]]

[[Time]] and me are also inextricably linked. I have a [[020 Timeline MOC|History]], as well as a [[Future]].


## Ideology
I'd like to start trying to pin down my beliefs. What is my ideology? What is my perspective on [[Metaphysics]]? What is [[000 Reality MOC|Reality]], and how do I fit in? How do humans generally fit in?

Politically speaking, I'd probably most identify as a [[Classical Liberalism|Classical Liberalist]].

Philosophically, I think I agree with the [[Humanism]] outlook, although I should learn more about it before I'm sure.---
aliases: ["Mind"]
---
[[000 Reality MOC]] | [[000 Life MOC]] | [[010 Me MOC]]
# 010 - Mind MOC
#MOC #🌲 
My mind is the single most important element of [[Me]]. This note is my attempt to start unravelling it as best I can, so I can go [[001 Meta MOC|Meta]] in my approach to [[000 Reality MOC|Reality]].

Our mind's purpose is to bring together our [[Mental Systems]], which are actually emergent properties of [[Neurons|Neural]] behaviour. 

The mind is a crazy thing. I've thought lots about it, but haven't really put my thoughts into concrete terms. I think it's about time I started doing that. This page will try to link my ideas together, and hopefully begin to make some interesting connections.

I need to do way more work documenting and thinking about my mind. I haven't even scratched the surface. This page so far is a joke and a disgrace to the existence of consciousness. The pure and unparalleled advantage of consciousness is the ability to access the [[001 Meta MOC]]
- [[Consciousness]] [[Thought]] [[Linguistics MOC]]
	- [[Meditation]]
- [[Existence]] [[000 Reality MOC|Reality]]


## Organizational Strategies
[[001 Meta MOC]]
[[My Forest]]

## Learning
[[Levels of Reading]]
[[Einstein Filtering Method]]
[[Feynman Learning Method]]


## Decision Making
[[Causes of Stupidity]]
[[011 Mental Models MOC]]
[[Decision Machines]]---
aliases: ["Mental Models"]
---
[[000 Life MOC]] | [[010 Mind MOC]] | [[My Forest]]
# 011 - Mental Models MOC
#🌲 #MOC #concepts/mental_models 

THIS NOTE IS TOO LONG!! I need to split things up in some very logical way and distribute this content out so you can easily navigate to what you need logically.

Actually I disagree. I should let this note grow and evolve as much as it needs to. There need not be a limit on individual complexity if notes are to remain strictly topical. Also because of my [[Narrative Self]]'s desire to personalize these notes a bit, I'm keeping the above and new thoughts.

Check out [[Heuristics and Biases]] for common pitfalls, sort of like flawed mental models people are predisposed to using.

## Understanding
[[Abstraction]]
[[Linguistics MOC]] <- lots more thought needed here.

## General
[[Circle of Competence]]


---

## Biology
[[054 Biology MOC]]
**Note:** These tend to be the root causes of all of the other non-abstract models.
- [[Biologically Innate Human Tendencies]] This should be a higher-order reference linking many of the topics on this page to human biochemistry. Furthermore, much of this should be explainable by [[Evolution]], since humans evolved key social, emotional, intellectual and physical traits to help us survive as a species.
	- [[Biological Determinism]]!!!!
- [[Evolution]]
- [[Associative Machine]]
- [[Ecosystems]]
- [[Niches]]
- [[Self Preservation]]
- [[Self Awareness]]
- [[Replication]]
- [[Cooperation]]
- [[Hierarchical Organization]] - most complex biological systems have an innate feel for how they should organize. This goes for humans as well as other animals. Link to [[Stanley Milgram Experiments]].
- [[Embodied Cognition]] - finite system 2 willpower, glucose, mood, emotion/ 
- [[Tendency to Minimize Energy Output]] - mental & physical, [[Energy Saving Brain]]
- [[Operant Conditioning]], consequences, risk/ reward, pavlov
- [[Dunbar's Number]]

---

## Human Nature and Judgement
**Note:** Need to separate out biases/ common predictable judgement error.
- [[Cognitive Biases TOC]]: [[Heuristics and Biases]]
	- [[Recency Bias]]
	- [[Affect Heuristic]] - Emotion (affect) plays a critical role in active decision making
	- [[Availability Heuristic]]
	- [[First Conclusion Bias]] - Munger's idea, sperm-egg but for ideas.
	- [[Survivorship Bias]] - difficulty of collecting past evidence, hedge fund performance, historiography see also [[Selection Bias]]
	- [[Confirmation Bias]]
	- [[Substitution Bias]] - answering an easier question in place of a harder one
	- Hindsight Bias - Narrative instinct, law of small numbers, stereotyping.
	- [[Overconfidence Bias]] - lotteries, endowmend ('mine')
	- [[Overoptimism Bias]]- ignoring base rates! Newlyweds.
	- [[WYSIATI Bias]] - What You See Is All There Is
	- [[Representativeness Bias]] - Linda problem. The tendency to judge the likelihood or frequency of an event by how much it fits 
		- [[Tendency to Stereotype]], [[Energy Saving Brain]]
		- Failure to See [[False Conjunctions]]
		- Failure to account for [[Base Rates]]
		- [[Stereotypes]].
	- [[Fundamental Attribution Bias]]: tendency to overestimate consistency of behavior for others.
	- [[Framing]] - order of homes shown by realtor, Loss vs gain (ties in with [[Loss Aversion]])
	- [[Priming]] and [[Anchoring]]
	- [[Cognitive Ease]] - more familar = better, goes for choices as well as understanding of the world.
- [[Human Instincts]]: Instincts fundamentally influence what we want and do, and are a combination of biological [[Evolution]] and social pressures
	- [[Narrative Instinct]] - preference for causality, stories.
	- [[Curiosity Instinct]]
	- [[Language Instinct]]
	- [[Survival Instinct]]
	- [[Inherent Laziness Instinct]]
- [[People Overestimate What They Could Do In A Day, And Underestimate What They Could Do In A Year]]
- [[Social Proof Theory]] (safety in numbers)
- [[Law of Small Numbers]] - overconfidence in predictions from small sample sizes 
- [[Influence of Stress]] - stress amplifies the impacts of cognitive biases.
- [[Cognitive Switching Penalty]]
	- [[Flow]]
- [[Dunning-Kruger Effect]] - don't know what you don't know, competence levels
- [[Man with Hammer]] - could just be priming
- [[Pygmalion Effect]] - expectations influence performance
- [[Halo Effect]] - online movie reviews before watching, order of essays graded. [[Priming]] and [[Anchoring]]
- [[Systems 1 and 2]] - experiencing vs remembering self. Finish thinking fast and slow!
- [[Negativity Dominance]] - loss -> fight harder
- [[Curse of Knowledge]] - assuming others have the same knowledge when communicating. Happens to experts.
- [[Attribution Error]] - fooled by randomness, me: circumstances/ skill, them: luck. Correlation vs causation
- [[Occam's Razor]] - simpler explanations are more likely to be true than more complicated ones.
- [[Hanlon's Razor]] - we should not attribute to malice what can more easily be explained by stupidity.
- [[Story-based Understanding]]
- [[Jumping to Conclusions]]

---

## Problem Solving
- [[Inversion]]
- [[Goal Orientation]] win, that is all that matters.
- [[First Principles Thinking]]
- [[Thought Experiment]]
- [[Second Order Thinking]]
- [[Probabilistic Thinking]]

---

## Systems
- [[Feedback Loops]]
- [[Equilibrium]]
- [[Bottlenecks]]
- [[Scale]]
- [[Margin of Safety]]
- [[Churn]] - standing still is losing, constant growth needed
- [[Algorithms TOC]] - sets of rules to solve problems
- [[Emergence]] - non-linear results from combining simple components
- [[Irreducibility]] - most systems contain irreducible quantitative properties. Typically complexity, minimums, time and length. EG multiple women can't have a baby quicker
- [[Diminishing Returns to Scale]]
- [[Meehl Patterns]] - In low validity environments, experts tend to be overconfident in their predictions and try to be clever or unexpected. This leads them to underperform simple statistical models in predicting outcomes ahead of time. This links strongly with a bunch fo the other biases mentioned above including [[Overconfidence Bias]], [[Base Rates]] and [[Reversion to the Mean]]. http://psychtutor.weebly.com/blog/meehl-patterns-predicted-grades-and-oxbridge-interviews
- [[Tight Coupling]] - dependent outcomes, components.
- [[Feedback Loops]]
- [[Equilibrium]]
- [[Complex Adaptive Systems]] - [[Systems Theory TOC]], Emergent Properties/ [[Emergence]]
- [[Collective Intelligence]] - crowdsourcing, diversity, [[Expected Value]]
- [[Higher-Order Consequences]] - peltzman/ seatbelts

---

## Numeracy
- [[Distributions]] - this one is super interesting, I should definitely write down my past musings.
- [[Compounding]] - exponential effects, multiplication vs addition
- [[Sampling]]
- [[Randomness]]
- [[Regression to the Mean]]/ [[Expected Value]]
- [[Multiplying by Zero]]
- [[Equivalence]] - using algebra to show that two seemingly different things are the same.
- [[Surface Area to Volume Ratio]]
- [[Global vs Local Maxima]]
- [[Power Laws]] - 80/20 rule
- [[Long Tail]]
- [[Law of Large Numbers]] ([[Expected Value]])
- [[Base Rates]]
- [[Mathew Effect]] - Small initial advantage snowballs
- [[Linear Algebra TOC]]
- [[Combinatorial Explosion]]

---

## Physics
- [[Relativity]]
- [[Reciprocity]] - [[Newton’s Laws]]
- [[Thermodynamics]] - energy conservation in a closed system
- [[Inertia]] - [[Newton’s Laws]]
- [[Friction and Viscosity]]
- [[Leverage]] - Archimedes: "give me a lever long enough and I shall move the world"
- [[Activation Energy]] - Tipping points, critical mass
- [[Catalysts]] - for chemical reactions but also [[Social Systems]]
- [[Chaos Theory]]
- [[Critical Mass]] - [[Radioactivity]], tipping points, [[Bubbles]]

---

## Social Beings
- [[Law of Vocal Minority]]
- [[Status Quo Bias]]
- [[Authority]] - [[Stanley Milgram Experiments]], nurse/ doctor
- [[Clanning]] - Us vs them
- [[Diffusion of Responsibility]]
- [[Social Proof Theory]] - when in ambiguous situations, people tend to copy the actions of those around them under the assumption that they possess more information about the situation. This is also tied to [[Herd Behavior]], a manifestation of [[Social Influence]] and/ or [[Conformity]]. Also tied to [[Status Quo Bias]]
- [[Group Polarization]] - a group will tend to make riskier or more extreme decisions than the average initial inclination of members.
- [[Gresham's Law]] - bad drives out good (money, behavior)
- [[Satisfaction and Misery are Relative]]
- [[Prisoner's Dilemma]]

---

## Economics
* [[Nash Equilibrium]]
* [[Keynesian Beauty Contest]]
* [[Prospect Theory]] [[Utility Theory]]
* [[Greater Fool Theory]]
* [[Bubbles]]
* [[Competitive Advantage]]
* [[Incentives]]
* [[Seizing the Middle]] - like in chess, get early advantage to snowball. This is tantamount to the [[Matthew Effect]].
* [[Bribery]]/ [[Bureaucratic Emergent Systems]]
* [[Arbitrage]]
* [[Supply and Demand]]
* [[Scarcity]]
* [[Mr. Market]]
* [[Paradox of Choice]] - maximizers vs satisficers
* [[Loss Aversion]]
- [[Sunk Costs Fallacy]]- gamblers fallacy -> [[Loss Aversion]]
- [[Time Value of Money]]
- [[Advantages of Scale vs Specialization]]
- [[Creative Destruction]]
- [[Opportunity Cost]]
- [[Moral Hazard]]
- [[Tragedy of the Commons]]
- [[Diminishing Returns]]
- [[Constraints]] - scarcity of resources, optimisation

---

## Military
- [[Seeing the Front]] - personally seeing the front before making decisions. This is [[The Map is not the Territory]].
- [[Asymmetric Warfare]] - Niches, guerilla warfare
- [[Two-Front War]] - Germany WW1/2 cockups, 
- [[Mutually Assured Destruction]]
[[000 Life MOC]] | [[015 Goals MOC]]
# 014 - Habits MOC
#MOC 

I need to start using a more [[James Clear]] - like habit tracking system. I want to make my use of time more intentional so I can get more out of each day I have remaining on earth. [[000 Life MOC]] | [[040 Interests MOC]]
# 015 - Goals MOC
#MOC 


## Health
### Food
- I really need to start eating way better. More consistently and with balance.
	- Honestly I've been getting better at this!
- Eat enough

 
## Personal Finance

## Family

## Personal Development
- Read every day for minimum of 15 minutes
- Chess?!
- Reading Reading Reading

### Skills I want to hone:
- Billiards - play 100 games
	- Done 8
- Poker - Play weekly, read strategy
- Chess - Play one game weekly

## Work
I want to be the best analyst the team has ever seen. I want to impress everyone so much with my drive and ambition, that people speak of me with awe.---
aliases: ["History", "Timeline"]
---
[[000 Life MOC]] | [[005 Active MOC]] | [[015 Goals MOC]]
# 020 - Timeline MOC
#MOC #🌲 

I would like to begin to categorize things with dates when I create notes relating to historical moments, discoveries or times. It would be a great way for me to internalize history. Perhaps I could one afternoon lay out eras, key empires and even link to my rather unused [[030 People MOC]].

#goal
I'd like to research one "event" per week, and place it within this timeline. I should also endeavor to place things I read in time too. 

## My Life 1998 - Present

## Modernity 1900-1998
[[1930s]]

## Antiquity

## Pre-History
* [[Paleolithic Age]] - early stone age, hunter gatherers
* [[Neolithic Age]] - late stone age, early human settlements, basic agriculture.

## Ancient History
[[Bronze Age]]
[[Iron Age]]

## Classical History
---
aliases: ["People"]
---
[[000 Life MOC]]
# 030 People MOC
#MOC

# I've Met
## Friends
[[Tommy Tarantino]]
[[Omar Alsaeed]]
[[Sergio Bobadilla]]
[[Andrew Gleason]]

## Colleagues
[[Aleksey Shevchenko]]
[[Robert Rubin]]

# I Haven't Met

## CS/ [[AI]]/ [[054 Neuroscience MOC]]
[[Geoffrey Hinton]]

## Economics
[[Friedrich Hayek]]
[[John Locke]]
[[Thomas Malthus]]
[[Adam Smith]]
[[David Ricardo]]
[[John Maynard Keynes]]
[[Edmund Burke]]
[[David Hume]]

## Science
[[Daniel Kahneman]]
[[James Clerk Maxwell]]
[[Isaac Newton]]
[[Paul Slovic]]
[[Amos Tversky]]
[[Richard Thaler]]
[[Charlie Munger]]
[[Jack Knetsch]]
[[Cass Sunstein]]
[[Seymour Epstein]]

## Linguistics
- [[Steven Pinker]]
- [[Ferdinand de Saussure]]
- [[Noam Chomsky]]

## Writers 
[[Mark Twain]]

## [[061 Philosophy TOC|Philosophy]] 
[[Aristotle]]
[[Socrates]]
[[Plato]][[000 Life MOC]]
# 035 Places MOC
#MOC[[XCAP Meeting Minutes]]
# 04/24 Meeting Minutes


## Plan
 - 2 new investments 
 - Discuss Akhil’s position
 - Analyst sectors
	 - Fintech/ Medtech
		 - Akhil 
		 - Xav
		 - (Michael)
	 - Industrials/ energy
		 - Sergio
		 - Daniel
	 - Gaming/ tech/ automation (Future of tech)
		 - Wiatt
		 - Carl
	 - Fixed Income
		 - Guo
		 - Michael

- Research criteria for analysts
	-  

## Plan for the Month
- Proposal structure
- Analyst structure
$2000 invested bpefore the end of semester
- Commitments to investment board

<!-- {BearID:9110514E-E4D7-4FBF-AD9E-C92638F82F2F-21279-0000B7BD00213393} -->
[[000 Life MOC]][[005 Active MOC]]
# Interests MOC
#MOC #todo 

#### [[Todo]]
* Organize this area!
* Add a bunch of categories, make things explicit.


## Media
[[Playlists TOC]]
[[Movie list]]

## Fun
[[Drinking Games]]

## Projects
[[076 Algo MOC]]
# 05/01 Meeting Minutes
<!-- #finance/xcap/meetings -->

##  
-

<!-- {BearID:6894717D-51EC-44BF-B469-C74D7784F9DC-21279-00014065EB685071} -->
[[000 Life MOC]]
# 050 - Concepts MOC
#concepts #MOC


* Reorganize this page to make it neater and more readable. I need a better way to break up/ present these categories, and explain perhaps in a moment what each refers to, if possible

This page should be closer to my original Reference Homepage design. I'd like to include links to sub-MOCs (or TOCs?) that correspond to disciplines.

This needs a dense reorganization - 

**Meta**
[[001 Meta MOC]]
[[011 Mental Models MOC]]

**Math**
[[051 Math TOC]]
[[Statistics TOC]]

**Physics**
[[052 Physics TOC]]
[[Quantum Mechanics TOC]]

**Computer Science**
[[053 Computer Science TOC]]
[[Machine Learning TOC|Machine Learning]]

**Biology**
[[054 Biology MOC]]
[[054 Neuroscience MOC]]

**Psychology**
[[056 Psychology TOC]]

**History**
[[060 History TOC]]

**Philosophy**
[[061 Philosophy TOC]]

**Religion**
[[062 Religion TOC]]

**Economics**
[[063 Economics TOC]]

[[064 Politics TOC]]
[[050 Concepts MOC]] [[051 STEM TOC]]
# 05 - Math TOC
#TOC #concepts 
* Math is an incredible language I can use to approach problems. I would almost say it is a [[Frames of Reference]] I can use to look at and rationalize the world around me. To an extent, I might even be able to argue Math is a [[Cosmogeny]]

#todo 
- [x] Create atomic files:
	- [x] [[Matrices]]
	- [x] [[Lines]]
	- [x] [[Hyperplanes]]
	- [x] [[Vector Spaces]]
	- [x] [[Dot Product]]
	- [x] [[Trigonometric Identities]]


[[Tensor Products]]
[[Complex Numbers]]
[[Linear Algebra TOC]]
[[Linear Regression]]
[[Statistics TOC]]
[[Graph Theory]]
	
* Many of the topics in math are beginning to have huge amounts of overlap at the levels I've been learning lately. [[]]
* [[050 Concepts MOC]]
# 051 - STEM TOC
#TOC

[[052 Physics TOC]]
[[Quantum Mechanics TOC]]
[[051 Math TOC]]
[[053 Computer Science TOC]]
[[055 Coding TOC]]
[[Statistics TOC]]
[[Linear Algebra TOC]]

[[Machine Learning TOC|Machine Learning]]
[[050 Concepts MOC]]
# 052 - Physics TOC
#TOC #concepts 


* Lots of thinking about how the world works here. I need to honestly ponder and ask questions more often. Had a little session exploring elements of [[Quantum Mechanics TOC]] such as  and the resulting .  See also [[Qubits]][[Quantum Measurement]]. For a list of my questions and musings, see [[Quantum Questions]] [[Quantum Cryptography]]
* Some thinking about [[Quantum Computing TOC]]: [[Quantum Gates]] [[Qubits]] [[Quantum States]] [[Quantum Measurement]] [[Quantum Bomb Game]] as well as a compendium of questions: [[Quantum Questions]]
* Some work on [[Quantum Mechanics TOC]]: [[Bell’s Theorem]] and resulting [[Interpretations of Quantum Mechanics]] like [[Copenhagen Interpretation of Quantum Mechanics]] and [[Superdeterminism]] [[Quantum Algebra]] 
* [[Laws of Thermodynamics]]
* [[Classical Mechanics]] [[Newton’s Laws]]
* [[Standard Model TOC]]

* [[Energy]]
	* [[Electric Charge]]
	* [[Electric Current]]
	* [[Electric Power]]
	* [[Electric Resistance]]
* [[Light]]
* [[Waves]] | [[Huygens Principle]]

* [[Aerospace]]

* [[Radiation]] and [[Radioactivity]]
* [[Atoms]], [[Neutrons]], [[Protons]], [[Electrons]], [[Quarks]]
* [[Nuclear Fusion]] [[Nuclear Fission]]

* [[Electrostatic Force]] [[Magnetism]] [[Electromagnetism]]
* [[Gravity]]


[[PHYS106 Physics for Educated Citizens]][[050 Concepts MOC]] [[051 STEM TOC]]
# 055 - Computer Science TOC
#TOC #concepts 
Lots of thinking about computational models and approaches. Simplistic ones only so far, although abstract enough to be able to solve all simple problems in a systematic way. I think fundamentally computation-based approaches are flawed, and I need a different model for thinking about cognition if I want to really rationalize it

* [[Algorithms TOC]]
* [[AI]]
* [[Chomsky Hierarchy TOC]]
* [[Turing Machines TOC]]
* [[Decidability TOC]]
* [[Machine Learning TOC]]
	* [[Binary Classification]][[Multi-Class Classification]][[Linear Regression]]
* [[Information Theory]]
* [[Cryptography]]
* [[Quantum Mechanics TOC]] [[Qubits]] 
	* [[Quantum Cryptography]]
* [[Logic Gates]] For a quick summary of how they work

	
[[Random Walks]]---
aliases: ["Biology"]
---
[[050 Concepts MOC]]
# 053 - Biology TOC
#concepts 

[[Neurons]]
[[054 Neuroscience MOC]]---
aliases: ["Neuroscience"]
---
[[010 Mind MOC]] | [[054 Biology MOC]] | [[050 Concepts MOC]]
# 054 - Neuroscience MOC
#concepts 
#🌲 

Biological brains are modular, with distinct but interacting subsystems underpinning key functions like memory, language and cognitive control. 

# Areas of interest
I need to delve much deeper into neuroscience. This should grow into an [[Map of Content|MOC]], much like that currently in effect for [[Machine Learning TOC|ML]] or [[Quantum Computing TOC|Quantum Computing]]. Also those both definitely need to be recategorized as MOCs, they aren't bridges or TOCs lol. Stoopid. 

Each of these probably deserves it's own TOC/ Bridge. Might make sense to swap psych TOC with this page, giving this page number 056.

[[Cognitive Neuroscience]], [[Systems Neuroscience]], [[056 Psychology TOC]]


[[Neurons]]
[[Ego Depletion]]
[[Systems 1 and 2]]

## Overlap with [[AI]]
Very interesting connections between artificial and human intelligence. As [[Demis Hassabis]] argues in [[Hassabis - Neuroscience Inspired AI]], it pays to consider both fields in tandem.

## Computing
[[Computational Biology]][[050 Concepts MOC]][[050 Concepts MOC]]
# 060 - History TOC
#concepts #TOC 

* [[Mughal Empire]]---
aliases: ["Philosophy"]
---
[[050 Concepts MOC]]
# 061 - Philosophy TOC
#TOC #concepts 

The study of truth and virtue. Can become awefully abstract. I am primarily concerned with the more practical side of philosophy, especially as it relates to [[Cognitive Neuroscience]] and the relationship between reality and meaning. 
- Thinking about the statement above, I don't agree anymore. I definitely have a weird relationship with [[Metaphysics]]. On one hand, I dislike the overly abstract. On the other hand, I think the pursuit of universal truth is one of the key elements of life. This also encourages agreement with [[My Motto]], **How & Why**

## Concepts
- [[Classical Liberalism]]
- [[Individualism]]
- [[Collectivism]]
- [[Socialism]]
- [[Capitalism]]


## Modern Philosophy
[[Friedrich Hayek]]

## [[Greek Philosophy]]
* [[Aristotle]]
* [[Socrates]]
* [[Plato]]

## Further Reading
* Starting points for further reading:
		* [[Plato]] - The apology
		* Plato - The republic
		* Plato meno - on memory/ epistomology
		* Plato - Phaedo 
		* The symposium
		* Aristotle - ethics, politics, metaphysics
		* Bloom - history of western philosophy
[[Humanism]]
[[Classical Liberalism]][[050 Concepts MOC]]
# 062 - Religion TOC
#TOC #concepts 

I would almost argue at this point that religion is the combination of [[Metaphysics]], [[061 Philosophy TOC|Philosophy]] and [[Faith]]

* This one is a major WIP, although I guess they all are. Definitely going to start getting into some spiritual concepts when I visit the idea of free will. 
* See [[God]].  Need to spend a lot more time thinking about this.

## Religions
* [[Islam]]
* [[Christianity]]

## Areligiosity
[[Secularism]] [[050 Concepts MOC]]
# 063 - Economics TOC
#TOC

## Classes at School
[[ECON411 Applied Econometrics]]
[[ECON211 Regression]]
[[ECON431 Economics of the EU]]

## Topics
[[Classical Liberalism]][[050 Concepts MOC]]
# 064 - Politics TOC
#TOC #concepts 

## Political [[061 Philosophy TOC|Philosophy]]
[[Classical Liberalism]][[000 Life MOC]] | [[005 Active MOC]]
# 070 Finance MOC
#MOC

This is an area for organizing all of my work and thoughts on the rather broad area of finance. 

## [[075 Quantitative Finance MOC]]
- [[076 Algo MOC]]


### Strategies
- [[Cannabis Arbitrage]]


### Projects
- [[072 MS TOC]]
- [[071 XCAP MOC]]
- [[076 Algo MOC]]

### Research
- [[Company Research]]

### Reading
- [[Robert Carver - Systematic Trading]]

### Learning
- [[Options MOC]]
	- [[Options Strategies]][[000 Life MOC]] [[005 Active MOC]] [[070 Finance MOC]] [[100 Projects MOC]]
# 071 - XCAP MOC
#MOC #projects/old/xcap #projects/old/xcap
#concepts/finance

[[076 Algo MOC]]
[[XCAP Branding]] 
[[XCAP Writing]]
[[XCAP Strategies]]
[[XCAP Watchlist]]

She's all tied up now. I suppose this page is mostly retired, although it was only ever for my benefit anyway. Might make sense recording my thoughts and notes here regardless of xcap being done. Let's see.


## Closing letters
[[Closing Letter Template]]

[[Akhil Closing Letter]]
[[Carl Closing Letter]]
[[Mohling Closing Letter]]
[[Bobadilla Closing Letter]]
[[Hinton Closing Letter]]
[[Manekia Closing Letter]]

## Annual Report
[[XCAP 2020 Annual Report]]
- Fund returned 74.5% for the year
- Ended with 10.5% cash
- Components:
	- Overall letter - ME
	- Meet the team - DAN
	- Performance
		- NAV page - ME
			- Net contributions through the year
			- Monthly NAV/ cash graph
			- Monthly NAV change, comp w/ SPY, QQQ, RUT
		- Positions page - JOE
			- Tickers/ subsectors
			- Winners
		- Macro page - CARL
			- 
	- Strategies:
		- Cannabis - me [[XCAP Cannabis Strategy Review]]
			- TCNNF
			- CURLF
		- Cyclicals - me [[XCAP Cyclical Strategy Review]]
			- NMIH
			- VRRM
		- WFH Winners - dan [[XCAP WFH Strategy Review]]
			- ATVI
			- DIS
			- NFLX
		- Pandemic “scrap hunting” - me carl [[XCAP Scrap Hunting Strategy Review]]
			- XLE
			- UAL
			- DAL
		- Big tech - CARL [[XCAP Tech Strategy Review]]
			- MSFT
			- SNAP
			- AMZN
			- FB
			- NVDA
		- Financials - dan gelo [[XCAP Financial Strategy Review]]
			- JPM
			- MA
			- PYPL
		- China - dan gelo [[XCAP China Hedge Strategy Review]]
			- BABA
			- TCEHY
			- MCHI
			- CQQQ
			- CNYA
		- Short strategies - me joe [[XCAP Short Strategy Review]]
			- VXX
			- Puts
			- SH
	- XCAP.QUANT [[XCAP Quant Update]]


## 3Q Report
<!-- #finance/xcap/3q20 -->
[[XCAP 3Q20 Meeting]]
*NOTE: DEPLOYED CAPITAL SCREWED THINGS UP!!*


## Report Structure
	- My letter
	- Performance 1 pager
	- Positioning/ allocation review
	- Macro Strategy note
	- Cannabis & the election
	- Individual company reports

* Gotta first handle my deliverables:
	* TCNNF report [[XCAP 3Q20 TCNNF]]
	* CURLF report [[XCAP 3Q20 CURLF]]
	* NMIH report [[XCAP 3Q20 NMIH]]
	* Macro strategy note [[XCAP 3Q20 Macro Note]]
	* [[XCAP 3Q20 Cannabis & the election]]
	* Positioning/ allocation review
	* Fund performance w/ Carl
	* Create new full doc layout
	* Create individual report layout
[[005 Active MOC]] [[070 Finance MOC]] [[100 Projects MOC]]
# 072 - MS TOC
#TOC #work/ms
[[MS stuff]]
[[MS big meets]]
[[MS Recruiting]]
[[MS Recruiting Efforts]]
[[MSWM Capital Markets Goals]]
[[MS Internal Communications and Alignment Report]]
[[Interviewing MS]]
[[Morgan Stanley Notes]]
[[Recap of my Morgan Stanley experience]][[000 Life MOC]] | [[005 Active MOC]] | [[070 Finance MOC]]
# Quantitative Finance MOC
#MOC 
#### HEAD
This will be the launch page for all my future quant endeavors. Lots and lots of ideas, with tons of work required to test many of them. 

#### NAV
#concepts/finance


#### REFS
[[076 Algo MOC]]

[[Alphavantage]]

[[Robert Carver - Systematic Trading]]

[[Trading Signal]]

## Projects
* [[MA Cross Strategy]] - my very first project, essentially deploying a “trivial” moving average crossing strategy that buys into momentum and sells when momentum falls for [[TCNNF]]. Positive results. Would be interesting to think about restricting the universe to “hype” stocks. Also thinking about creating stricter sell rules, perhaps. 
* [[Cannabis Arbitrage]]- An interesting one I think might actually have some potential IRL involving [[TCNNF]] and [[CURLF]]
* [[Residential Mortgages]] - My second project. Here I’m looking at [[NMIH]]

## Ideas
* PEG investing
* Mean reversion strategy
* Volatility assumptions?
* EPS growth strategy
* Mkt share * profit margin * revs growth approach?
* Pair trading 
* Sentiment reversion
* [[Short Interest Theory]]

## Notes
* [[Robert Carver - Systematic Trading]] - Robert Carver notes
	* Fundamental (macro/ micro) strategies
	* Technical strategies - using price and volume data

## Research
* Adam Atkins, Mahesan Niranjan, & Enrico Gerding. (2018). Financial news predicts stock market volatility better than close price. /Journal of Finance and Data Science/, /4/(2), 120–137. https://doi.org/10.1016/j.jfds.2018.02.002
	* Interesting deal about using news to predict volatility, probably more effective than predicting price. Then just decide whether to sell/ buy options 
* [Predicting stock market movements using network science: an information the…: LibrarySearch Middlebury](http://eds.a.ebscohost.com/eds/detail/detail?vid=0&sid=b184097d-f536-4865-85fd-ea9babbce28d%40sessionmgr4008&bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#AN=edsdoj.677d29cad14b8a84f9dc948be6fd73&db=edsdoj)
	* Using network science to predict stock movements
* Wang, H., Lu, S., & Zhao, J. (2018). /Aggregating multiple types of complex data in stock market prediction: A model-independent framework/ http://eds.a.ebscohost.com/eds/detail/detail?vid=0&sid=fa8a2c71-791b-439a-a975-19ce96ff7f68%40sdc-v-sessmgr03&bdata=JnNpdGU9ZWRzLWxpdmUmc2NvcGU9c2l0ZQ%3d%3d#AN=edsarx.1805.05617&db=edsarx
	* Model-agnostic stock price prediction framework, very cool tbh---
aliases: ["Algo"]
---
[[000 Life MOC]] | [[071 XCAP MOC]]
# ALGO MOC
#MOC #projects/algo

## Description
The long-awaited trading system. The three goals of the project are to **provide intelligence** on *market structure and function*, **insight** into the veracity of claims about how to take advantage of this, and finally the ability to **execute particular strategies autonomously.** 

### Main Goal 1: Execution
Create a system able of making active and passive investments over a variety of timeframes. Focus on modularity so trading rules can be created independently of the system at large, then tested and implemented with ease
#### Subgoal 1: Live trading (paper & real)
#### Subgoal 2: Backtesting

### Main Goal 2: Intelligence
Develop a process and system for gaining market intelligence. I need a *research pipeline*. This doesn't need to be completely automated! In fact, I'd argue there are parts of the process that only suffer from automation. Rather, I should focus on the strengths of the automation process, and pair them with human strengths. The [[Algo CLI Docs|Algo CLI]] actually aimed to achieve this, just without solving a problem. This is the problem. With that in mind, I can be much more intentional about where I put my time. I think this could actually be a major point of focus in the project for me, and where the biggest value add to my current life as it stands may lie.

It may also be the most useful part for relating to my work. I'll need to see how/ if it can actually be applied. I need to be *very* careful to do everything by the book.

A big part of this is data processing, integrity and integration. This is actuallly probably where I should next direct my [[Attention]]
#### Subgoal 1: Clean and refine data pipeline
- This is gonna be the next goal I'm gonna focus on. The main ideas here are to declutter and clarify exactly what data I want to maintain in my records, and ease my access to it.
- I also need to have an automated way for all this data in my universe to get updated, and records generally maintained. If something is missing, a datapoint is out of place etc, I want to know about it. 
- I'd also like to go in the direction of more passive monitoring with alerts set up for any extraneous datapoints. Interesting things should be alerted, as per MG1:SG2
#### Subgoal 2: Data visibility
I need better ways to look at current market status and gain a deeper understanding of ongoing market dynamics. I don't think I'm going to gain any crazy knowledge from this, but I should try to get to the point where I can monitor ongoing sector rotations, money flow and things like that. 
- System for alerting me when certain things happen, so things can become more passive.
- Generating automated market reports that are tailored to what I'm interested in/ focusing on.
- 

### Main Goal 3: Insight
Create a modular system that slots into goals 1 & 2, allowing me to easily *create, test and implement* market hypotheses. This is the 'rule' portion of the project, I suppose. 
#### Subgoal 1: Create and maintain robust rule system
#### Subgoal 2: Research specific rules for trading/ allocation

Aiming for a modular system that is easily extensible and will allow me to trade multiple rules and instruments. 
Focusing on weighting by Volatility targeting

The XCAP.ALGO project involves all of my explorations in the quantitative finance space. In particular we are looking at two approaches to strategies, asset allocation and diversified trading. 

The goal of the overall project is to create modular components that can work together or individually to test, plan and execute quantitative strategies. 

We are also developing a CLI (Command Line Interface) for conducting research manually, since much of the required infrastructure already exists for the overall system.
 
## Quick Links
- [[Algo Plan]]
- [[Algo Project Reflection]]
- [[Algo Strategies]]
- [[Algo Changelog]]
- [[Algo Docs]]
- [[Algo CLI Docs]] 
- [[Algo ML]]

## Strategy Ideas
[[Algo Strategies]]
Let’s brainstorm strategies here! 

## Live Operation
Live operation of the system is somewhat different from the simulated operation. The live system first tells all components to update their allocations, then executes these in bulk.

*Control Flow of Each Update*
1. Reset list of requested allocations
2. Update all components - this means the system tells all portfolios to update and portfolios tell their screeners to update
3. Last step of portfolio update is to request an allocation for each position it wants to have. It just requests the total number it wants held and attributed to it.
4. The system aggregates all of these values first in a list of all the requests, then in a dictionary mapping tickers to quantities, since multiple systems could make a number of different requests for the same ticker.
5. The difference between current and desired allocations is calculated
6. The system then checks this difference against an inertia value to determine whether to proceed. Imagine the existing position is 100000 shares, and the requested allocation is now 100001 shares. It would not be logical to make the trade in this case, as there would be needless noise as a result of random intraday price moves. So I have a value, now preset to 0.1, where I say if the desired amount is more than 10% away (up or down) from the actual amount, I make the trade.

## Development
A little spot to gather thoughts on development practices, now that the project is getting bigger. I'm thinking of creating a cleaner structure soon.
Some ideas on good dev practices: https://github.com/thuijskens/production-tools

### Testing
We are working on implementing testing throughout the project. We will use the [pytest](https://docs.pytest.org/en/6.2.x/) framework. 
[[Algo Data Pipeline]]

## CLI
The CLI is used to retrieve and display company data in a compact, efficient way. 

- [[Algo CLI Docs - Stock]]
- [[Algo CLI Docs - Quote]]
- [[Algo CLI Docs - Alpaca Monitor]]
- [[Algo CLI Docs - Sort]]
- [[Algo CLI Docs - Screen]]
- [[Algo CLI Docs - Graph]]
- [[Algo CLI Docs - Market Profile]]
- [[Algo CLI Docs - DataManager]]
- [[Algo CLI Docs - DataLoader]]

## Testing


## Trading System
The trading system was created with the goal of maximum modularity. This allows the project to take advantage of significant abstraction and thus simplify the creation of complex, multi-rule strategies.

### Project Setup
Setting up the project is not trivial, but is doable with a bit of work.

1. Clone/ download the repository from github
    `git clone git@github.com:michaeljohncalvey/algo.git`
2. Run virtual environment (from source directory)
    `source env/bin/activate`
 
    And to deactivate: `deactivate`
3. Download stock data
    `python3 data_manager.py TICKER1 TICKER2 TICKER3...`  (see data_manager documentation for more. Need API key!)
4. Run stock CLI
    `python3 stock.py TICKER [-info|-fundo]`


### Testing - Legacy
Note: none of the files in the trading system are designed to be run directly, only for testing. All the code in *all* of the main functions in the files within this directory is for testing and development purposes. To actually execute a strategy, you should create a separate file.

**Note:** I want to change up this functionality pretty significantly, tbh. This will be the first area I look at. I need strong data validation as well. [[Algo Data Pipeline]]
[[000 Life MOC]]
# 080 - Personal Finance MOC
#MOC #my/finances
[[Budget]]
[[Fair Price Residential]][[000 Life MOC]]
# 090 - Lists MOC
#MOC 
[[Quotes]]
[[Movie list]]
[[Reading List TOC]]
[[Drinking Games]]
[[Beer]]
[[Playlists TOC]]

[[Calls]]  <-- Work-related! Interviewing freshies
 [[000 Life MOC]]
 # 095 - Journals MOC
 #MOC
 
 This is where I'd like to gather my private musings, ponderings and reflections. I already have some in the /bear subdirectory, but need to go through, clean and import. Might be nice to revisit some of the era of journalling when I was just getting back into bear heavily. Will search.
 
 [[Daily Log 2020-10-14]]
 [[2020-10-15]]
 [[Daily Log 2020-10-16]]
 [[Daily Plan 2021-02-01]]
[[000 Life MOC]]
# 100 - Ideas MOC
#MOC 

#todo 
[[Todo]]
* Create backlinks!
* [[000 Life MOC]]
# 100 - Projects MOC
#MOC

There's some decent overlap here with my [[005 Active MOC]]. Should think about how this interaction should play out in future.

- [[076 Algo MOC]]
- [[Photon Project]]
- [[071 XCAP MOC]]
- # 15 Minute City
[[100 Ideas MOC]]
Really interesting concept that aligns v closely with my view of an ideal urban environment, where home work and play are within a 15 minute area. Sort of a mini town, densely packed with everything residents need on a day to day basis. 

* Brent Cross Town - example being planned in London.

<!-- {BearID:CFDE54BB-D004-401F-9A6D-B195A1053897-3020-000001B40481817E} -->
[[020 Timeline MOC]]
# 1930s
#timeline/1900s 

1935: [[EPR Paradox]][[000 Life MOC]]
# 200 - Outputs MOC
#MOC
[[Sergio letter]]
[[Notes to make]]
[[Notes from papa]]
[[Rise Notes]]

# Daily Log 2020-10-15
#outputs/log
*Last log:* [[Daily Log 2020\/10\/14]]
*Next log:* [[Daily Log 2020\/10\/16]]
## Goals Today:
- Research question for Econ of EU
- Econ of EU discussion post
- CS Midterm
- Study up for wunnavaxam
	- Need to start laying out docs for regression, same as I did for [[Linear Algebra TOC]] yesterday

<!-- {BearID:A3176623-6A47-4330-AE18-600EC697452F-37181-000244D0DFF90CFC} -->
[[095 Journals MOC]]
#outputs/journal #my/plans
# Daily Log 2021-03-23
[[Todo]] #todo
Set up lots of stuff here in obsidian. I implemented a version of the IMF system, which I must be sure to document here soon. I have a new core file titled [[000 Life MOC]], which contains references to the other core MOC files. MOC files for me represent sort of the core of a constellation of similar ideas. Ideas may connect to more than one MOC, creating a web or map of my knowledge. I would like to create several formal definitions now with regards to how I will leave personal notes here. 

***Actors:***
- Past me
- Present me
- Future me

I should honestly try to think of my life with these as almost 3 discrete entities who I must plan for and work around.

I'm quite excited for what Obsidian seems like it has in store. It appears to be a very powerful system.

![[Pasted image 20210323005206.png]]
![[Pasted image 20210323042239.png]]

I think that's really just incredible. This is mostly just what I've imported from bear, with a few key files added to help aggregate and organize ideas. The new concepts are critical, and I must be sure to elaborate exactly on all of them. 

It would be awesome if I kept adding to this occasionally throughout my life, filling out key details.

## Meta

One of the abstractions for the different levels of details was:
1. Data
2. Information
3. Knowledge
4. Wisdom

The ultimate goal is to seek wisdom. The only way to achieve that is to obtain knowledge and reflect. To obtain knowledge, I must understand and learn information, that I need to be able to recall on demand to be able to connect complex ideas. 

## Areas to consider creating
[[Notes to make]]
- Sustainability!
- Keep going with quantum, fill it out.
- Add stuff from physics. I need to read the textbook and create relevant files.
- ML for sure! Having a blast so far, need to document everything and understand the links. 
- Meta shit - more thinking about thought, 
	- Frames of reference - perspectives through which I view the world
	- Mental Models - specific models of cognition I can use to consider problems.
		- Oppositional 
		- See Munger!
- Linguistics, should learn about it!


It might be interesting to set a challenge to make at least a couple modifications every day. Would be a great way to internalize this way of thinking.

I did some cool shit in the header of [[ML]] to make it representable either like that or [[Machine Learning TOC]]. Neat. I gotta figure out what else I can stick up there.[[095 Journals MOC]] [[005 Active MOC]]
# Daily Log Tue 2021-03-23
**Some brief summary of the day, written at the end**
#outputs/journal  #outputs/log
Last day: [[2021-03-22]]
Next day:  [[2021-03-24]]

Ontology!!

## Notes Worked On
* Core MOCs
* [[Ontology]]
* [[Linear Algebra TOC]]
- [[Quantum Gates]] LaTeX!
- [[Quantum Measurement]]
- [[Quantum Questions]]
- [[Linear Models]]
	- [[Frames of Reference]]
- [[Perceptrons]]
- [[001 Meta MOC]]
	- [[Daily Notes]]
	- [[Evergreen Notes]] in the following stages/ states:
		- [[Seedling Notes]] - create an empty/ only tagged note for future filling.
		- [[Concept Notes]] - Simple definitions, mostly rehashing what I learn elsewhere
		- [[Bridge Notes]] - Connecting two or several concepts specifically
		- [[Table of Content]] - TOC, designed to connect together Bridges, stubs and definitions. 
		- [[Map of Content]] - MOC, The highest level note, a *constellation* of sorts, linking all kinds of concepts, TOCs, ideas and experiences together. The idea is to generally try to connect more TOCs rather than concepts themselves, but that is not a hard rule 
	

## Tasks
- [[005 Active MOC]]
- The church of the larger Hilbert space

## Thoughts
- 

## Class
[[CS333 Week 5]]
[[CS451 Week 5]][[095 Journals MOC]]
# Daily Log Wed 2021-03-24
**Some brief summary of the day, written at the end**
#outputs/journal
Last day: [[2021-03-23]]


## Notes Created
- [ ] 
- [ ]  


## Tasks
#todo 
- [x] Create a slightly more complex tagging system for a kanban-style workflow. Have 2-3 "next", a few more in progress, urgent, icebox, idea, question etc.


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-03-25
**Some brief summary of the day, written at the end**
#outputs/journal  #outputs/log 

## Notes Created
- [ ] [[files/RELI254 Week 5]]
-

## Tasks
- 

## Thoughts**
- Differential Geometry - the shape of space
- Vernacular Islam
- 
[[095 Journals MOC]]
# Daily Log 2021-03-26
Really quite a productive day today. I worked through the PHYS problem set, and while doing so filled out lots of great missing knowledge on subatomic particles. The kind of stuff it's worth having written down, I think. Also a majore reorg of the algo stuff.
 #outputs/log

## Notes Created
- [ ] [[Atoms]]
- [ ] [[Protons]]
- [ ] [[Neutrons]]
- [ ] [[Electrons]]
- [ ] [[Quarks]]
- [ ] [[Isotopes]]
- [ ] [[Elements]]
- [ ] [[Radioactivity]]
- [ ] [[Radiation]]
- [ ] [[Photons]]
- [ ] [[Nuclear Fission]]
- [ ] [[Nuclear Fusion]]
- [ ] [[Weak Force]]
- [ ] [[Mass Extinction Events]]
- [ ] [[Exponential Growth]]
- [ ] [[Algo Plan]]
- [ ] [[Algo Strategies]]
- [ ] [[Algo Changelog]]
- [ ] [[Algo Docs]]
- [ ] [[Algo CLI Docs]]
- [ ]    - [[Algo Docs - System]]
			- [[Algo Docs - Portfolio]]
				- [[Algo Docs - Screener]]
					- [[Algo Docs - ScreenerRule]]
				- [[Algo Docs - Sorter]]
					- [[Algo Docs - SorterRule]]
			- [[Algo Docs - Subsystem]]
				- [[Algo Docs - Forecaster]]
					- [[Algo Docs - Rule]]
		- [[Algo Docs - Instrument]]


## Tasks
#todo 
- [x] [[PHYS106 PS5a]] 
- [x] [[PHYS106 PS5b]]


## Thoughts
- Happy with the pace of work. It's really cool how making these densely linked notes really does help me parse through concepts. It forces you to think about how things interrelate, which is more important than understanding anything in a vacuum.
- I need to be careful not to over-organize. It's only as useful as it is, not how much time I put into it. I defo have a tendency to overengineer shit and then not use it, so I'll be tryna avoid that.


## Plan for weekend
- [[RELI254 Essay 1]]
- [[Machine Learning TOC]] upgrade time baby! I need to do a detailed dive on a bunch of the models. Explain in english how they work, strengths, weaknesses, include code implementations from class.
- [[011 Mental Models MOC]] and [[Frames of Reference]] - both important concepts that I arguably haven't explored enough. As in thought#2 I must keep this lean, but I bet it'd be nice to elucidate a few clear ways of approaching problems. 


[[095 Journals MOC]]
# Daily Log 2021-03-28
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] 

## Tasks
#todo 
- [x] [[RELI254 Essay 1]]
- [x] Playlists!!


## Thoughts
- 

## Evening
Started around 7:20
Going to reflect on life, school and friends. I'd like to plan my next steps for learning and my garden. I'll do this stream of consciousness style so my thoughts just flow and I won't delete. 

[[My Forest]][[095 Journals MOC]]
# Daily Log 2021-03-29
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Electrostatic Force]]
- [ ] [[Gravity]]
- [ ] [[Quantum Mechanics TOC]]
- [ ] [[Beam Splitter]]
- [ ] [[Entanglement]]
- [ ] [[Tensor Products]]
- [ ] [[Matrices]]
- [ ] [[Vector Spaces]]
- [ ] [[Quantum States]]
- [ ] [[Quantum Measurement]]
- [ ] [[Partial Quantum Measurement]]
- [ ] [[Multi Qubit Systems]]

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-03-30
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Law of Vocal Minority]]
- [ ] [[011 Mental Models MOC]]
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
- Pluralism - a situation where multiple traditions or cultures coexist
- Religious Nationalism vs Fundamentalism
	- Nationalists don't say there can't be muslims in India, they say that Hindu culture and politics are simply prioritized.
	- Eg in America we use christmas even tho hannukah is allowed.
	- ie assimilate vs GTFO
	- 
[[095 Journals MOC]]
# Daily Log 2021-03-31
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
- 
electrostatic & electric same
electrostatics & electromagnetics not same
electrostatics - all charges are fixed in space, they do not move. Still exert forces!
magnetostatics - A lot of charges moving steadily so current is constant over time
electromagnetism - Broadest category, includes elecrostatics and magnetic forces
* [[Electric Charge]]
* Energy
* Power
* Current - amps - the speed of the charge
* Resistance
* Charge[[095 Journals MOC]]
# Daily Log 2021-04-01
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-04-06
52 days left
 #outputs/log
[[2021-04-07]]

## Notes Created
- [ ] [[CS333 PS6]]

## Tasks
#todo 
- [ ]


## Thoughts
- Need to implement a quantum algo for the [[076 Algo MOC]]
- Add documentation on [[Multi Qubit Systems]] and [[Quantum Measurement]] in particular. RN only for the single qubit case.
- Elaborate page for [[Quantum Gates]]. Need to transcribe all basic gates too!
-[[095 Journals MOC]]
# Daily Log 2021-04-07
[[2021-04-06]]
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[PHYS106 PS7a]]

## Tasks
#todo 
- [ ]


## Thoughts
-  [[Waves]]
[[095 Journals MOC]]
# Daily Log 
{{title}}
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-04-12

**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Electromagnetism]] 
- [ ] [[Qiskit]]
- [ ] [[XCAP Strategies]]

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-04-13
**Some brief summary of the day, written at the end**
 #outputs/log


## Notes Created
- [ ] [[Deutsch's Algorithm]]
- [ ] [[Quantum Algorithms]]
- [ ] [[Inner Products]]
- [ ] [[Outer Products]]
- [ ] [[RELI Week 7]]
- [ ] [[Complex Conjugate]]
- [ ] [[Conjugate Transpose]]
- [ ] [[Transpose]]
- [ ] [[Random Walks]]
- [ ] [[Quantum Random Walks]]

## Tasks
#todo 
- [ ]


## Thoughts
- Quantum algos focus on [[Quantum Gates]] rather than [[Quantum Measurement]] because this way quantum information can be preserved and used for funky operations.
[[095 Journals MOC]]
# Daily Log 2021-04-14
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Partial Quantum Measurement]]

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 
{{title}}
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Deutsch's Algorithm]]
- [ ] [[Left Stochastic Matrices]]

## Tasks
#todo 
- [x] Figure out CS minor
- [x] Exam review by EOD! Then sign up for midsem checkin!
- [x] MIDTERM COMING UP!!!
- [ ] 


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-04-18
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] 

## Tasks
#todo 
- [x] [[Quantum Random Walks]]
- [x] [[CS333 PS7]]
- [x] [[RELI254 Islam in South Asia]] Do a couple formal respos!


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-04-20
{{title}}
**Some brief summary of the day, written at the end**
 #outputs/log

## Notes Created
- [ ] [[Deutsch's Algorithm]]

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log
 #outputs/log

## Notes Created
- [ ] [[CS333 PS7]]
- [ ] [[CS333 PS8]]

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log
 #outputs/log

## Notes Worked On
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
- 
[[095 Journals MOC]]
# Daily Log 2021-05-03
 #outputs/log

[[2021-05-02]] < + > [[2021-05-04]]

## Notes Worked On
- [ ] 

## Tasks
#todo 
- [ ]


## Thoughts
[[095 Journals MOC]]
# Daily Log 2021-05-04
 #outputs/log

[[2021-05-03]] < + > [[2021-05-05]]

## Notes Worked On
- [ ] [[020 Timeline MOC]]
- [ ] [[003 Process MOC]]
- [ ] [[011 Mental Models MOC]] and tons more in here! This will take a TON of active work. Lots and lots of very different theories to research, categorize and understand.
- [ ] [[Cognitive Biases TOC]]
- [ ] [[Affect Heuristic]]
- [ ] [[Availability Heuristic]]
- [ ] [[Heuristics and Biases]]


## Tasks
#todo 
- [ ]


## Thoughts
- I've been doing some thinking on how to better delineate notes in different stages of their evergreen lifecycle. [[095 Journals MOC]]
# Daily Log 2021-05-05
#outputs/log

[[2021-05-04]] < + > [[2021-05-06]]

## Notes Worked On
- [ ] [[011 Mental Models MOC]] Still so much more to do on this one. I am really honestly super excited about the idea of putting all of these concepts together in a structured way. I'm already seeing *perfect* parallels with many of the things I've been thinking about with regards to the [[Decision Machines]]. The mental models are really great because they attempt to span the breadth of the causes and consequences of human consciousness and critically ability to learn. I need to do a lot more reading soon on the [[054 Biology MOC]]
- [ ] [[Decision Machines]]
- [ ] [[Neurons]] This one needs way more work. I know some stuff, so I should honestly try the [[Feynman Learning Method]]
- [ ] [[010 Mind MOC]]


## Tasks
#todo 
- [ ]


## Thoughts
[[095 Journals MOC]]
# Daily Log 2021-05-06
#outputs/log

[[2021-05-05]] < + > [[2021-05-07]]

## Notes Worked On
- [ ] [[Quantum Error Correction]]
- [ ] [[Partial Quantum Measurement]]
- [ ] [[RELI254 Islam in South Asia]]

## Tasks
#todo 
- [ ]


## Thoughts
- Birthday Ideas!!!
	- Goshen Beer Die
	- Rent a boat!
	- Party at the knoll?
- [[Oklahoma Grad Trip Plan]][[095 Journals MOC]]
# Daily Log 2021-05-06
#outputs/log

[[2021-05-05]] < + > [[2021-05-07]]

## Notes Worked On
- [ ]

## Tasks
#todo 
- [ ]


## Thoughts
- [[095 Journals MOC]]
# Daily Log 2021-05-09
#outputs/log

[[2021-05-08]] < + > [[2021-05-10]]

## Notes Worked On
- [ ]

## Tasks
#todo 
- [ ]


## Thoughts
- [[Dunbar's Number]][[095 Journals MOC]]
# Daily Log 2021-05-10
#outputs/log

[[2021-05-09]] < + > [[2021-05-11]]

## Notes Worked On
- [ ]

## Tasks
#todo 
- [x] Pay NJ bill
- [x] Pay Serg
- [x] Pay insurance bill
- [ ] 


## Thoughts
- [[095 Journals MOC]]
# Daily Log 2021-05-24
#outputs/log

[[2021-05-23]] < + > [[2021-05-25]]

Tripping today. Having a really interesting reflective experience. I'm finally tripping and revisiting the notes at the same time, and this has been a long time coming.

Yo I can't ever forget my mission. [[000 Life MOC|Life]], and existence are the mission. I strive to understand as much as I can and progress humanity to the next level of civilization. We are destined for the stars, the question is who will bring us there and when. I access the [[001 Meta MOC|Meta]] to wield existence to my benefit.

I've done a crazy bit of thinking and reflecting on the nature of it all. It's really quite wild, to be honest. I think the most basic, low [[Abstraction]] category of being is [[000 Reality MOC]]. Existence is matter, things, planets, forces, and fundamentally [[Quantum Mechanics TOC|Quantum Mechanics]], which is the crazy thing about all of it. *Reality*

Today actually marks a turning point in the way I view my [[000 Life MOC|Life]], [[000 Reality MOC]], [[Existence]]

I'm curious to see what the impact of my renamings is going to be. From index to life. Body to me. 


## Notes Worked On
- [ ] [[Abstraction]]
- [ ] [[010 Mind MOC|Mind]]
- [ ] [[Consciousness]]
- [ ] [[Orders of Reality]]
- [ ] [[Time]]
- [ ] [[000 Life MOC|Life]]
- [ ] [[010 Me MOC|Me]]


## Tasks
#todo 
- [ ]


## Thoughts
- [[095 Journals MOC]]
# Daily Log 2021-05-25
#outputs/log

[[2021-05-24]] < + > [[2021-05-26]]

## Notes Worked On
- [ ]

## Task
#todo 
- [x] [[RELI254 Op Ed + Reflection]] - 
- [x] [[CS333 Corrections]] - [[2021-05-26]]
- [x] [[CS333 Final Grade Proposal]] - [[2021-05-26]]
- [x] [[PHYS106 Essay Final Draft]]


## Thoughts
- [[095 Journals MOC]]
# Daily Log 2021-06-04
#outputs/log

[[2021-06-03]] < + > [[2021-06-05]]

Out at the lake for our first full day today. I'm with [[Omar Alsaeed]] and [[Sergio Bobadilla]] on our road trip. We conquered about 1800 miles of driving to get here to Oklahoma from VT, and we're now taking a break at the half way point here at [[Grand Lake]].

I'm honestly contemplating moving my journal over to Obsidian. I feel like I can write way quicker and it'll be nice to have my thoughts link up. I can reference stuff, search stuff and so on. Can also start building out [[035 Places MOC]] and [[030 People MOC]] and [[020 Timeline MOC|Timeline]]. NOT a terrible idea. I should still do more reflective journals in my main journal though. I enjoy the process of writing in those, and it's nice to be able to lay out ideas.

Getting here was crazy. We spent a few days for senior week out at a place organized by [[Tommy Tarantino]] out near [[Goshen]]. HELLA fun time. Something like 25 of the boys just booling out. All the russian guys, HHH guys and my suitemates. Had a sweet darty, boomed a couple times and just generally enjoyed the vibes. Of particular note was our time spent out by the fire for several long nights with [[Andrew Gleason]].

### [[Summer Road Trip]]

After Goshen on the morning of the 2nd, MAD early, we got up and drove from VT down to 

Today we've just been chilling around theWe've been having a really truly lovely day so far. I got up pretty late at like 10:30, went for a run and brought back milk and eggs for breakfast. Saw a vulture eating some dead animal in the road on the way and it was pretty damn neat. I didn't run enough, but am very happy that I've run a couple times last few days. I gotta get mad serious and track times and distances, perhaps even here. [[020 Body MOC]]

## Notes Worked On
- [ ] [[Fair Price Residential]]
- [ ] [[080 Personal Finance MOC]]

## Tasks
Gotta plan my shit lets go.


#todo 
- [x] Transfer myself all the money needed back to [[Fair Price Residential]] Bank account.
- [x] Car return options
- [ ] 

## Spending refund
$2890.49
[[095 Journals MOC]]
# Daily Log 2021-07-04
#outputs/log

[[2021-07-03]] < + > [[2021-07-05]]

It's been quite a while since I've made a daily note. Gotta get back in the habit now.

I've been thinking it may be interesting/ worthwhile to transcribe work notes into obsidian. I need to find out from HR if this is a valid idea. Never seemed to have issues as an intern, but that shit's different. 


## Tasks
#todo 
- [ ]

## Notes Worked On
- [ ] [[AI]]
- [ ] [[Hassabis - Neuroscience Inspired AI]]
- [ ] [[011 Mental Models MOC|Mental Models]]
- [ ] [[Neural Networks]]
- [ ] [[054 Neuroscience MOC]]
- [ ] [[054 Biology MOC|Biology]]
- [ ] [[Hippocampus]]
- [ ] [[Reinforcement Learning]] <- Really wanna go in on this one!
- [ ] [[Dimensionality Reduction]]

[[095 Journals MOC]]
# Daily Log 2021-07-05
#outputs/log

[[2021-07-03]] < + > [[2021-07-05]]



## Tasks
#todo 
- [ ]

## Notes Worked On
- [ ]

[[095 Journals MOC]]
# Daily Log 2021-07-29
#outputs/log

[[2021-07-28]] < + > [[2021-07-30]]

Working on studying for the series 7, and in a bout of productive procrastination, decided to think once more about my [[076 Algo MOC]]. Specifically, I created [[Algo Project Reflection]] and wrote down my thoughts on why productivity dropped so much the last month or two of school (besides the copious alcohol intake). Some interesting takeaways about the lack of direction and focus in my planning process. In hindsight, this is actually much the same mistake I made when working towards developing CloudCity VR, interestingly. I think that's a crucial life learning/ takeaway. I need a better long term planning process. 



## Tasks
#todo 
- [ ]

## Notes Worked On
- [ ] [[076 Algo MOC]]
- [ ] [[Algo Project Reflection]]
- [ ] [[Algo Changelog]]
- [ ] i

[[095 Journals MOC]]
# Daily Log 2021-08-02
#outputs/log

[[2021-08-01]] < + > [[2021-08-03]]

Some quick reflections on the overall process I've developed here and how it fits in with my general knowledge are in order soon. I've been working a lot on more personal, reflectioney type stuff in my notebook. It partly defeats the purpose of this vault to have things separated like that, but I think there's some value to doing some of this stuff by hand vs in obsidian. I prefer having random musings, poetry and reflections on a more personal level in my notebook, and instead focus obsidian more on sources/ outputs and the more technical side of my knowledge, to which this system lends itself well. 


## Tasks
#todo 
- [ ]

## Notes Worked On
- [ ]

[[095 Journals MOC]]
# Weekly Plan 2021-W18
#outputs/log

[[2021-05-12]] < + > [[2021-05-14]]

# Reflecting

## Last Week Reflection

# Planning


## Coming Week Plan

### Sunday
### Monday
### Tuesday
### Wednesday

### Thursday
- 
### Friday
- Physics exam 1
### Saturday

## Work That Needs Doing
- [[CS451 Project Exploration]]
- [[CS333 PS9]]
- [[CS333 PS10]]


# Tracking

## Notes Worked On
- [[Machine Learning TOC|Machine Learning]]


## Tasks
#todo 
- [ ]


## Thoughts
- [[095 Journals MOC]]
# Weekly Plan 2021-W19
#outputs/log

[[2021-05-16]] < + > [[2021-05-18]]

# Reflecting

## Last Week Reflection

# Planning


## Coming Week Plan

### Sunday
### Monday
- [[PHYS106 Annotated Bibliography]]
	- 

### Tuesday
### Wednesday

### Thursday

### Friday

### Saturday

## Work That Needs Doing

# Tracking

## Notes Worked On
- [ ]


## Tasks
#todo 
- [ ]


## Thoughts
- [[095 Journals MOC]]
# Weekly Plan 2021-W26
#outputs/log

[[2021-07-03]] < + > [[2021-07-05]]

# Reflecting

## Last Week Reflection

# Planning


## Coming Week Plan

### Sunday
### Monday
### Tuesday
- 
### Wednesday

### Thursday

### Friday

### Saturday

## Work That Needs Doing

# Tracking

## Notes Worked On
- [ ]


## Tasks
#todo 
- [ ]


## Thoughts
- [[XCAP Writing]]
# 2Q20 Shareholder Letter
#outputs/letter 
Dear Rise Shareholders,

We are living through historic times that will likely define the next decades of our lives. Our second quarter, which lasted from April through the end of June, encompassed a majority of the initial post-covid rebound. At the end of last quarter, covid abruptly put a stop to much of life as we knew it and tore apart industries, educations and families. Things have been difficult, but I have been proud of how all of our members rose to the occasion to learn more and contribute to the group. With that context in mind, the fund saw its best-ever performance of 62.6% unrealised gain for the quarter - a result I and I hope all of you are very pleased with. Moving forward, we face a tricky investment environment with diverging leadership that will not be representative of recent winners. While the covid crash saw outperformance by tech and companies that were considered more resilient to business interruption, we now expect more cyclical companies to perform as the economy begins a new cycle and companies surprise to the upside with earnings through growing operating margins. With this in mind we need to be nimble on our feet and ready to react to leading data, making changes to our positioning as the situation develops.

Strategically, we made the macro call last quarter to partially pull out of risky assets, putting us on solid footing at the start of Q2 with respect to our allocation as the market troughed. Once the outlook stabilised, we opened several new positions, the highest-conviction of which has been a $1600 investment in NMIH, and we upsized our cannabis position in Trulieve by almost 200%. We liquidated our small tactical plays on airlines and energy for roughly a net-zero impact overall. Alongside this we engaged in a number of smaller tactical trades with the goal of managing our downside risk and experienced varying degrees of success. The VXX long ETF was the most successful trade we had on this front, however after the quarter’s conclusion we have explored SPY puts which seem to be the most cost effective (and least asset-intensive) way of buying protection. 

This more active approach to managing our money has has brought positives and negatives. For the quarter, we traded a notional value of  $5,233, representing more than 100% of our equity holdings at the start of the quarter. Although we bought more than we sold on a net basis, we ended the quarter with more cash than we started with by virtue of realising some of our positions throughout the quarter and averaging down our cost bases in several positions. 

The more we trade, the more we risk making the wrong decision time-wise; however, I think we managed the risk well and managed to achieve great returns while doing so. The big test will now be on us to continue to deliver results in a much more expensive environment where the biggest winners may not be the most obvious high growth names. 

Organisationally, we’ve been undergoing a slow but steady shift in the way we operate to optimise our processes and streamline important decisions. I’ve been happy with how everyone has thrown themselves at the new work and responsibilities, especially given how difficult things have been in our personal lives with respect to the quarantine. Luckily we’ve been communicating remotely for most of the year so we have been well prepared for this environment when many institutions and organisations have struggled. 

Thank you all for your continuing faith in me to run this group. I know I couldn’t do it without the full commitment everyone on the operating team has made over the quarter. I am so excited to see what we’re going to be able to achieve when most of us are back in the same room. Looking past the second quarter, we’re set to more than double our 2Q AUM over the course of the summer through capital raises and investment gains, and I cannot wait to start with the new challenge of managing over $25k with you all.

Thank you,

Michael Calvey
Chairman & Portfolio Manager
Rise Capital Partners

<!-- {BearID:C643B15D-DD85-4E83-AC79-B681EFE8A55B-4760-0001B95FFD7AED2A} -->
[[000 Life MOC]] [[005 Active MOC]]
# 300 - Inputs MOC
#MOC


## Reading

### #inputs/books 
[[Kahneman - Thinking Fast and Slow]]
[[Hayek - The Road To Serfdom]]
[[Johnson - Socrates, A Man For Our Times]]

### #inputs/articles
[[Hassabis - Neuroscience Inspired AI]]
# A New Force
[[Playlists TOC]]
<!-- #_reference/music -->

## Track List
Ivre
Galileo
Dream State
Moonbound
Keep on rocking
Highway
resonance
memories from the past
wicked world
miss otis regrets
Crossfire
Mindreader
Pressure
Spiritual but not religious
small-town boy
home
looking for something
drip too hard
deep chicken
ride
fly away from here
lazy
alive
la boheme
Tieduprightnow
one of these nights
wasted time
let it be (aretha)

<!-- {BearID:2BC1ADC9-A9A4-4B5C-9635-88358BE4B86E-1476-0000A826FA4D4556} -->
[[053 Computer Science TOC]]
# AI
#concepts/cs #🌲 

How can you codify decision making algorithmically? Can we represent human intelligence in algorithmic form? 

Lots of hurdles to overcome before this can be accomplished.

Areas for further research:
- [[Emergence]]
- [[054 Neuroscience MOC]]

Very interesting how a lot of my past thoughts on this topic generally have been closely mirrored by some of what I've been reading on the topic. 

I really liked [[Hassabis - Neuroscience Inspired AI]]. I feel like it hit on a lot of my beliefs that AI/ ML would benefit from a closer study of human intelligence.


[[Machine Learning TOC|Machine Learning]]

[[Symbolic AI]]

[[Decision Machines]]

I need to really condense and rethink my thinking on this field. Very deep topic. 

[[054 Neuroscience MOC]][[Statistics TOC]] | [[Machine Learning TOC|ML]]
# Area Under [[ROC]] Curve
#concepts/cs/ml 

AUC or Area Under [[ROC]] Curve is a single number that helps us evaluate the accuracy of our [[ML Models TOC]]. It is a number between 0 and 1, where 0.5 would be random performance. Numbers above 0.5 are better than random, 1 is perfect, and less than 0.5 are worse than random.

[[011 Mental Models MOC]]
# Abstraction
#concepts/mental_models 

Abstraction is one of the most powerful tools in the human intelligence toolbelt. It allows us to grapple with ideas far more complex than anything we could otherwise handle. It is also a definite outcome of the physiology of the [[Brain]]. Specifically, the brain's capacity for [[Relational Thinking]] is what gives us the ability to abstract.

This is really interestingly empowered by [[Linguistics MOC]][[Company Research]]
# Aerofarms
#inputs/companies

Current ticker:

Very interesting growing "aeroponics" company. World leader in vertical farming. Lots of IP. Focus now on leafy greens 
* 390 times more produce per sqft
* Uses 95% less water
* Zero pesticides

Acquired via `SV` - Spring Valley Acquisition Corp [[SPAC]]

- TAM ~$100bn in leafy greens, $1.9tn in adjacent markets
- # Aerospace
#concepts
#### HEAD
Space for thinking about flight and space - supersonic, low carbon etc

#### REF
[[000 Life MOC]] [[Notes to make]]
[[Technology]]


# Space
* SpaceX
	* Starship
		* Runs on liquid methane (cheaper than rocket fuel
		* Fully reusable (unlike Falcon 9)
		* $100 cheaper than current cost to LEO!!!! Time to think of things to stick up there!!!
		* Earth - LEO commerce
			* Earth - Moon
			* Earth - Mars
	* Starlink - Gonna be insane. MUCH more \$\$\$ than in launch services
		* Global telecommunications revenue hit $2.4bn 2020!! - could capture tens of billions
			* HUGE jump in R&D budget
		* Totally undercutting current internet providers, esp in rural areas (1 Gbit down, 100mbps down rn)
* In space manufacturing - 0 g creates TONS of possibilities, and decreasing launch costs could change the game completely

# Supersonic flight
*Contenders:*
	* Boom aerospace - X1, first supersonic flight 2021!! scale around 2026
	* Aerion - Mach 1.6 business jet, ready for production
	* Hermeus - Mach 5 20 passenger !?!?
	* Exosonic

*Interesting tests to watch:*
	* NASA x-59 flights - testing for acceptable levels of supersonic boom, paving the way for overland supersonic growth in the US
	*
[[Heuristics and Biases]] | [[Cognitive Biases TOC]]
# Affect Heuristic
#concepts/mental_models

This is a mental shortcut ([[Heuristic]]) where current human decision making is significantly influenced by current emotions.[[071 XCAP MOC]]
# Akhil Closing Letter
#projects/old/xcap 

Dear Akhil,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |            |
| -------------------- | ---------- |
| Amount Invested      | $ 2,560.00 |
| Shares Purchased     | 2548.29    |
| Cash-out Share Price | $2.16      |
| Cash-out Value       | $ 5,504.30 |
| Investment Return    | 115%       |


- Amount Invested: $2,560.00
- Shares Purchased: 2548.29
- Cash out share price: $2.16
- Cash out value: $5,504.30
- Investment Return: 115%

Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL# Akhil Letter
[[XCAP Writing]]

Dear Akhil,

Thank you for remaining the oldest shareholder in Rise (now X CAPITAL) since those February days down in the basement of Battell in 2018. I’m writing to update you on the current status of your holdings, as well as how things are looking for the quarter and the year.

In our third quarter, the fund appreciated by *6.05%*, bringing our current year-to-date performance up to a fantastic *26.76%.* I am incredibly proud that we’ve managed to reach a point where we have started making serious gains, and I hope we can carry this momentum into the 4th quarter and beyond.

As we have grown, we have gradually updated our methods of governance to be more “correct” or in line with what might be expected in the professional sphere. With that in mind, one of the remaining 

Your investment of *$2560.00*

<!-- {BearID:58E0A3D6-D975-41D3-96E5-21A23A4CC37F-1228-0000A89788939BC9} -->
[[030 People MOC]]
# Aleksey Shevchenko

Think about secret sauce or the differentiator! Spend max time doing useful stuff.

Allocation engine is the brain of the system - more of that!

Risk associated with future - try risk weighting

two twins, 17 yrs

Office fan

Poker fan


[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
This CLI tool is used to monitor various aspects of our Alpaca portfolio. Alpaca API keys must be provided in a .env file for this script to work. 
# Algo Alpaca Monitor Docs
#projects/algo/docs #projects/algo/docs
### Usage
```
> python3 monitor_alpaca.py
*** Main overview printed ***

> python3 monitor_alpaca.py -t
*** Recent trades printed ***

> python3 monitor_alpaca.py -o
*** Recent orders printed ***

> python3 monitor_alpaca.py -g
*** Graph of recent performance displayed ***
```
[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# DataLoader
#projects/algo/cli #projects/algo/docs
This component is very helpful for loading all kinds of financial data. The methods are all static, with the class acting as a container for ease of use.

### Usage
```
from trading_system.data_loader import Data

price_data = Data.load_data_from_file("NMIH", "daily")

nasdaq_tickers = Data.retrieve_nasdaq_tickers()
```

### API
* `load_data_from_file(ticker, frequency, (start_date), (end_date))`
    Loads techno data on a ticker. Start and end date optional
* `load_fundamental_data_from_file(ticker, datatype)`
    Loads requested fundamental data. Datatypes are passed as strings [“is”, “cf”, “bs”, “eps”]
* `retrieve_spy_tickers()`
    Does what it says on the tin and returns a list of all SPY tickers (as of like 3 months ago lmao)
* `retrieve_nasdaq_tickers()`
    Same but for nasdaq
* `load_ticker_instruments(ticker_list, Instrument)`
    [[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# DataManager
#projects/algo/cli #projects/algo/docs
This crucial component of the overall system provides a command line interface to download stock data. 

### Notes
I've quite successfully reworked this component from the ground up recently. Now it's multi-threaded, handles myriad datatypes and is generally a lot more robust. There is still no system for scheduling or record-maintenance. I need to figure out a good way to approach these challenges, as they are needed for fulfillment of my [[076 Algo MOC#Subgoal 1 Clean and refine data pipeline]]

*Datatypes to handle:*
		* Individual stock/ grouped stock:
			* OHLCV - (bars)
			* trades - tick
			* NBBO quotes
			* fundamental
				* Maintain full fundamental history going back to earliest moment
				* Earnings calendar?
			* info
			* dividends
			* news
		* Whole market/ agg:
			* Available ticker types
			* Winners/ losers
			* Get similar tickers
			* Get sector tickers


### Usage
```
# Retrieve a single stock's daily technical data
>>> python3 data_manager.py NMIH
['NMIH']
Retrieving data for ['NMIH']
...

NMIH retrieved

# Retrieve multiple stocks' daily technical data
>>> python3 data_manager.py NMIH TCNNF CURLF AAPL
(output abridged)

# Retrieve all data about a stock
>>> python3 data_manager.py NMIH -all
(output abridged)

# Retrieve one type of fundamental info for a stock
>>> python3 data_manager.py NMIH [-bs | -is | -cf | -eps | -info]
(output abridged)
```

### API
* TODO
* If you want to edit the DataManager, good luck[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# Graph
#projects/algo/docs #projects/algo/docs
This component integrates termplotlib with the instrument implementation to display price graphs in the terminal.

### Usage
```
> python3 graph.py NMIH [-revenue | -eps | -income]

Displaying monthly chart for NMIH

    25 +------------------------------------+
       |                                    |
       |             *         N*IH ******* |
  24.5 |         **  *          ***         |
       |         ** **       ******         |
    24 |         ** **       ** * *         |
       |         * ***       *  *  *        |
       |*        * ***       *     *        |
  23.5 |*  *    ** *  *      *     *        |
       |** **   *  *  *      *     *        |
    23 |** **   *      *    *      *       *|
       | * * ***       **   *      **      *|
       |  ** ***        ** **      **     * |
  22.5 |     ***          ***      * *    * |
       |     **           ***        *    * |
       |     *             *         **  ** |
    22 |     *             *         **  ** |
       |                             *****  |
  21.5 |                             ****   |
       |                             ** *   |
       |                             **     |
    21 +------------------------------------+
       0     10    20     30    40    50    60
```

### API
* ``

[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# MP
#projects/algo/docs #projects/algo/docs
This component integrates some of the earlier research done on market profile representations of prices. Check out James Dalton’s book on the topic to learn more. A simple way to think of this is a histogram of prices, with prices on the y axis and each letter on the right representing the fact that the instrument traded at that price during that range.

### TODO
I need to clean up some of this stuff more. Need to make the API closer to the other CLI components.


### Usage
```
> python3 mp.py NMIH
24.00: 	N
23.80: 	N
23.60: 	N
23.40: 	N
23.20: 	N
23.00: 	ABCELMN
22.80: 	ABCELM
22.60: 	ABCELM
22.40: 	ABCELM
22.20: 	ABCELM
22.00: 	ABCDEFGHIJKLM
21.80: 	ADEFGIJ
21.60: 	ADEFGIJ
21.40: 	ADEFGIJ
21.20: 	ADEFGIJ
21.00: 	ADEFGIJ
20.80: 	A
20.60: 	A
20.40: 	A
20.20: 	A
20.00: 	A
```

### API
* ``[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# Algo Quote CLI Docs
#projects/algo/docs #projects/algo/docs

This function provides an easy command line way to pull price data on one or multiple tickers. Support for stdin is included.

### Usage
```
# Get quotes for one ticker
> python3 quote.py NMIH
NMIH:	$23.44     	(1.472%)

# Get quotes for multiple tickers
> python3 quote.py NMIH VRRM AMZN
NMIH:	$23.44     	(1.472%)
VRRM:	$14.49     	(1.684%)
AMZN:	$3185.58     	(0.152%)

# Get quotes for tickers in file
> python3 watchlist.txt | quote.py
NMIH:	$23.44     	(1.472%)
VRRM:	$14.49     	(1.684%)
AMZN:	$3185.58     	(0.152%)
...

# Output results of a screen directly
> python3 screen.py NMIH ESNT RDN -rules="eps_rule=8" | quote.py
NMIH:	$23.44     	(1.472%)

# Display live data
> python3 screen.py NMIH -l
```[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# `Screen`
#projects/algo/cli #projects/algo/docs
The Screen CLI works similarly to the Sort CLI. It is a wrapper for the screener, and allows quick identification of companies that meet certain financial metrics.

### Usage
```
> python3 cli/screen.py NMIH GNW RDN ESNT -rules="eps_rule=8, max_mkt_cap=5000000000"
NMIH

# Combined screen and sort
> python3 screen NMIH ESNT RDN GNW -rules="max_mkt_cap=5000000000" | sort
```

### API
* tickers: Passed in space separated, or else in a `-tickers="T1, T2, T3..."` arg
* `-rules="` Flag: 
    * `eps_rule=x` Pass companies which surprised on EPS 
    * `max_mkt_cap=x` Pass all companies with market caps less than x
    * `min_mkt_cap=x` Pass all companies with market caps more than x
    * `equal_weight` Passes all companies, used to test and create equal-weighted portfolios
* `-v` verbose output
* `-nasdaq` Use all nasdaq tickers as universe
* `-spy` Use all spy tickers as universe
* `-reverse` Reverse the sorting
[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]]
# `Sort`
#projects/algo/cli #projects/algo/docs
This CLI wrapper for the sorter allows easy io for sorting tickers based on their underlying characteristics.

### Usage
```
> python3 sorter.py NMIH ESNT RDN GNW -rule=ownership
GNW RDN NMIH ESNT
```

### API
* tickers: Passed in space separated, or else in a `-tickers="T1, T2, T3..."` arg
* `-rule=` Flag: 
    * `fwd_pe` Sort by increasing forward PE ratio
    * `ownership` Sort by my proprietary ownership monitoring stat
    * `mkt_cap` Sort by market cap 
    * `margin` Sort by profit margin
* `-v` verbose output
* `-nasdaq` Use all nasdaq tickers as universe
* `-spy` Use all spy tickers as universe
* `-reverse` Reverse the sorting
[[076 Algo MOC]] | [[Algo Docs]] | [[Algo CLI Docs]] 
# Stock CLI Docs
#projects/algo/cli #projects/algo/docs
The stock file is used to interact with stock data through the terminal. The eventual idea with this is to be able to use screeners in conjunction with this class to display all stocks in a sector or following certain `ScreenerRule`s that pick high-EPS companies

### Usage
```
>>> python3 stock.py NMIH

NMIH			$23.75		[$8.06 - $35.66]
50DMA | 200DMA:		$23.3758	$19.8911
Mkt cap:		$2.064bn

>>> python3 stock.py -fundo

FUNDAMENTAL REPORT for NMIH

EPS - last 8 quarters, recent first
Reported:		0.47	0.4	0.75	0.75	0.71	0.59	0.56	0.46
Surprise %:		33.0	4.0	13.0	6.0	14.0	8.0	7.0	0.0

INCOME STATEMENT - ($ millions)
Total Revenue:		107.3	106.7	113.6	102.3	102.7	89.5	75.6	79.8
% margin:		35.6	25.1	51.3	49.1	48.5	43.7	43.5	44.5
Net Income:		38.2	26.8	58.3	50.2	49.8	39.1	32.9	35.5

CASH FLOW - ($ millions)
CF from Ops:		87.6	79.5	47.9	68.6	52.2	59.0	28.3	46.5
CF from Fin:		-2.3	466.2	-4.9	3.7	0.2	-0.1	-1.8	0.3
CF from Inv:		-406.6	-140.1	25.8	-77.1	-42.3	-63.0-12.0	-39.7
Capex:			4.5	3.1	1.5	17.4	3.0	2.7	1.8	13.4

BALANCE SHEET - ($ millions)
Cash:			188.6	513.4	107.3	38.4	43.0	34.3	39.8	23.9
Current assets:		56.2	56.7	60.0	61.6	63.6	62.7	66.2	66.4
Current Liabilities:	59.3	104.8	20.7	39.9	39.3	24.4	17.0	31.1
Total LT debt:		403.4	405.0	158.3	160.1	146.0	146.3171.8	173.9

>>> python3 stock.py NMIH -info

NMI Holdings, Inc
Financial Services/ Insurance-Property & Casualty

NMIH		$23.75	[$8.06 - $35.66]
50DMA | 200DMA:	23.3758	19.8911
EPS:		$2.254		(-35.0% YoY)
RPS:		$5.803		(6.0% YoY)
Mkt cap:	$2.064bn
Employees:	261

VALUATION
Forward PE:	10.5708
EV to EBITDA:	0
Price to book:	1.5781
Profit Margin:	0.4051

OWNERSHIP SUMMARY
Institutional:	97.144%
Insider:	1.358%
Short % float:	4.0%
Short ratio:	5.2
```

### API
* `quote(symbol)`
    Retrieve latest quote for a ticker
* `fundamental_report(instrument)`
    Print out a fundamental report for an instrument
* `info_report(instrument)`
    Print out an informational report for an instrument
* `technical_report(instrument, timeframe)` TODO
    Print out text data about a stock’s recent price & volume moves. Perhaps market profile can be added.
* `display_chart(instrument, params)`
    Display a chart of `instrument`’s price data.---
aliases: ["Algo CLI"]
---
[[055 Coding TOC]] | [[076 Algo MOC]] | [[Algo Docs]]
# Algo CLI 
#projects/algo/cli 
#projects/algo/docs

## Description
This is the page for explaining my own new “algo CLI” API. The goal of the CLI portion of the [[076 Algo MOC]]
This will start as a project for displaying brief data/ info on companies, or companies in certain sectors, of certain sizes. Need to incorporate the Screener I’m using in algo.

[[Quantitative Finance]] -> [[Robert Carver - Systematic Trading]] -> [[076 Algo MOC]]

[[Algo Docs]]

## CLI Docs
#### Data Interaction
- [[Algo CLI Docs - DataManager]]
- [[Algo CLI Docs - DataLoader]]

#### Data Display/ Analysis
- [[Algo CLI Docs - Stock]]
- [[Algo CLI Docs - Sort]]
- [[Algo CLI Docs - Screen]]
- [[Algo CLI Docs - Quote]]
- [[Algo CLI Docs - Graph]]
- [[Algo CLI Docs - Market Profile]]

[[076 Algo MOC]]
# Algo Changelog
#projects/algo 

- 0.2.9 (Next) [[Algo Plan]]

- 0.2.1 (LATEST) - Complex multi-component strategy
	* ADDITIONS:
		* Readds old alphavantage ownership and info data
		* Overhauls data manager to make it much more robust. Now features multithreading and a much larger number of available datatypes. It is now in a somewhat “stable release format.” One minor addition plan is to add stdio input
		* *NEW STRATEGY*
			* New strategy combines EPS beats and a quality proxy
			* Currently implemented on live ALPACA (papertrading)
			* More to come, still tweaking
		* Can now take leverage into account, as well as keeping some cash available generally
	* IMPROVEMENTS:
		* Makes order orchestration/aggregation system more robust
		* Fixes operation with multiple screeners, multiple screenerrules

- 0.2.0 - Paper Trading
    * DONE:
        * Added new order orchestration system that automatically pools orders of the same ticker before placing them all at once
        * Improved reporting throughout the update and purchase/sale process
        * Created CLI tools to monitor Alpaca portfolio
        * Fixed subsystem & portfolio components to work with realtime updates.
        * Sets up brokerage account with Alpaca
        * Moving to polygon.io for data, from alphavantage
        * Implements EPS surprise strategy

- 0.1.3 - The CLI Update -
	* TODO:
		* Add websockets for streaming quotes for a single stock
		* Overhaul datamanager class. It needs to be cleaner and ideally multithreaded
		* Complete transition to polygon

- 0.1.2 - Alpaca Update - 
	* TODO:
		* Integrate Alpaca
		* Lmao
		* ???
		* Profit
	* BUGFIXES:

- 0.1.1 - Iterating on paper trading - 72eb5cda68b3049c85363bf185a2009837c40d2f
    * Some good progress on positions module
    * Currently working to make paper trading viable/ working
    * In the process of creating `positions` module to deal with open positions tracking
    * Need a reliable way to update data
    * This implementation still ignores Alpaca. Need to now do much of what I already have, but instead after pulling relevant data from Alpaca.
    * TODO: The first next step when I have internet is to get Alpaca to start updating/ monitoring an even-allocation portfolio for me. Start simple, work up.
        * Incorporate the subsystem_generator with the system

- 0.1.0 - (Initial Release) - 14c07326f5ab346fab5a7372e95a64db5e93236b

    * First release version of XCAP.ALGO
    * Target full backtests working for Portfolio and Subsystem
    * Basic CLI implemented:
        * `stock` script - helps display data about a stock
        * `graph` script - quickly graphs certain data in the terminal
        * `mp` script - display a market profile for a stock
        * `sort` script - sorter wrapper
        * `screen` script - screener wrapper
        TODO:
        * `quote` script - incomplete, needs to get latest price for a stock
    * Great saving point for main `trading_system`:
        * `forecaster`
        * `rule`
        * `instrument`
        * `portfolio`
        * `subsystem`
        * `sorter`
        * `screener`
        * `sorterrule`
        * `screenerrule`
        * `subsystem_generator`
        * `system`
[[076 Algo MOC]] | [[Algo Docs]]
# Forecaster Class Docs
#projects/algo/docs
This class loads a single instrument and rule (or multiple variations of a single rule) and creates a weighted `combined_forecast`. This makes it easy to write a single generic rule, instantiate 3 timeframe variations of it, and run them all simultaneously.

### Usage:
```
# Create objects
nmih = Instrument("NMIH")
mrr1 = MeanReversionRule(5)
forecaster = Forecaster(nmih)

# Setup logic
forecaster.add_rule(mrr1)

# Run forecaster for date range and plot results
forecaster.update_forecasts_range(datetime(2019, 1, 1), datetime(2020, 12, 31))
forecaster.combined_forecast.plot()
plt.show()
```

### Data
* `instrument` The instrument this forecaster is using
* `rules = []` A list of all `Rule` objects applied to this forecaster
* `forecasts = DF` a DF with a column for each rule, row for each date
* `combined_forecast = DF` A DF containing historical forecasts from all rules combined

### API
* `add_rule(rule)` 
    Adds the rule to forecaster
* `update_forecasts(date)`
    Runs rules and saves forecasts the `forecasts` df for a given date
* `update_combined_forecast(date)`
    Generates a single combined, weighted forecast from all individual forecasts and saves them to the `combined_forecast` df
* `update_forecasts_range(start_date, end_date)`
    Runs the two above functions for all days the instrument traded between the two dates provided (inclusive)
* `summarize_forecasts()`
    Prints mean and standard deviations of forecasts[[076 Algo MOC]] | [[Algo Docs]]
# Instrument Class Documentation
#projects/algo/docs

## Description
This class is an abstraction for a single stock. It loads all sorts of data onto itself and presents several methods for convenient data extraction.


## Usage
To instantiate a new instrument, import the module and write eg
`nmih = Instrument("NMIH")`
This will automatically try to load data for a stock with symbol “NMIH”. Note that by default, creating an instrument does not download necessary data. This must be done separately, beforehand.

## Data
Data is stored on an instance of `InstrumentData` called `data` on the instrument object.

* `info` - object, contains useful basic info for the instrument. This data comes from Polygon
* `alpha_info` - object, contains some overlapping and some useful information, from alpha vantage. Unclear which is more accurate, need to test. Some useful things are on here that aren’t on the other info, and vice versa
* `fundamental_data` - pd.DataFrame object, contains a time series with fundamentals going back as far as data can be found.
* `surprise` - pd.DataFrame object, contains a time series of past earnings estimates and surprises, based on consensus.
* `daily_bars` - pd.DataFrame object with past price history, Open, High, Low, Close, Volume, Num_trades, VWAP for each day
* `ticker` - string

###### Other
* `instrument.ticker` - string with ticker
* `instrument.daily_vol` - one standard deviation of daily returns
* `instrument.annual_vol` - one standard deviation of annual returns
* `instrument.value_vol` - one standard deviation of annual returns in dollar terms
* `instrument.block_value` - cost of one unit of an instrument (eg recent price)
* `instrument.corr_bench` - correlation with a benchmark # NOTE: THIS IS CURRENTLY COMPARING NON-STATIONARY SERIES IN A NONSENSICAL WAY. UPDATE NEEDED!!!
* `instrument.corr_other` - correlation of closes with another asset # SAME ISSUE AS ABOVE

## API
The Instrument class contains a number of handy functions for accessing key data and calculating certain statistics.

###### Helper Functions
* `date_range(start_date, end_date)`
    This method returns a list of dates the instrument traded on between the start_date and end_date inclusive
* `mkt_open(date)`
    Returns whether the instrument traded on a given day
* `last_mkt_date(date)`
    Returns most recent date instrument traded before the one provided
* `get_periods(num_periods)`
    returns the last `num_periods` periods of technical data

###### Calculator Methods
* `calculate_vol(window)`
    Calculates the daily volatility over the most recent `window` days, annualizes it, and calculates value vol. Saves them into the relevant variables
* `calculate_corr_other(other, window)`
    Calculates close price correlation with another time series. Currently assuming stationarity for non stationary (level) data
* `calculate_corr_bench(other, window)`
    Same as above but saves the result into the corr_bench variable
* `calculate_pct()`
    Calculates and saves own pct-change per period
* `buy_hold_return(start_date, end_date)`
    TODO: calculate the total return if holding the stock from the start of start_date to the end of end_date

###### Individual Data Access Methods
* `get_latest_price(date)`
    Returns price at open for a given date
* `get_closing_price(date)`
    Returns price at close for a given date
* `mkt_cap()`
    Returns latest market cap in billions to 3 dp
* `revenue(periods_ago)`
    Returns a string of the revenue from the period `periods` ago before most recent
* `income(periods_ago)`
    Returns the net income from `periods` ago
    
###### Grouped Data Access Methods
* `revenue_string(periods)`
    Returns a formatted string with revenues from the last `periods` periods
* `income_string(periods)`
    Returns a formatted string with net income for last `periods` periods
* `eps_string(periods)`
    TODO
* `surprise_string(periods)`
    TODO
* `capex_string(periods)`
    Returns a formatted string with capital expenditures for last `periods` periods
* `margin_string(periods)`
    Returns a formatted string with operating margin for last `periods` periods
* `cffo_string(periods)`
    Returns a formatted string with cash flow from operations from last `periods` periods
* `cfff_string(periods)`
    Returns a formatted string with cash flow from financing from last `periods` periods
* `cffi_string(periods)`
    Returns a formatted string with cash flow from investment from last `periods` periods
* `current_liabilities_string(periods)`
    Returns a formatted string with current liabilities from last `periods` periods
* `current_assets_string(periods)`
    Returns a formatted string with current assets from last `periods` periods
* `lt_debt_string(periods)`
    Returns a formatted string with long term debt from last `periods` periods
* `cash_string(periods)`
    Returns a formatted string with cash from last `periods` periods[[076 Algo MOC]] | [[Algo Docs]]
# Portfolio Class Docs
#projects/algo/docs

This class contains one or more screeners, and makes equal-weighted purchases of all stocks in the universe that pass the screen. It is roughly equivalent to the subsystem, except focuses on what to buy rather than when to buy. Portfolio objects should be registered with the System, where they are run from.

At the moment I’m thinking a lot about the flow of execution through the program and how to optimize things a bit. There are some crucially different things that need to happen when running in realtime vs backtest that still need to be addressed. I think the methods that handle this will need to be separated.

### Usage - backtest:
```
# Instantiate objects
tickers_list = Data.retrieve_spy_tickers()
folio = Portfolio(tickers_list)
epsrule = EPSSurpriseRule(8)
screener = Screener(tickers_list, folio.instruments)

# Register objects
screener.add_rule(epsrule)
folio.add_screener(screener)

# Run simulation
folio.update_date(datetime(2020, 1, 21))
folio.update_date(datetime(2020, 2, 20))
folio.update_date(datetime(2020, 3, 20))
folio.update_date(datetime(2020, 4, 20))
folio.update_date(datetime(2020, 5, 20))
folio.update_date(datetime(2020, 6, 22))
folio.update_date(datetime(2020, 7, 20))
folio.update_date(datetime(2020, 8, 20))
folio.update_date(datetime(2020, 9, 21))

# Outputs
print(folio.positions)
folio.total_value.plot()
plt.show()
```

### Data
* `verbose = True/False` Boolean that controls whether to print lots of intermediate output
* `screeners = []` List of screeners
* `frequency` Frequency of rebalancing (UNUSED - for now TODO)
* `universe = []` List of tickers in universe
* `instruments = {'ticker': instrument}` Mapping of tickers to instruments
* `start_cash` Starting cash amount
* `curr_cash` Current cash amount
* `last_value` Most recent total value

* `positions = DF` DF with columns for each position in the universe and rows for each date. 1 = buy, 0 = don’t
* `position_alloc = DF` DF with columns for each position in the universe and rows for each date. Indicates number of units of an instrument to own on that date
* `position_change = DF` DF with same setup as above, but indicating number to buy or sell in a period in order to align with the alloc target.
* `position_value = DF` DF indicating the value of an allocation for a given date. Same setup as DFs above.
* `total_value = DF` DF with one column indicating total portfolio value for each given day.

* `last_rebalanced` datetime for when the portfolio was last rebalanced
* `last_updated` datetime for last time update_date function was run. This way updates and rebalancing can both be configured.

### API
* `add_screener(screener)`
    Registers another screener object
* `initialize_instruments`
    Creates the ticker-instrument mapping object `instruments`
* `init_universe_df`
    Sets up all dfs that have each item in the universe as a column
* `buy(ticker, alloc, date, price)`
    Attempt to execute a buy, if there is enough cash. Will update cash and assign to `position_change`.
* `sell(ticker, alloc, date, price)`
    Same as buy, but for sell
* `set_allocation(new_allocation)`
    Resets the portfolio to use a max of “new_allocation” dollars in next rebalancing, irrespective of current value. Ratios are maintained.
    
###### Individual Time Update Methods
* `update_screeners(date)`
    Runs through each screener and applies their rules, builds the `positions` DF
* `rebalance(date)`
    MEATY method. This one figures out how much to allocate to each instrument based on the data stored in the `positions` DF. Updates the `position_alloc` DF
* `update_value(date)`
    Runs through the `position_alloc` and calculates the total value of each position each day.
* `update_date(date)`
    Runs all of the update methods for that date. TODO: Implement `frequency`!! 

###### Helper Methods
* `round_down(number, decimals)`
    Rounds down a number to decimals DP.[[076 Algo MOC]]
# Positions Class Docs
#projects/algo/docs
**Warning: Not completely implemented**
This might be a redundant component. I need to think some more before going down this road. It *may* be necessary for really deep performance tracking.

This class is designed to make managing a bunch of open positions easier. It should calculate P/L and track time-related stuff too. Might make sense to offload cash management here entirely, and have the system just try to fetch the latest cash - if it is linked into alpaca, this would return actual latest cash, if it is not, it will return whatever the latest saved value is, or smth like that.

This system works closely with the [[076 Algo MOC#Live Operation]] portion of the `System` class - [[Algo Docs - System#Live Components]]

### Data
* `positions = {"TICKER": num_held, "TICKER2": num_held2...}` Dictionary mapping to how many shares of a given stock are currently held.
* `trades = [Trade(...), Trade(...), ]` List of all trade objects registered for this system.
* `cost_basis = {"TICKER": total_cost_basis...} Does what it says on the tin.`
[[076 Algo MOC]] | [[Algo Docs]]
# Rule Class Documentation
#projects/algo/docs

This class is a very lightweight class that is really a container for the method `generate_forecast(data, current_price)`. This method takes in some window of most recent data and a quote for the latest available price, and returns a discrete value ranging from -20 to +20, where -20 is the most bearish and +20 is the most bullish. For ease of comparability, -10 and +10 are generally targeted as average sell and buy signals respectively. 

All created rules must inherit from the base Rule class. All objects are passed a window parameter at instantiation that simplifies the process of creating multiple variations of the same rule dynamically at instantiation.

### Usage:
```
mrr1 = MeanReversionRule(5)
mrr2 = MeanReversionRule(25)
```[[076 Algo MOC]] | [[Algo Docs]]
# Screener Class Docs
#projects/algo/docs 
This class applies the logic of a `ScreenerRule` to a set universe of instruments, and records which instruments pass the screen.

### Usage:
```
atvi = Instrument("ATVI")
rule = EPSSurpriseRule(8)
screen = Screener(tickers)

screen.add_rule(rule)
screen.run_all_rules(datetime(2021, 1, 20))
screen.report()
```

### Data
* `universe = []` List of all tickers in universe
* `instruments = {'ticker': instrument}` Dictionary mapping all tickers in universe to their respective instruments
* `rules = []` List of all rules applied to screener. Instruments only pass the screen if ALL rules pass (so AND logic)
* `latest_date` Most recent date run
* `trimmed_universe = []` List of tickers that pass the screen
* `universe_trimmed` Boolean for whether the screener has run
* `items_trimmed = []` List of tickers that didn’t pass the screen
* `number_trimmed` Number of instruments that didn’t pass the screen

### API
* `initialize_instruments()`
    Create the instrument object mapping
* `add_rule(rule)`
    Register another rule for screening
* `set_universe(universe)`
    Set universe for screening
* `run_all_rules(date)`
    Execute all rules for a given date, record instruments that pass all screens.
* `run_rule(rule, universe)`
    Execute rule on provided universe, returning all tickers that pass.
* `report()`
    Print out some quick report data on screen[[076 Algo MOC]] | [[Algo Docs]]
# ScreenerRule Class Docs
#projects/algo/docs 
This class is applied using Screeners, and provides access to a single core function `apply_rule(data, date)` that returns whether a given set of data and date pass or fail the screen. This function should return True or False for every possible input. Each new ScreenerRule should inherit from the parent class[[076 Algo MOC]] | [[Algo Docs]]
# Sorter Class Docs
#projects/algo/docs 
This class is pretty lightweight, and essentially operates the same as the screener does; it takes a sorterrule and applies it on a list of instruments, recording the sorted list in a variable `sorted`

/### Data/
* `universe = []` List of tickers
* `instruments = []` LIST of instruments (this differs from the implementation of screener, where a dictionary of instruments is used instead)
* `sorted = []` List of tickers in sorted order (not instruments!)
* `sorterrule` SorterRule object used for logic[[076 Algo MOC]] | [[Algo Docs]]
# Algo SorterRule Docs
#projects/algo/docs
This class is used as a generic container to contain specific sorting rules. See how the current ones are implemented to implement new ones. 

Each rule class needs to have a rule() function which returns a lambda that defines sorting logic.[[076 Algo MOC]] | [[Algo Docs]]
# Subsystem Class Docs
#projects/algo/docs
This class handles a lot of the buying/ selling logic. The subsystem contains knowledge of it’s allocated cash volatility target, and uses that to allocate capital to one or multiple forecasters, combining the results based on the volatility of the underlying instrument to provide a single integer value for the number of units of an instrument to buy or sell in a given period of time. The idea is to group all forecasters used for a single instrument within a single subsystem. 

### Usage:
```
# Instantiate objects
tcnnf = Instrument("TCNNF")
mrr1 = MeanReversionRule(5, False)
f = Forecaster(tcnnf)
subsys = Subsystem(tcnnf)

# Register objects
f.add_rule(mrr1)
subsys.add_forecaster(f)

# Run simulation
subsys.simulate_range(datetime(2019, 1, 1), datetime(2020, 12, 31))
subsys.report_performance()
subsys.positions.value.plot()
plt.show()
```

### Data
* `instrument` the instrument object used for this subsystem
* `forecasters = []` A list of all forecasters registered to this subsystem
* `daily_cash_vol_target` A dollar target for the daily cash volatility, or one standard deviation of returns in cash terms on a given day
* `start_cash` Cash the system starts with

* `trade_cost` Cost of selling position
* `slippage_factor` Percentage of slippage on buy and sell

* `curr_cash` (c) Current cash position
* `vol_scalar` (c) Volatility scalar, used for changing position sizing based on volatility. Updated based on daily cash vol target and instrument value volatility
* `latest_date` (c) Most recent date for which positions have been saved
* `latest_position` (c) Most recent combined, scaled position
* `daily_vol` (c) Calculated after full run, daily volatility
* `annual_vol` (c) Calculated after full run, annual volatility
* `sr` (c) Sharpe Ratio for combined subsys, calculated after full run
* `total_trades` (c) Total number of trades in the simulation range
* `start_date` (c) Simulation start date
* `end_date` (c) Simulation end date

* `positions = DF` A dataframe for calculating position sizing. One row per date, columns described below.
* `positions['combined']` The column containing the single combined forecast for the instrument, designed to range from -20 to 20.
* `positions['actual']` The actual number of units of the instrument to buy or sell.
* `positions['change']` The change in number of units of the instrument from the prior period, or amount to trade in other words
* `positions['traded']` 1 if a trade was made this day, 0 otherwise
* `positions['cost']` Cost of making the trade in `positions['change']`
* `positions['cash']` Cash at EOD
* `positions['value']` Total value at EOD
* `positions['value_change']` Change in value from prior day
* `positions['value_change_pct']` Change in value pct from prior day
* `positions['daily_slippage_costs']` Daily loss to slippage
* `positions['daily_trade_costs']` Daily loss to trade costs


### API

###### External Updaters
* `add_forecaster(forecaster)`
    Registers an additional forecaster object
* `update_cash_vol_target(new_cvt)`
    Updates the daily_cash_vol_target
###### Individual Timeframe Sim Methods
* `update_forecasters(date)`
    Updates all of the forecasters for a given date (ie runs `forecaster.update_forecasts(date)` and `forecaster.update_combined_forecast(date)`)
* `update_position(date)`
    Iterates through forecasters and aggregates combined forecast, saving into `positions['combined']`
* `update_inertia_position(date)`
    Calculates how much to trade given preference to not trade (will only make trades when forecast is more than 10% away from current position size. This significantly reduces trade costs)
* `calculate_position_change(date)`
    Calculates change in position (required trade amount)
* `calculate_position_cost(date)`
    Calculates the cost of making the required trades)
* `calculate_position_value(date)`
    Calculates the daily value and change of the whole subsystem
* `update_full(date)`
    Handles running all of the above individual timeframe sim methods
###### Time Range Sim Methods - these are primarily for testing, being deprecated
* `update_forecasters_range(start_date, end_date)`
    Runs `update_forecaster(forecaster)` for each forecaster for each date in the range
* `update_positions_range(start_date, end_date)`
    Runs `update_position(date)` for each date in the range
* `update_inertia_range(start_date, end_date)`
    Runs `update_inertia(date)` for each date in the range
* `simulate_range(start_date, end_date)`
    Runs all three of the above functions
###### Initialization Functions
* `initialize_inertia(start_date)`
    A helper function to initialize some fields in the DF
###### Helper Functions
* `update_vol(time)`
    Updates the vol_scalar value
* `liquidate`
    Liquidates all positions and returns to cash
* `report_performance()`
    Calculate and print a bunch of key reporting statistics[[076 Algo MOC]] | [[Algo Docs]]
# System Class Docs
#projects/algo/docs

This class gathers the final logic for all underlying subsystems and portfolios, and coordinates everything. A key aspect of the way this class works is the fact that it updates cash volatility assumptions for each subsystem, ensuring strategies are consistently rebalanced depending on their success/ failure to maintain similar allocations.

Eventually, this file will likely be the core of our trading system.

One important thing to remember is that the system requires one instrument to be passed to it at instantiation, to be used to determine when the market is open.

### Live Components
A portion of the system is used to run all components in live operation mode. This entails individual components generating their own forecasts and registering trade requests with the system. The system then pools all the trade requests and places then all at once, tracking which components are responsible for which allocation requests.

Live usage is fairly simple, and involves calling `runlive`

### Usage:
```
mkt_open = Instrument("ATVI")
sys = System(100000, 100000, mkt_open, verbose=True)

# Subsystem Setup
subsys = Subsystem(atvi, 0)
tfr1 = TrendFollowingRule(5, False)
tfr2 = TrendFollowingRule(20, False)
tfr3 = TrendFollowingRule(60, False)
f = Forecaster(atvi)
f.add_rule(tfr1)
f.add_rule(tfr2)
f.add_rule(tfr3)

subsys.add_forecaster(f)
sys.add_subsystem(subsys)

# Portfolio Setup
portfolio = Portfolio(["GNW", "NMIH", "RDN", "ESNT", "ACGL"])
screener = Screener(["GNW", "NMIH", "RDN", "ESNT", "ACGL"], portfolio.instruments)
ew_rule = EqualWeightRule(1)

screener.add_rule(ew_rule)
portfolio.add_screener(screener)
sys.add_portfolio(portfolio)

# Run System Backtest
sys.simulate_date_range(datetime(2020, 1, 2), datetime(2020, 12, 31))
sys.overall_performance.value.plot()
plt.show()
```

### Data
* `verbose` Boolean used to control when to print lots of info.d
* `subsystems = []` List of all subsystems
* `portfolios = []` List of all portfolios
* `mkt_open` An instrument used to retrieve market open days
* `subsystem_performance = DF` DF containing daily performance of each constituent subsystem. One column per subsys, one row per date
* `subsystem_allocation = DF` DF containing the allocation to each subsystem for each day. Not fully implemented yet, for now all are equal weighted
* `portfolio_performance = DF` Same but for each portfolio.
* `portfolio_allocation = DF` Same but for each portfolio
* `overall_performance = DF` DF for tracking overall performance, row for each day. Columns:
    * `overall_performance['value']` Value for a given day
    * `overall_performance['value_change']` Value change in dollars for a given day from previous day
    * `overall_performance['value_change_pct']` Value change in pct for a given day from previous day
    
###### Simulation calculated
* `tr` Total return from simulation
* `annualized_tr` TR divided by number of years
* `sr` Sharpe Ratio
* `daily_vol` Daily volatility
* `annual_vol` Annualized volatility
* `start_date` start of simulation
* `end_date` end of simulation
* `start_cash` Obvious
* `curr_cash` Current cash tracker, used during the simulation
* `daily_vol_target` Target daily volatility, mostly used for subsystems (so far. Would be nice to extend volatility targeting to portfolios)

### API
* `add_subsystem(subsystem)`
    Registers the provided subsystem for full tracking by system 
* `add_portfolio(portfolio)`
    Registers the provided portfolio for full tracking by system
* `update_cash_vol_tgts(first=False)`
    Updates the cash volatility targets and allocations to subsystems and portfolios. This is controlled mostly automatically, and simply gives equal-weighted allocations to each component, for now.
* `report_performance()`
    Prints out a nice little readout with TR, SR and volatility
* `run_system()`
	Runs the system in live mode

###### Backtesting Simulation
* `initialize_simulation(date)`
    Prepares necessary datastructures for simulation
* `simulate_date_range(start_date, end_date)`
    Runs simulate_date(date) for each of the dates between start_date and end_date after initializing simulation
* `simulate_date(date)`
    Runs system for a given date. This is a core method, used for simulation as well as soon paper trading.[[076 Algo MOC]] | [[Algo Docs]] 
# Trade
#projects/algo/docs
The trade class is a very lightweight wrapper for saving the data belonging to a single trade, for simple future perusal. The idea is to set the system up so it can easily calculate current holdings from a list of past trades. I’m not sure yet how much “rationality checking” this whole deal should have - little will make it pretty simple, but will open up lots of potential errors. For now let’s keep things simple on this front.

### Data
* `ticker` - string
* `quantity` - int
* `total_cost` - decimal
* `date` - datetime object
[[076 Algo MOC]]
# Algo Docs 0.1.0
#projects/old/xcap #projects/algo
project involves all of our explorations in the quantitative finance space. In particular we are looking at two approaches to strategies, asset allocation and diversified trading. 

The goal of the overall project is to create modular components that can work together or individually to test, plan and execute quantitative strategies. 

We are also developing a CLI (Command Line Interface) for conducting research manually, wince much of the required infrastructure is necessary for the rest of the system already.

### Table of Contents
___

- [CLI](#cli)

    - [Stock](#stock)
    
    - [Sort](#sort)
    
    - [Screen](#screen)
    
    - [Graph](#graph)
    
    - [Launchpad](#launchpad)
    
    - [DataManager](#datamanager)
    
    - [DataLoader](#dataloader)
    
- [Trading System](#tradingsystem)

    - [Instrument](#instrument)

    - [Rule](#rule)

    - [Forecaster](#forecaster)

    - [Subsystem](#subsystem)

    - [ScreenerRule](#screenerrule)

    - [Screener](#screener)
    
    - [SorterRule](#sorterrule)
    
    - [Sorter](#sorter)

    - [Portfolio](#portfolio)

    - [System](#system)
___

## CLI
The CLI is used to retrieve and display company data in a compact, efficient way. 

## `Stock`
The stock file is used to interact with stock data through the terminal. The eventual idea with this is to be able to use screeners in conjunction with this class to display all stocks in a sector or following certain `ScreenerRule`s that pick high-EPS companies

### Usage
```
>>> python3 stock.py NMIH

NMIH			$23.75		[$8.06 - $35.66]
50DMA | 200DMA:		$23.3758	$19.8911
Mkt cap:		$2.064bn

>>> python3 stock.py -fundo

FUNDAMENTAL REPORT for NMIH

EPS - last 8 quarters, recent first
Reported:		0.47	0.4	0.75	0.75	0.71	0.59	0.56	0.46
Surprise %:		33.0	4.0	13.0	6.0	14.0	8.0	7.0	0.0

INCOME STATEMENT - ($ millions)
Total Revenue:		107.3	106.7	113.6	102.3	102.7	89.5	75.6	79.8
% margin:		35.6	25.1	51.3	49.1	48.5	43.7	43.5	44.5
Net Income:		38.2	26.8	58.3	50.2	49.8	39.1	32.9	35.5

CASH FLOW - ($ millions)
CF from Ops:		87.6	79.5	47.9	68.6	52.2	59.0	28.3	46.5
CF from Fin:		-2.3	466.2	-4.9	3.7	0.2	-0.1	-1.8	0.3
CF from Inv:		-406.6	-140.1	25.8	-77.1	-42.3	-63.0-12.0	-39.7
Capex:			4.5	3.1	1.5	17.4	3.0	2.7	1.8	13.4

BALANCE SHEET - ($ millions)
Cash:			188.6	513.4	107.3	38.4	43.0	34.3	39.8	23.9
Current assets:		56.2	56.7	60.0	61.6	63.6	62.7	66.2	66.4
Current Liabilities:	59.3	104.8	20.7	39.9	39.3	24.4	17.0	31.1
Total LT debt:		403.4	405.0	158.3	160.1	146.0	146.3171.8	173.9

>>> python3 stock.py NMIH -info

NMI Holdings, Inc
Financial Services/ Insurance-Property & Casualty

NMIH		$23.75	[$8.06 - $35.66]
50DMA | 200DMA:	23.3758	19.8911
EPS:		$2.254		(-35.0% YoY)
RPS:		$5.803		(6.0% YoY)
Mkt cap:	$2.064bn
Employees:	261

VALUATION
Forward PE:	10.5708
EV to EBITDA:	0
Price to book:	1.5781
Profit Margin:	0.4051

OWNERSHIP SUMMARY
Institutional:	97.144%
Insider:	1.358%
Short % float:	4.0%
Short ratio:	5.2
```

### API
* `quote(symbol)`
    Retrieve latest quote for a ticker
* `fundamental_report(instrument)`
    Print out a fundamental report for an instrument
* `info_report(instrument)`
    Print out an informational report for an instrument
* `technical_report(instrument, timeframe)` TODO
    Print out text data about a stock's recent price & volume moves. Perhaps market profile can be added.
* `display_chart(instrument, params)`
    Display a chart of `instrument`'s price data.

[Back to top](#projects/old/xcapquant)

<br />

## `Sort`
This CLI wrapper for the sorter allows easy io for sorting tickers based on their underlying characteristics.

### Usage
```
> python3 sorter.py NMIH ESNT RDN GNW -rule=ownership
GNW RDN NMIH ESNT
```

### API
* tickers: Passed in space separated, or else in a `-tickers="T1, T2, T3..."` arg
* `-rule=` Flag: 
    * `fwd_pe` Sort by increasing forward PE ratio
    * `ownership` Sort by my proprietary ownership monitoring stat
    * `mkt_cap` Sort by market cap 
    * `margin` Sort by profit margin
* `-v` verbose output
* `-nasdaq` Use all nasdaq tickers as universe
* `-spy` Use all spy tickers as universe
* `-reverse` Reverse the sorting


[Back to top](#projects/old/xcapquant)

<br />

## `Screen`
The Screen CLI works similarly to the Sort CLI. It is a wrapper for the screener, and allows quick identification of companies that meet certain financial metrics.

### Usage
```
> python3 cli/screen.py NMIH GNW RDN ESNT -rules="eps_rule=8, max_mkt_cap=5000000000"
NMIH
```

### API
* tickers: Passed in space separated, or else in a `-tickers="T1, T2, T3..."` arg
* `-rules="` Flag: 
    * `eps_rule=x` Pass companies which surprised on EPS 
    * `max_mkt_cap=x` Pass all companies with market caps less than x
    * `min_mkt_cap=x` Pass all companies with market caps more than x
    * `equal_weight` Passes all companies, used to test and create equal-weighted portfolios
* `-v` verbose output
* `-nasdaq` Use all nasdaq tickers as universe
* `-spy` Use all spy tickers as universe
* `-reverse` Reverse the sorting


[Back to top](#projects/old/xcapquant)

<br />

## `Graph`
This component integrates termplotlib with the instrument implementation to display price graphs in the terminal.

### Usage
```
```

### API
* ``

[Back to top](#projects/old/xcapquant)

<br />

## `Launchpad`
This component provides a centralized launching point for all the other CLI services. Unless specific functionality is required, this is the best way to access general features.

### Usage
```
```

### API
* ``

[Back to top](#projects/old/xcapquant)

<br />
## `DataManager`
This crucial component of the overall system provides a command line interface to download stock data. 

### Usage
```
# Retrieve a single stock's daily technical data
>>> python3 data_manager.py NMIH
['NMIH']
Retrieving data for ['NMIH']
...

NMIH retrieved

# Retrieve multiple stocks' daily technical data
>>> python3 data_manager.py NMIH TCNNF CURLF AAPL
(output abridged)

# Retrieve all data about a stock
>>> python3 data_manager.py NMIH -all
(output abridged)

# Retrieve one type of fundamental info for a stock
>>> python3 data_manager.py NMIH [-bs | -is | -cf | -eps | -info]
(output abridged)
```

### API
* TODO
* If you want to edit the DataManager, good luck


[Back to top](#projects/old/xcapquant)

## `DataLoader`
This component is very helpful for loading all kinds of financial data. The methods are all static, with the class acting as a container for ease of use.

### Usage
```
from trading_system.data_loader import Data

price_data = Data.load_data_from_file("NMIH", "daily")

nasdaq_tickers = Data.retrieve_nasdaq_tickers()
```

### API
* `load_data_from_file(ticker, frequency, (start_date), (end_date))`
    Loads techno data on a ticker. Start and end date optional
* `load_fundamental_data_from_file(ticker, datatype)`
    Loads requested fundamental data. Datatypes are passed as strings ["is", "cf", "bs", "eps"]
* `retrieve_spy_tickers()`
    Does what it says on the tin and returns a list of all SPY tickers (as of like 3 months ago lmao)
* `retrieve_nasdaq_tickers()`
    Same but for nasdaq
* `load_ticker_instruments(ticker_list, Instrument)`
    

<br />

## Trading System
The trading system was created with the goal of maximum modularity. This allows the project to take advantage of significant abstraction and thus simplify the creation of complex, multi-rule strategies.

### Project Setup
Setting up the project is not trivial, but is doable with a bit of work.

1. Clone/ download the repository from github
    `git clone git@github.com:michaeljohncalvey/algo.git`
2. Run virtual environment (from source directory)
    `source env/bin/activate`
 
    And to deactivate: `deactivate`
3. Download stock data
    `python3 data_manager.py TICKER1 TICKER2 TICKER3...`  (see data_manager documentation for more. Need API key!)
4. Run stock CLI
    `python3 stock.py TICKER [-info|-fundo]`

[Back to top](#projects/old/xcapquant)

<br />

## `Instrument`

This class is an abstraction that uses pandas to create a datastructure to contain asset data for an individual instrument like a stock. 

To instantiate a new instrument, import the module and write eg
`nmih = Instrument("NMIH")`
This will automatically try to load data for a stock with symbol "NMIH"

### DATA
The data is stored in two main objects, `technical_data` and `fundamental_data`. 

* `technical_data` contains daily open, high, low, close, volume info.
        1. `pct` - contains pct change series

* `fundamental_data` is broken up into four components:

        1. `fundamental_data['cf']` For historic cash flow statements
        2. `fundamental_data['is']` For historic income statements
        3. `fundamental_data['bs']` For historic balance sheets
        4. `fundamental_data['eps']` For quarterly eps performance, estimates and surprises.

###### Other
* `instrument.ticker` - string with ticker
* `instrument.daily_vol` - one standard deviation of daily returns
* `instrument.annual_vol` - one standard deviation of annual returns
* `instrument.value_vol` - one standard deviation of annual returns in dollar terms
* `instrument.block_value` - cost of one unit of an instrument (eg recent price)
* `instrument.corr_bench` - correlation with a benchmark # NOTE: THIS IS CURRENTLY COMPARING NON-STATIONARY SERIES IN A NONSENSICAL WAY. UPDATE NEEDED!!!
* `instrument.corr_other` - correlation of closes with another asset # SAME ISSUE AS ABOVE

### API
The Instrument class contains a number of handy functions for accessing key data and calculating certain statistics.

###### Helper Functions
* `date_range(start_date, end_date)`
    This method returns a list of dates the instrument traded on between the start_date and end_date inclusive
* `mkt_open(date)`
    Returns whether the instrument traded on a given day
* `last_mkt_date(date)`
    Returns most recent date instrument traded before the one provided
* `get_periods(num_periods)`
    returns the last `num_periods` periods of technical data

###### Calculator Methods
* `calculate_vol(window)`
    Calculates the daily volatility over the most recent `window` days, annualizes it, and calculates value vol. Saves them into the relevant variables
* `calculate_corr_other(other, window)`
    Calculates close price correlation with another time series. Currently assuming stationarity for non stationary (level) data
* `calculate_corr_bench(other, window)`
    Same as above but saves the result into the corr_bench variable
* `calculate_pct()`
    Calculates and saves own pct-change per period
* `buy_hold_return(start_date, end_date)`
    TODO: calculate the total return if holding the stock from the start of start_date to the end of end_date

###### Individual Data Access Methods
* `get_latest_price(date)`
    Returns price at open for a given date
* `get_closing_price(date)`
    Returns price at close for a given date
* `mkt_cap()`
    Returns latest market cap in billions to 3 dp
* `revenue(periods_ago)`
    Returns a string of the revenue from the period `periods` ago before most recent
* `income(periods_ago)`
    Returns the net income from `periods` ago
    
###### Grouped Data Access Methods
* `revenue_string(periods)`
    Returns a formatted string with revenues from the last `periods` periods
* `income_string(periods)`
    Returns a formatted string with net income for last `periods` periods
* `eps_string(periods)`
    TODO
* `surprise_string(periods)`
    TODO
* `capex_string(periods)`
    Returns a formatted string with capital expenditures for last `periods` periods
* `margin_string(periods)`
    Returns a formatted string with operating margin for last `periods` periods
* `cffo_string(periods)`
    Returns a formatted string with cash flow from operations from last `periods` periods
* `cfff_string(periods)`
    Returns a formatted string with cash flow from financing from last `periods` periods
* `cffi_string(periods)`
    Returns a formatted string with cash flow from investment from last `periods` periods
* `current_liabilities_string(periods)`
    Returns a formatted string with current liabilities from last `periods` periods
* `current_assets_string(periods)`
    Returns a formatted string with current assets from last `periods` periods
* `lt_debt_string(periods)`
    Returns a formatted string with long term debt from last `periods` periods
* `cash_string(periods)`
    Returns a formatted string with cash from last `periods` periods

[Back to top](#projects/old/xcapquant)

<br />

## `Rule`
This class is a very lightweight class that is really a container for the method `generate_forecast(data, current_price)`. This method takes in some window of most recent data and a quote for the latest available price, and returns a discrete value ranging from -20 to +20, where -20 is the most bearish and +20 is the most bullish. For ease of comparability, -10 and +10 are generally targeted as average sell and buy signals respectively. 

All created rules must inherit from the base Rule class. All objects are passed a window parameter at instantiation that simplifies the process of creating multiple variations of the same rule dynamically at instantiation.

### Usage:
```
mrr1 = MeanReversionRule(5)
mrr2 = MeanReversionRule(25)
```

[Back to top](#projects/old/xcapquant)

<br />

## `Forecaster`
This class loads a single instrument and rule (or multiple variations of a single rule) and creates a weighted `combined_forecast`. This makes it easy to write a single generic rule, instantiate 3 timeframe variations of it, and run them all simultaneously.

### Usage:
```
# Create objects
nmih = Instrument("NMIH")
mrr1 = MeanReversionRule(5)
forecaster = Forecaster(nmih)

# Setup logic
forecaster.add_rule(mrr1)

# Run forecaster for date range and plot results
forecaster.update_forecasts_range(datetime(2019, 1, 1), datetime(2020, 12, 31))
forecaster.combined_forecast.plot()
plt.show()
```

### Data
* `instrument` The instrument this forecaster is using
* `rules = []` A list of all `Rule` objects applied to this forecaster
* `forecasts = DF` a DF with a column for each rule, row for each date
* `combined_forecast = DF` A DF containing historical forecasts from all rules combined

### API
* `add_rule(rule)` 
    Adds the rule to forecaster
* `update_forecasts(date)`
    Runs rules and saves forecasts the `forecasts` df for a given date
* `update_combined_forecast(date)`
    Generates a single combined, weighted forecast from all individual forecasts and saves them to the `combined_forecast` df
* `update_forecasts_range(start_date, end_date)`
    Runs the two above functions for all days the instrument traded between the two dates provided (inclusive)
* `summarize_forecasts()`
    Prints mean and standard deviations of forecasts

[Back to top](#projects/old/xcapquant)

<br />

## `Subsystem`
This class handles a lot of the buying/ selling logic. The subsystem contains knowledge of it's allocated cash volatility target, and uses that to allocate capital to one or multiple forecasters, combining the results based on the volatility of the underlying instrument to provide a single integer value for the number of units of an instrument to buy or sell in a given period of time. The idea is to group all forecasters used for a single instrument within a single subsystem. 

### Usage:
```
# Instantiate objects
tcnnf = Instrument("TCNNF")
mrr1 = MeanReversionRule(5, False)
f = Forecaster(tcnnf)
subsys = Subsystem(tcnnf)

# Register objects
f.add_rule(mrr1)
subsys.add_forecaster(f)

# Run simulation
subsys.simulate_range(datetime(2019, 1, 1), datetime(2020, 12, 31))
subsys.report_performance()
subsys.positions.value.plot()
plt.show()
```

### Data
* `instrument` the instrument object used for this subsystem
* `forecasters = []` A list of all forecasters registered to this subsystem
* `daily_cash_vol_target` A dollar target for the daily cash volatility, or one standard deviation of returns in cash terms on a given day
* `start_cash` Cash the system starts with

* `trade_cost` Cost of selling position
* `slippage_factor` Percentage of slippage on buy and sell

* `curr_cash` (c) Current cash position
* `vol_scalar` (c) Volatility scalar, used for changing position sizing based on volatility. Updated based on daily cash vol target and instrument value volatility
* `latest_date` (c) Most recent date for which positions have been saved
* `latest_position` (c) Most recent combined, scaled position
* `daily_vol` (c) Calculated after full run, daily volatility
* `annual_vol` (c) Calculated after full run, annual volatility
* `sr` (c) Sharpe Ratio for combined subsys, calculated after full run
* `total_trades` (c) Total number of trades in the simulation range
* `start_date` (c) Simulation start date
* `end_date` (c) Simulation end date

* `positions = DF` A dataframe for calculating position sizing. One row per date, columns described below.
* `positions['combined']` The column containing the single combined forecast for the instrument, designed to range from -20 to 20.
* `positions['actual']` The actual number of units of the instrument to buy or sell.
* `positions['change']` The change in number of units of the instrument from the prior period, or amount to trade in other words
* `positions['traded']` 1 if a trade was made this day, 0 otherwise
* `positions['cost']` Cost of making the trade in `positions['change']`
* `positions['cash']` Cash at EOD
* `positions['value']` Total value at EOD
* `positions['value_change']` Change in value from prior day
* `positions['value_change_pct']` Change in value pct from prior day
* `positions['daily_slippage_costs']` Daily loss to slippage
* `positions['daily_trade_costs']` Daily loss to trade costs


### API

###### External Updaters
* `add_forecaster(forecaster)`
    Registers an additional forecaster object
* `update_cash_vol_target(new_cvt)`
    Updates the daily_cash_vol_target
###### Individual Timeframe Sim Methods
* `update_forecasters(date)`
    Updates all of the forecasters for a given date (ie runs `forecaster.update_forecasts(date)` and `forecaster.update_combined_forecast(date)`)
* `update_position(date)`
    Iterates through forecasters and aggregates combined forecast, saving into `positions['combined']`
* `update_inertia_position(date)`
    Calculates how much to trade given preference to not trade (will only make trades when forecast is more than 10% away from current position size. This significantly reduces trade costs)
* `calculate_position_change(date)`
    Calculates change in position (required trade amount)
* `calculate_position_cost(date)`
    Calculates the cost of making the required trades)
* `calculate_position_value(date)`
    Calculates the daily value and change of the whole subsystem
* `update_full(date)`
    Handles running all of the above individual timeframe sim methods
###### Time Range Sim Methods - these are primarily for testing
* `update_forecasters_range(start_date, end_date)`
    Runs `update_forecaster(forecaster)` for each forecaster for each date in the range
* `update_positions_range(start_date, end_date)`
    Runs `update_position(date)` for each date in the range
* `update_inertia_range(start_date, end_date)`
    Runs `update_inertia(date)` for each date in the range
* `simulate_range(start_date, end_date)`
    Runs all three of the above functions
###### Initialization Functions
* `initialize_inertia(start_date)`
    A helper function to initialize some fields in the DF
###### Helper Functions
* `update_vol(time)`
    Updates the vol_scalar value
* `liquidate`
    Liquidates all positions and returns to cash
* `report_performance()`
    Calculate and print a bunch of key reporting statistics
    
[Back to top](#projects/old/xcapquant)
    
___________________________

<br />

## `ScreenerRule`
This class is applied using Screeners, and provides access to a single core function `apply_rule(data, date)` that returns whether a given set of data and date pass or fail the screen. This function should return True or False for every possible input. Each new ScreenerRule should inherit from the parent class

[Back to top](#projects/old/xcapquant)

<br />

## `Screener`
This class applies the logic of a `ScreenerRule` to a set universe of instruments, and records which instruments pass the screen.

### Usage:
```
atvi = Instrument("ATVI")
rule = EPSSurpriseRule(8)
screen = Screener(tickers)

screen.add_rule(rule)
screen.run_all_rules(datetime(2021, 1, 20))
screen.report()
```

### Data
* `universe = []` List of all tickers in universe
* `instruments = {'ticker': instrument}` Dictionary mapping all tickers in universe to their respective instruments
* `rules = []` List of all rules applied to screener. Instruments only pass the screen if ALL rules pass (so AND logic)
* `latest_date` Most recent date run
* `trimmed_universe = []` List of tickers that pass the screen
* `universe_trimmed` Boolean for whether the screener has run
* `items_trimmed = []` List of tickers that didn't pass the screen
* `number_trimmed` Number of instruments that didn't pass the screen

### API
* `initialize_instruments()`
    Create the instrument object mapping
* `add_rule(rule)`
    Register another rule for screening
* `set_universe(universe)`
    Set universe for screening
* `run_all_rules(date)`
    Execute all rules for a given date, record instruments that pass all screens.
* `run_rule(rule, universe)`
    Execute rule on provided universe, returning all tickers that pass.
* `report()`
    Print out some quick report data on screen

[Back to top](#projects/old/xcapquant)

<br />

## `SorterRule`
This class is used as a generic container to contain specific sorting rules. See how the current ones are implenented to implement new ones. 

Each rule class needs to have a rule() function which returns a lambda that defines sorting logic.

[Back to top](#projects/old/xcapquant)

<br />

## `Sorter`
This class is pretty lightweight, and essentially operates the same as the screener does; it takes a sorterrule and applies it on a list of instruments, recording the sorted list in a variable `sorted`

### Data
* `universe = []` List of tickers
* `instruments = []` LIST of instruments (this differs from the implementation of screener, where a dictionary of instruments is used instead)
* `sorted = []` List of tickers in sorted order (not instruments!)
* `sorterrule` SorterRule object used for logic


[Back to top](#projects/old/xcapquant)

<br />

## `Portfolio`
This class contains one or more screeners, and makes equal-weighted purchases of all stocks in the universe that pass the screen. It is roughly equivalent to the subsystem, except focuses on what to buy rather than when to buy. Portfolio objects should be registered with the System, where they are run from.

### Usage:
```
# Instantiate objects
tickers_list = Data.retrieve_spy_tickers()
folio = Portfolio(tickers_list)
epsrule = EPSSurpriseRule(8)
screener = Screener(tickers_list, folio.instruments)

# Register objects
screener.add_rule(epsrule)
folio.add_screener(screener)

# Run simulation
folio.update_date(datetime(2020, 1, 21))
folio.update_date(datetime(2020, 2, 20))
folio.update_date(datetime(2020, 3, 20))
folio.update_date(datetime(2020, 4, 20))
folio.update_date(datetime(2020, 5, 20))
folio.update_date(datetime(2020, 6, 22))
folio.update_date(datetime(2020, 7, 20))
folio.update_date(datetime(2020, 8, 20))
folio.update_date(datetime(2020, 9, 21))

# Outputs
print(folio.positions)
folio.total_value.plot()
plt.show()
```

### Data
* `verbose = True/False` Boolean that controls whether to print lots of intermediate output
* `screeners = []` List of screeners
* `frequency` Frequency of rebalancing (UNUSED - for now TODO)
* `universe = []` List of tickers in universe
* `instruments = {'ticker': instrument}` Mapping of tickers to instruments
* `start_cash` Starting cash amount
* `curr_cash` Current cash amount
* `last_value` Most recent total value

* `positions = DF` DF with columns for each position in the universe and rows for each date. 1 = buy, 0 = don't
* `position_alloc = DF` DF with columns for each position in the universe and rows for each date. Indicates number of units of an instrument to own on that date
* `position_change = DF` DF with same setup as above, but indicating number to buy or sell in a period in order to align with the alloc target.
* `position_value = DF` DF indicating the value of an allocation for a given date. Same setup as DFs above.
* `total_value = DF` DF with one column indicating total portfolio value for each given day.

### API
* `add_screener(screener)`
    Registers another screener object
* `initialize_instruments`
    Creates the ticker-instrument mapping object `instruments`
* `init_universe_df`
    Sets up all dfs that have each item in the universe as a column
* `buy(ticker, alloc, date, price)`
    Attempt to execute a buy, if there is enough cash. Will update cash and assign to `position_change`.
* `sell(ticker, alloc, date, price)`
    Same as buy, but for sell
* `set_allocation(new_allocation)`
    Resets the portfolio to use a max of "new_allocation" dollars in next rebalancing, irrespective of current value. Ratios are maintained.
    
###### Individual Time Update Methods
* `update_screeners(date)`
    Runs through each screener and applies their rules, builds the `positions` DF
* `rebalance(date)`
    MEATY method. This one figures out how much to allocate to each instrument based on the data stored in the `positions` DF. Updates the `position_alloc` DF
* `update_value(date)`
    Runs through the `position_alloc` and calculates the total value of each position each day.
* `update_date(date)`
    Runs all of the update methods for that date. TODO: Implement `frequency`!! 

###### Helper Methods
* `round_down(number, decimals)`
    Rounds down a number to decimals DP.

[Back to top](#projects/old/xcapquant)

___________________________

<br />

## `System`

This class gathers the final logic for all underlying subsystems and portfolios, and coordinates everything. A key aspect of the way this class works is the fact that it updates cash volatility assumptions for each subsystem, ensuring strategies are consistently rebalanced depending on their success/ failure to maintain similar allocations.

Eventually, this file will likely be the core of our trading system.

One important thing to remember is that the system requires one instrument to be passed to it at instantiation, to be used to determine when the market is open.
### Usage:
```
mkt_open = Instrument("ATVI")
sys = System(100000, 100000, mkt_open, verbose=True)

# Subsystem Setup
subsys = Subsystem(atvi, 0)
tfr1 = TrendFollowingRule(5, False)
tfr2 = TrendFollowingRule(20, False)
tfr3 = TrendFollowingRule(60, False)
f = Forecaster(atvi)
f.add_rule(tfr1)
f.add_rule(tfr2)
f.add_rule(tfr3)

subsys.add_forecaster(f)
sys.add_subsystem(subsys)

# Portfolio Setup
portfolio = Portfolio(["GNW", "NMIH", "RDN", "ESNT", "ACGL"])
screener = Screener(["GNW", "NMIH", "RDN", "ESNT", "ACGL"], portfolio.instruments)
ew_rule = EqualWeightRule(1)

screener.add_rule(ew_rule)
portfolio.add_screener(screener)
sys.add_portfolio(portfolio)

# Run System Backtest
sys.simulate_date_range(datetime(2020, 1, 2), datetime(2020, 12, 31))
sys.overall_performance.value.plot()
plt.show()
```

### Data
* `verbose` Boolean used to control when to print lots of info.d
* `subsystems = []` List of all subsystems
* `portfolios = []` List of all portfolios
* `mkt_open` An instrument used to retrieve market open days
* `subsystem_performance = DF` DF containing daily performance of each constituent subsystem. One column per subsys, one row per date
* `subsystem_allocation = DF` DF containing the allocation to each subsystem for each day. Not fully implemented yet, for now all are equal weighted
* `portfolio_performance = DF` Same but for each portfolio.
* `portfolio_allocation = DF` Same but for each portfolio
* `overall_performance = DF` DF for tracking overall performance, row for each day. Columns:
    * `overall_performance['value']` Value for a given day
    * `overall_performance['value_change']` Value change in dollars for a given day from previous day
    * `overall_performance['value_change_pct']` Value change in pct for a given day from previous day
    
###### Simulation calculated 
* `tr` Total return from simulation
* `annualized_tr` TR divided by number of years
* `sr` Sharpe Ratio
* `daily_vol` Daily volatility
* `annual_vol` Annualized volatility
* `start_date` start of simulation
* `end_date` end of simulation
* `start_cash` Obvious
* `curr_cash` Current cash tracker, used during the simulation
* `daily_vol_target` Target daily volatility, mostly used for subsystems (so far. Would be nice to extend volatility targeting to portfolios)

### API
* `add_subsystem(subsystem)`
    Registers the provided subsystem for full tracking by system 
* `add_portfolio(portfolio)`
    Registers the provided portfolio for full tracking by system
* `update_cash_vol_tgts(first=False)`
    Updates the cash volatility targets and allocations to subsystems and portfolios. This is controlled mostly automatically, and simply gives equal-weighted allocations to each component, for now.
* `report_performance()`
    Prints out a nice little readout with TR, SR and volatility

###### Backtesting Simulation
* `initialize_simulation(date)`
    Prepares necessary datastructures for simulation
* `simulate_date_range(start_date, end_date)`
    Runs simulate_date(date) for each of the dates between start_date and end_date after initializing simulation
* `simulate_date(date)`
    Runs system for a given date. This is a core method, used for simulation as well as soon paper trading.

<!-- {BearID:84FA12A4-6C47-403A-B3E4-B55831F8C172-616-000001C436513E60} -->
[[075 Quantitative Finance MOC]] | [[055 Coding TOC]] | [[076 Algo MOC]]
# Algo Docs
#projects/algo/docs

## Core Classes
[[Algo Docs - Instrument]]
[[Algo CLI Docs - DataManager]]

## Trading System
 [[076 Algo MOC#Trading System]]
   - [[Algo CLI Docs]]
   - [[Algo Docs - System]]
	    - [[Algo Docs - Portfolio]]
			- [[Algo Docs - Screener]]
				- [[Algo Docs - ScreenerRule]]
			- [[Algo Docs - Sorter]]
				- [[Algo Docs - SorterRule]]
	    - [[Algo Docs - Subsystem]]
			- [[Algo Docs - Forecaster]]
				- [[Algo Docs - Rule]]
    - [[Algo Docs - Instrument]]

## CLI
![[Algo CLI Docs#CLI Docs]][[071 XCAP MOC]] | [[076 Algo MOC]]
# Algo ML
#projects/algo #projects/old/xcap 

## Fundamental Analysis
[[CS451 Machine Learning]]
* Trying to predict fundamental quality programmatically
* Would be really interesting to track how this changes over time, and whether there are any features associated with temporal shifts - does anything typically precede improving scores?
* Need to do an analysis on my labels
* [[076 Algo MOC]]
# ALGO project plan
#projects/algo


*Next Steps:*
* Project-wide testing
	* I have some more specific thoughts on where to start for this. Realistically, the way I've approached this in the past is nearing a [[Mental Squeeze Point]], and the complexity is getting away from me slightly. It's hard to know when distant things break, and I need to incorporate a proper testing framework.
* Pursuing ML strategies
	* Sentiment Analysis
	* Fundamental Analysis
	* Technical Analysis
* Exploring volatility
	* How to measure?
	* How to change our behaviour?
		* I've had some more interesting thoughts on modeling and measuring volatility in novel ways. I want to play with them a bit, do some charting and analysis. I've been thinking of incorporating different metrics mesides just price change. Perhaps things like volume for example, as well as just price changes over some given period of time. -> [[Thoughts on Volatity]]
* Modelling the current market state as a random process? 
* Measure / account for strategy skew.
* Drawdown analysis
* Better live reporting
	* Active Components
	* Overall System current state/ operation


[[Algo Changelog]][[076 Algo MOC]]
# Algo Project Reflection
#projects/algo 

The algo development process really slowed down, and it happened exactly when I tried to induct other people into the project. I spent a lot of useful time teaching others about how it worked, when I should really have been bringing it to completion. Noone else get's excited enough about this shit, I've honestly found very few people in life who genuinely get excited enough about the development process to commit to stuff enough for me to want to work with them. Hasan, Tayo and Henmo are probably unique in that sense. They were always ready to **Fookin Grind *TM* **

I gotta take away my reflections into the future though. That's what really matters. So I need to create a more streamlined way to plan what I want to work on step by step, in a way that contributes as much as possible per unit time to my end goal. In fact, I should spend *much* more time in the planning phase. The vagueness of my [[Algo Plan]] is telling enough. In fact, it isn't just telling, it's embarrassing. I *want* to spend my time working on this thing. I need to create a better, more structured way of planning and then following through on said plans. The plan so far was too vague, and I didn't really link it back to larger goals. 

I did like what I had going on in the changelogs. I think I should just be a little bit more intentional about **where I'm headed.** I should maybe even try to divide that up into several levels of goals, in fact, 3 seems perfect:
- Major goal, first digit
	- End goal, huge things I want to accomplish
- Medium goal, second digit
	- Significant goal along the way to achieving the end goal. This may not be very actionable on it's own, but should link together related tasks that[[076 Algo MOC]]
# Algo Strategies
#projects/algo


* *Quality proxy*
	* Ownership rule
		* `ownership = institutional + insider - shorted`

* *Value Proxy*
	* EPS beat strategy
	* Book value strategy
		* Company health/ leverage status?[[006 School MOC]]
# Algorithms And Complexity
#school/algorithms-and-complexity
#school

#### Office Hours:
	* Mon: 12-1
	* Wed: 10-11
	* Thur: 9:30-10:30

## Deadlines:
	* Programming Assignment 1: March 22
	* Midterm 1: Self scheduled from March 13-15

## General
	+ Gather all deadlines here in the todo list
	- Make a single list of all topics for studying, broken down by D&C, Greedy, and Dynamic[[011 Mental Models MOC]] | [[053 Computer Science TOC]] | [[051 Math TOC]] | [[052 Physics TOC]]
# Algorithms
#concepts #🌱

## Basic Types

### Divide and Conquer
### [[Greedy Algorithms]]

## Complexity
Complexity can be measured and compared in different dimensions.
### Query Complexity
### Time Complexity# Alphavantage
<!-- #finance/algo -->
[[Quantitative Finance]]
API key: 44CY2ULTHM5R18A5
new_api key: OGD4LRILXNOIRBAQ

<!-- {BearID:943F116C-D481-4EE0-AF13-0D6B20A85E24-631-0000544E69C20B4D} -->
# Amazon Investment Thesis
<!-- #finance/xcap/Investments -->
Amazon is a very powerful company. They are currently dominating retail and e-commerce in America and a number of other markets. We believe Amazon’s strength comes from its ability to leverage its Prime loyalty platform to generate sales on its retail platform.

<!-- {BearID:E70C73C3-3761-40E9-9683-25C415097D83-406-00000247DE4BB3A8} -->
[[Linguistics MOC]]
# Ambiguous Sentences
#concepts/linguistics 

[[Syntax]] creates potential pitfalls, and doesn't always help us understand meaning. [[Greek Philosophy]]
# Aristotle
#concepts 

- Catharsis
- Tragedy/ epic

* Aristotle thought a single, unified plot was the best kind of plot - stern aristotelians do /not/ write romance. Had an impact on Tasso when he wrote [[Liberation of Jerusalem]], explains why Tasso is somewhat interested in writing a poem with a moral lesson.
[[010 Mind MOC]] | [[011 Mental Models MOC]]
# Associative Machine
#concepts #concepts/mental_models #🌱 


The associative machine theory of the mind purports that the mind creates meaning by building links between numerous layers of associations. Links can be positive or oppositional.

This is a really powerful concept. It underlies CS topics like [[Neural Networks]] and [[Machine Learning TOC|Machine Learning]], and is heavily relevant to my thinking on [[Decision Machines]]. I need to spend a bunch more time processing these ideas and linking to other disciplines.

There's a chapter on this in [[Kahneman - Thinking Fast and Slow]]![[051 STEM TOC]] 
# Atmosphere
#concepts #🌱 
![[Pasted image 20210407113714.png]]
Ozone causes heating after 50k feet!!---
aliases: ["Atomic", "Atom"]
---
[[Standard Model TOC]] | [[052 Physics TOC]]
# Atoms
#concepts 

* Atoms are composed of a nucleus ([[Protons]] & [[Neutrons]]) and a surrounding cloud of [[Electrons]].
* The number of protons in the nucleus is the *atomic number* which is equal to the number of electrons for that atom.
* Changing the number of neutrons creates [[Isotopes]] of given [[Elements]]
* [[Radiation]] is emitted from nuclei that explode due to [[Radioactivity]], often as [[Nuclear Fission]] or [[Nuclear Fusion]]



The nucleus is about 100,000 times smaller than the atom itself.

A human hair is about 125,000 atoms wide, red blood cells are about 40,000 atoms in diameter.[[011 Mental Models MOC]] | [[054 Neuroscience MOC]]
# Attention
#🌲 #concepts 

I really want to think more about the role of attention in [[Cognitive Neuroscience]] and [[Learning]]
[[Heuristics and Biases]] | [[Heuristics TOC]]
# Availability Heuristic
#concepts/mental_models 

With this heuristic, people make decisions based on the first example of something that comes to mind. It's probably pretty closely linked to [[Recency Bias]], although this heuristic can probably more accurately be described as resulting in that bias.[[Machine Learning TOC]] | [[ML Models TOC]] | [[Linear Models]]
# Averaged Perceptrons
#concepts/cs/ml 

Averaged perceptrons are really just collections of perceptrons trained on the same dataset shuffled multiple times that vote to determine optimal outcome. This smooths out some [[Overfitting]].

## Advantages
- More smoothed output compared to regular [[Perceptrons]]
- Help avoid random data orderings from impacting results as much

## Implementation
```
def train_averaged_perceptron(
    y, X, y_vali, X_vali, num_iter=100, seed=1231
) -> LinearModel:
    rand = np.random.default_rng(seed)
    (num_examples, num_features) = X.shape
    assert len(y) == num_examples
    w_avg = np.zeros((num_features, 1))
    b_avg = 0.0
    w = np.zeros((num_features, 1))
    b = 0.0
    current_correct = 0
    indices = list(range(num_examples))
    for iteration in range(num_iter):
        rand.shuffle(indices)
        wrong = 0
        for i in indices:
            if y[i]:
                y_val = 1
            else:
                y_val = -1

            x_i = X[i, :].reshape((num_features, 1))

            activation = np.dot(w.transpose(), x_i) + b
            if y[i] != (activation > 0):
                # update 'average' vector:
                w_avg += current_correct * w
                b_avg += current_correct * b
                current_correct = 0
                # update 'current' vector
                wrong += 1
                # we got it wrong! update!
                w += y_val * x_i
                b += y_val
            else:
                current_correct += 1
        if wrong == 0:
            break
        tmp = LinearModel(w_avg, b_avg)
        learning_curves["Averaged-Perceptron"].add_sample(tmp, X, y, X_vali, y_vali)
    return LinearModel(w_avg, b_avg)
```
See [[Linear Models#Implementation]] and [[Perceptrons#Implementation]]# Back For More
[[Playlists TOC]]
<!-- #_reference/music -->

## Track list
Nyaga
Downbound Train
Dos Gardenias
Sunrise
Qué Pretendes
Mojaita
Controller
Roses
winona
slap city
metazoa
masterblaster 200
highway 71
Right place wrong time
Neighborhood #1
Wake up
Rebellion
Damn girl
Dreams
Boy Racers
The return
The london
Voices
let go
The battlefield
Get over it
My generation
Back where i belong
Otis
Don’t look back in anger
Fire
Move on up
Gimme all your loving
Almost ready
Born to lose
Bare feet on wet grippe
summertime clothes
Disorder
The gravy
Punching goodbye out front
For energy infinite
Sleeper hold
Where wolves drink 
London Dungeon
Pillar of salt
the long run
So many nights
Donald trump
The weight (aretha)
chain of fools
until the end
love train
keep on growing

<!-- {BearID:F97C2509-416B-47F1-B067-A5878997B0A1-1476-0000A812C6D35024} -->
[[Machine Learning TOC]]
# Bagging
#concepts/cs/ml 

**Bootstrap AGGregatING**

See also: [[Boosting]], [[Bootstrap]]

Bagging is a technique in [[Machine Learning TOC|Machine Learning]] to improve the accuracy of models by combining multiple models into one single weighted models

- Classification: Have a bunch of different models vote on the correct answer
- Regression: Take the mean of predictions

## Bagging vs Individual Models

### Pros
- Helps minimize overfitting
- Reduces variance of predictions or hyperparameters
- Can learn subsamples in parallel

### Cons
- 100x slower to predict
- Lose interpretability

## Examples
[[Random Forest]][[Statistics TOC]]
# Basketball statistical test

## Solal
001001000000011



## Mike
000101100000100[[Quantum Mechanics TOC]] | [[Quantum Gates]]
# Beam Splitter
#concepts 
A beam splitter is a partially reflective mirror that keeps either just the horizontal or just the vertical part of the qubit.

Transmission depends on polarization. Photons enter from left and top, horizontally polarized photons continue in their original direction of travel, vertically polarized photons travel orthogonally to their original arm direction.

A beamsplitter does not conduct a [[Quantum Measurement]] and has no effect on a [[Photons]] polarization.

$\ket{0}_P=$ photon polarization
$\ket{0}_A=$ arm polarization

#### Beamsplitter effect on [[Quantum States#Orthonormal Bases]]
| Input State          | Input description             | Output State         |
| -------------------- | ----------------------------- | -------------------- |
| $\ket{0}_P\ket{H}_A$ | Horiz polarization, Horiz arm | $\ket{0}_P\ket{H}_A$ |
| $\ket{0}_P\ket{V}_A$ | Horiz polarization, vert arm  | $\ket{0}_P\ket{V}_A$ |
| $\ket{1}_P\ket{H}_A$ | Vert polarization, horiz arm  | $\ket{1}_P\ket{V}_A$ |
| $\ket{1}_P\ket{V}_A$ | Vert polarization, vert arm   | $\ket{1}_P\ket{H}_A$ |

#### Beamsplitters as [[Matrices]]
$$
\begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix}\begin{pmatrix}1 \\ 0 \\ 0 \\0 \end{pmatrix}=\begin{pmatrix}1 \\ 0 \\ 0 \\0 \end{pmatrix}
$$

Essentially applies a not if second qubit is a 1! See CNot in [[Quantum Gates]].# Bearish JKS Spread
<!-- #finance/xcap/options #finance/xcap/Investments -->
[[Options Strategies]]
[[XCAP Current Positions]]

Long $80 put @ 12.85
	* delta = -0.3586
Short $60 put @ 4.40
	* delta = -.1641

Total position delta: .1935 
Representing ~19 short shares in the range?

<!-- {BearID:0565B4D7-F559-46BA-A93A-401FF2168964-37181-0003CE5EDA34A56D} -->
# Beer

# IPAs
- Punk IPA
-

<!-- {BearID:7A8FBC6E-3340-4293-8035-0FE13922BF78-1014-0000010E568C1858} -->
# Bell’s Theorem
#concepts/physics #concepts/cs/quantum 
This is one if the most important ideas in quantum mechanics that shows us that some of our fundamental assumptions about the workings of quantum particles are contradictory when considered in unison. The result of this thought experiment (and multiple following physical experiments) has been the creation of several running interpretations of quantum mechanics. The theorem shows that our understanding of quantum mechanics cannot be reconciled with [[Classical Mechanics]].

The theorem builds on ideas introduced in the original [[EPR Paradox]]

## Two important assumptions:
* Locality
	* Hidden variables can only work on a local level, not once entangled particles are separated. 
	* see the Bohm interpretation of quantum mechanics.	
* Realism
	* all particles in fact have an existing “true” state, regardless of whether particles are observed

## Interpretations
* Non local hidden variables
	* this interpretation states that there is some method for transfer of information between entangled, or possibly all, particles, informing each other of their respective states. This is the *Bohmian interpretation* of quantum mechanics. Essentially locality assumption is broken. I definitely need a page on the Bohmian interpretation specifically.
* Transactional interpretation 
	* This interpretation states that *hidden variables* are able to communicate with each other faster than light, potentially even backwards in time along their *light cones*.
* Many worlds interpretation
	* local and deterministic, stating that when the particles are separated there is no longer a single true outcome, and observations force a decision that may not reflect the truth. See *stochastic processes* for more on this. This interpretation states that bell’s theorem is not a paradox, but rather a proof that the reality assumption cannot be valid. 
* Superdeterminism
	* One of the possible ways to satisfy the paradox is the idea that everything in the universe is fully deterministic down to the quantum level. Bell states:
# Bene
[[Playlists TOC]]
<!-- #_reference/music -->

## Track List
What’s love got to do with it
because you move me
simmer down
casper
bad sneakers
let go
try a little tenderness
tupelo honey
just dropped in
going up the country
cool out son
cracklin rosie
about a tree
lets make up
a hymn
let it rain
chain of fools
nairobi
the deer hunter (JMT)
New shoes
Midnight train

<!-- {BearID:F5501BDA-393B-49A1-BA0E-1890FF68E367-1476-0000A8503BF3186C} -->
[[Machine Learning TOC]]
# BERT Models
#🌱 
#todo 
- [x] Research this!!!

[Article on BERT models](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
# Bickback
[[Playlists TOC]]
<!-- #_reference/music -->


## Track List
Hey Ma
Kiss me
House arrest
A vida e agora
life can be so beautiful
mr saxobeat
johnny b goode
Rocket 69
Closed on sunday
Mediterranean
1539 N. Calvert
Unz Unz
Ottolengthi
Into the night
white tiger
bounce out with that
Rubbin off the paint
Malamente
The Charmer
Andy is coming
Sum 2 prove
All it takes
Simmerdown
The battlefield
Savagely attack
Because you move me

<!-- {BearID:F6C0AA91-218C-436B-833B-CA49279F576A-1476-0000A890EC9AE8C8} -->
# Binary Classification
#concepts 
[[Machine Learning TOC]][[CS451 Machine Learning]]

* Trying to answer a yes/ no question
This is a type of [[Decision Trees]]

# Implementation
``` python
# Decision Trees: Feature Splits
# Python typing introduced in 3.5: https://docs.python.org/3/library/typing.html
from typing import List
from dataclasses import dataclass
from shared import TODO

@dataclass
class DataPoint:
    temperature: float
    frozen: bool

    def secret_answer(self) -> bool:
        return self.temperature <= 32

    def clone(self) -> "DataPoint":
        return DataPoint(self.temperature, self.frozen)

data = [
    # vermont temperatures; frozen=True
    DataPoint(0, True),
    DataPoint(-2, True),
    DataPoint(10, True),
    DataPoint(11, True),
    DataPoint(6, True),
    DataPoint(28, True),
    DataPoint(31, True),
    # warm temperatures; frozen=False
    DataPoint(33, False),
    DataPoint(45, False),
    DataPoint(76, False),
    DataPoint(60, False),
    DataPoint(34, False),
    DataPoint(98.6, False),
]

def is_water_frozen(temperature: float) -> bool:
    """
    This is how we **should** implement it.
    """
    return temperature <= 32


# Make sure the data I invented is actually correct...
for d in data:
    assert d.frozen == is_water_frozen(d.temperature)

def find_candidate_splits(data: List[DataPoint]) -> List[float]:
    midpoints = []
        # sort by temperature
    # Loop through looking at two at a time
    # Return all the midpoints
    # all_snapshots = sorted(all_snapshots['tickers'], key=lambda snapshot: snapshot['ticker'])
    sorted_data = sorted(data, key=lambda datapoint: datapoint.temperature)

    for i in range(len(sorted_data)-1):
        left = sorted_data[i]
        right = sorted_data[i+1]
        mid = (left.temperature + right.temperature) / 2.0
        midpoints.append(mid)

    return midpoints


def gini_impurity(points: List[DataPoint]) -> float:
    """
    The standard version of gini impurity sums over the classes:
    """
    p_ice = sum(1 for x in points if x.frozen) / len(points)
    p_water = 1.0 - p_ice
    return p_ice * (1 - p_ice) + p_water * (1 - p_water)
    # for binary gini-impurity (just two classes) we can simplify, because 1 - p_ice == p_water, etc.
    # p_ice * p_water + p_water * p_ice
    # 2 * p_ice * p_water
    # not really a huge difference.


def impurity_of_split(points: List[DataPoint], split: float) -> float:
    smaller = []
    bigger = []


    for pt in points:
        if pt.temperature > split:
            bigger.append(pt)
        else:
            smaller.append(pt)
    TODO("split the points based on the candidate split value")

    return gini_impurity(smaller) + gini_impurity(bigger)


if __name__ == "__main__":
    print("Initial Impurity: ", gini_impurity(data))
    print("Impurity of first-six (all True): ", gini_impurity(data[:6]))
    print("")
    for split in find_candidate_splits(data):
        score = impurity_of_split(data, split)
        print("splitting at {} gives us impurity {}".format(split, score))
        if score == 0.0:
            break

```

<!-- {BearID:FFAD1C93-6B95-44F0-990C-95B7FBCC44A1-405-0000A4C3F0300F3F} -->
[[011 Mental Models MOC]]
# Biologically Innate Human Tendencies
#concepts/mental_models 
[[300 Inputs MOC]] [[063 Economics TOC]] [[070 Finance MOC]]
# Blockchain in the Liberal Arts? - Professor F. Van Gansbeke
[[Frank Van Gansbeke]]

Middlebury Blockchain Student Committee

Labor vs Capital 
Very opposing
20m people threatened

_Some of the Fault Lines:_
FANGs monopolize content prices
QE problems
Share buybacks

Populist regimes promising dignity

Main characteristics:
- Ledger
- Distributed
- Decentralized
- Immutable

## Real world potential
- Enabling smart contracts - conditions that need to be met for a transaction to be valid
- Establishing transparent p2p comms
- Promoting dynamic efficient prices - Social media/ adtraffic etc
- Allowing micro metering/ micro monetizing - IOT
- Establishing a reputation system 

### Democracy.earth
Potential solution for voting fraud 

### regen.network
issuing carbon credits and natural resource monitoring

## My Ideas
### Blockchain crowdsourced supply lines for food distribution

## Solving problems
### How can we assure good value?
Issuing utility tokens
prize discovery/ adaptation
A platform must get big enough before it is accepted
It is when people start to band together against FANGesque companies that we see potential. Need to ride that populist backlash babyyy

<!-- {BearID:FCD46113-7C04-4E8C-83AF-06C9D5359B69-406-00002119E516198B} -->
[[071 XCAP MOC]]
# Board Member Selection
There will be an interview process for board member selection. I will propose a board composed of 4 seats
*Chairman:* Michael Calvey
**

<!-- {BearID:9263255F-1C59-4A76-A073-1F6737C328D8-303-000005F2E77DE759} -->
[[071 XCAP MOC]]
# Bobadilla Closing Letter

Dear Sergio,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |          |
| -------------------- | -------- |
| Amount Invested      | $350.00  |
| Shares Purchased     | 323.06   |
| Cash-out Share Price | $2.16    |
| Cash-out Value       | $ 697.80 |
| Return on Investment | 99.37%   |

- Amount Invested: $350.00
- Shares Purchased: 323.06
- Cash out share price: $2.16
- Cash out Value: $697.80
- Investment Return: 99.37%

Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL[[XCAP Writing]]
# Boeing Investment Thesis
#projects/old/xcap
[[Machine Learning TOC]]
# Boosting
#concepts/cs/ml 

See also: [[Bagging]]

Boosting is a [[Machine Learning TOC|Machine Learning]] technique we can use to improve models' predictions. It is about trying to fix errors in a model's predictions.

- Regression: Sum up the predictions of the model so far. Train a new model on the residuals and repeat.
- Classification: Weight the instances you have, and increase weight when you gget them wrong. Decrease their weight when you get them correct. Train on those weights.[[054 Biology MOC]] | [[054 Neuroscience MOC]] | [[056 Psychology TOC]]
# Brain
#concepts/biology 

This is a core note. For now I'm not working here, look at links to this note.

## Parts of the Brain
[[Visual Cortex]]
[[Hippocampus]]

## Theoretical Components
[[Executive Function]]

Links:
[[Narrative Instinct]]
[[Energy Saving Brain]]
[[Hassabis - Neuroscience Inspired AI]]
[[Dunbar's Number]]
[[Learning Algorithms]]
[[Reading List TOC]]
[[What we are]]
[[Interesting quote from reddit]]
[[001 Meta MOC]]
# Bridge Notes
#concepts #concepts/definition

## Bridge Note Definition
Bridge notes are designed to connect two or several tigntly connected concepts. More specific than [[TOCs]], but less atomic than individual [[Concept Notes|Concepts]]. 

Bridges should form and live organically, also being [[Evergreen Notes]]. As they grow and consolidate, they may become [[Table of Content|TOCs]] or even [[Map of Content|MOCs]]

## Example Bridge Notes
[[General Observations - Laws]]
# Brooks's Law
#inputs/quotes 

Adding manpower to a late project will make it even later

Brook acknowledges that this is a gross simplification, yet there is truth to the adage.

1. **Ramp up time** - it takes some time for new people to familiarize themselves with preceding work
2. **Communication overhead** - for each person added, communication overheads increase in a [[Polynomial]] fashion. This is an example of [[Combinatorial Explosion]][[011 Mental Models MOC]]
# Bubbles
#concepts #🌲 

# Ray Dalio’s bubble indicators:
* How high are prices relative to traditional measures?
* Are prices discounting unsustainable conditions?
* How many new buyers?
* How broadly bullish is sentiment?
* Are purchases being financed by leverage?
* Have buyers made exceptionally extended forward purchases (ie building inventory or contracting forward purchases) to speculate or insulate against future price gains?
# Budget
#my/finances 
[[080 Personal Finance MOC]]
[[Subscriptions]]

Weekly spend: $250
- Food
	- Eating out: $50
	- Groceries: $40
		- milk + cereal
		- Yoghurt
		- Chia
		- Bananas
- Drink
	- 2x 6 packs of beer: $12
	- 3 bar pints: $15
- Transportation 
	- uber to/from work: $80
- Discretionary: $43

[[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# CHSH Game
#concepts 
Problem: winning a collaborative game
Referee, Ahmed and Bei

A & B cannot communicate, but can strategize ahead of time

* Referee sends a bit to each  x, y in {0, 1} (question)
* A & B respond with answers a, b in {0, 1}
* A and B win when  `a(xor)b = x(AND)y`

p	q	xor	and
0	0	0	0
0	1	1	0
1	0	1	0
1	1	0	1

Q: What is A & B’s best strategy if ref chooses x, y randomly
a = b = 0
[[052 Physics TOC]] | [[053 Computer Science TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Gates]]
# CNOT Gates
#concepts/cs/quantum 
CNOT gates are essential components of [[Gate Based Quantum Computers]].

### Operation
CNOT gates flip the target qubit if and only if the control qubit is $\ket{1}$

### Algebra Representation
Transforms state:
$$
a\ket{00}+b\ket{01}+c\ket{10}+d\ket{11}
$$
Into:
$$
a\ket{00}+b\ket{01}+c\ket{11}+d\ket{10}
$$

### Matrix Representation
$$
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{pmatrix}
$$
This is a [[Permutation Matrix]][[006 School MOC]]
#school #school/cs 
# CS Major Plan
<!-- #school/algorithms-and-complexity #Important -->
Interdisciplinary track with honors:
+ 1xx
+ 200
+ 201
+ 202
+ 302
- El1 (LSE)
- El2 (LSE)
- El3 (Fall)
- El4 (Fall)
- El5 (Fall)
- El6 (Spring)
- 701 (Spring)



need to write a proposal with links to syllabi for the courses I plan to count
ID track with 4 year long courses in London, be clear about overlap. I want to count courses x and y as two CS electives.

<!-- {BearID:F72E596A-4375-46D2-8AEC-44D710FEFB55-314-00004C7BF051FF39} -->
[[006 School MOC]]
#school #school/cs
# CS Midterm 2
# 1
a)
A = {a^n b^2n c^k | k,n ≥ 1} 
In this set, we know that there should be some non-zero number of a’s, followed by twice as many b’s, followed by an unrelated number of c’s.

S -> DC
D -> AB | ADB
A -> a
B -> bb
C -> c | CC

b) B = {a^i b^j c^k d^i+j+k | i,j,k ≥ 0} 
In this set, we know that there is some non-negative number of a’s, followed by a non-negative number of b’s, followed by

S -> ASB | AB
A -> a | b | c
B -> d

c) C = {a,b} − {ww^R | w ∈ {a,b} } (where w^R denotes the reverse 
of string w). 
This set is equivalent to all strings that are *not* palindromes.

S -> ABC | AS | SC
A -> AC | a
B -> b | BB
C -> a | S


d) Convert to CNF
For this question, we will be converting our answer to part a) into Chomsky Normal Form. We start with the following grammar:

S -> DC
D -> AB | ADB
A -> a
B -> bb
C -> c | CC

And make just several changes to arrive in CNF:

S -> DC
D -> AB | ADB
C -> E | CC
E -> c
A -> a
B -> bb

The grammar is now in CNF

# 2
a)
	* Generated by the grammar:
		1. S -> ba 
		2. S -> AB -> BSB -> aSB -> aABB -> abaa
	* Not generated by the grammar:
		1. aaa
		2. aaaaaa
	
b) Convert grammar to GNF:
Start grammar:
S -> AB
A -> BS | b
B -> SA | a

First break up A -> BS and B -> SA:
S -> AB
A -> BSA | aS | b
B -> ABA | a

S -> AB
A -> ABASA | aSA | aS | b
B -> ABA | a

S -> AB
A -> aSAC | aSC | bC | aSA | aS | b
B -> ABA | a
C -> BASA | BASAC

S -> aSACB | aSCB | bCB | aSAB | aSB | bB
A -> aSAC | aSC | bC | aSA | aS | b
B -> aSACBA | aSCBA | bCBA | aSABA | aSBA | bBA | a
C -> BASA | BASAC

S -> aSACB | aSCB | bCB | aSAB | aSB | bB
A -> aSAC | aSC | bC | aSA | aS | b
B -> aSACBA | aSCBA | bCBA | aSABA | aSBA | bBA | a
C -> aSACBAASA | aSCBAASA | bCBAASA | aSABAASA | aSBAASA | bBAASA | aASA | aSACBAASAC | aSCBAASAC | bCBAASAC | aSABAASAC | aSBAASAC | bBAASAC | aASAC


# 3
a) `{x ∈ {a,b,c,d}∗ | #a(x) = #b(x) = #c(x) = #d(x)}`
Not CF

b) `{aj |j is a power of 2}`
Not CF

c) `{x ∈ {0, 1}∗ | x represents a power of 2 in binary}`
Regular

d) `L(0* 1* 0* 1*`
Regular

e) `strings of balanced parentheses of four types ( ) [ ] < > { }`
not cf

f) `{a^n b^m |n<m}`
cf

g) `{a^n b^m c^n d^m | n, m ≥ 0}`
Not cf

h) `{a^n b^m c^m d^n | n, m ≥ 0}`
Not cf

i) `{a^n b^n c^m d^m | n, m ≥ 0}`
cf

j) `{a^n b^m c^k d^l |n=k or m=l}`
Not cf

# 4
* a)
If A is a context free set, then there exists a constant k>0 such that for all elements z in the set A,  the length of z is greater than or equal to k. Furthermore there also exist variables u, v, w, x, y where the aforementioned variable z=uvwxy, such that the length of vwx is not greater than k, the length of vx is greater than 0 and for all i greater than or equal to 0, u(v^i)w(x^i)y is in A.
* b)
Let A be a set of strings. If for all k > 0 there exists some string z in A, with the length of z greater than or equal to our k, we can say that for all substrings u, v, w, x, y where z = uvwxy, the length of vwx is less than or equal to k and the length of vx is greater than 0. Then there exists an i greater than or equal to 0 such that u(v^i)w(x^i)y is not in set A. 
If this happens, then A is not context-free.
* c) Prove that the set B = {w in {a, b, c, d}* | num(a) > num(c) > num(d) > num(b) } is not CF
	1. Let k be given
	2. Consider z = a^(k+3) c^(k+2) d^(k+1) b^(k)
	3. Let z = uvwxy, |vwx| <= k, |vx| > 0

Now consider two cases:
	1. *Case 1:* If vwx contains any c’s or d’s, take i=2
	2. *Case 2:* If vwx contains no c’s or d’s, take i=0

Now in either case, u(v^i)w(x^i)y is not in set A.
# 5
As the CKY excerpt shown below demonstrates, the string /abbbab/ is *not* generated by the given CFG.  
<!-- {BearID:7ED08457-ACBB-479E-903B-36A01CD2F1D8-2136-000024DCCAFFC51B} -->
[[006 School MOC]]
#school #school/cs
# CS301 Final Exam Core Reference
#school/cs

Ok I need to really buckle up now and go through everything in a very methodical way. I plan to lay out all the topics we studied in class, focusing on the last 4 weeks of class/ Turing machines. I’ll first lay out the topics, then go through each video in turn, before reviewing all homework and solutions, and linking to all of them here for easy reference. Before taking, I also need to go through the final review. Sounds good enough!

# Topics
### Turing Machines
[[Turing Machines TOC]] 


### Reductions

### Classifying sets[[006 School MOC]]
#school #school/cs
# CS301 Final Exam
#school/cs 
# Topics to make notes on:
- Complexity classes P, NP, NP-hard, NP-complete
- Reduction practice!!! And definition page!!
- Classifying sets as regular, recursive, context-free, recursively enumerable,  co-re
- [[Chomsky Hierarchy TOC]]

<!-- {BearID:D5AA7BC8-9E17-4927-ADBC-F5D2426951CC-5857-000038EA20D55B09} -->
[[006 School MOC]]
#school #school/cs 
# CS301 HW1 Regex
Regular expressions are a powerful and versatile tool allowing for an easily definable and readable objective string interpreter. They are really useful tools for all kinds of string pattern mapping with a wide array of interesting use cases. I have personally used regex in python, C#, js and ruby for several interesting uses: at one point to verify form inputs were emails, matching the @ - domain - .com pattern. Another time, I used it to clean phone number inputs and verify inputs made sense. I have also used regex while working on my computer in bash, trying to match filenames and contents of various types.

<!-- {BearID:CE9AFC1B-CDB8-4F45-99E4-07ABF41992DD-75570-00054E6144890610} -->
[[006 School MOC]]
#school #school/cs
# CS301 HW9

# 2
	* *a):* We would be able to enumerate descriptions for DFAs, NFAs, PDAs and TMs for one simple reason; Enumeration machines are computationally equivalent to Turing machines, and TMs can simulate all computational paradigms contained within in the Chomsky hierarchy,  namely the ones listed above.
	* *b):* It is impossible to enumerate descriptions of all total TMs because total TMs must halt, and by the halting problem we know it is impossible to determine with consistency which TMs will halt.
	
# 3 
	* *a):* AnB is RE, if A and B are RE, since RE is closed under intersection.
	* *b):* AB is RE, if both A and B are RE, since we know that RE sets are closed under concatenation.

# 4
	* *a):* Decidable, since we can deduce that the TM loops on input 10110 if the TM goes through more than 5 states. If it does, by the pigeonhole principle, we know the read head must visit some position on the tape more than once, so must by extension loop on one of the given inputs.
	* *b):* Decidable, since we can let the TM run for a total of 101 states on input E, and accept if the machine halts before 101 states are up. We keep track of the number of elapsed states with another tape (or an abstraction of another tape if using a single tape machine)
	* *c):* Decidable, since we would only need to test this machine on all inputs up to length 101 for 101 steps, as any inputs longer than this will necessarily run for more than 101 steps. Thus if all inputs less than or equal to 101 in length take more than 101 steps, we accept.
	* *d):* Not decidable since it is impossible to verify whether one TM’s outputs are truly a subset of another, or there is simply overlap. If we bounded the problem and said something like “for all inputs under length 101 L(M) is a subset of L(N).

# 5
Given our EM E, there exists a total TM M | L(M) = L(E), meaning TM M accepts all sets enumerated by E. We know that if E enumerates a string X it will accept, whereas if it enumerates a string longer than X it will reject. This way the TM becomes a total TM. We can also check this operation in reverse, starting with a total TM | Total TM -> there exists EM E with output in increasing order. We know that this computation does not require time sharing. 

Thus we can conclude that an RE set is recursive since we have shown that it can be enumerated in increasing order.

<!-- {BearID:7EA05FCE-0AB8-48E6-B23D-4FF41874403C-16983-0001675513596339} -->
[[006 School MOC]]
#school #school/cs 
# CS301 Quiz 10
[[Chomsky Hierarchy TOC]]
Should be:
- Regular
- Regular
- Recursive
- EMPTY SET: RE
- Not EMPTY SET: CO-RE
- Loops onX: 
 

This should be:
- False
- True
- True
- False

<!-- {BearID:4AC0D802-6198-49CA-8297-41C358C872E2-5857-0000586000864098} -->
[[CS333 PS10]] [[Partial Quantum Measurement]] [[Bloch Sphere]] [[Quantum Measurement]] [[Phase Kickback]] [[Quantum Algorithms]] [[Entanglement]] [[Quantum Gates]] [[Quantum Error Correction]]

# CS333 Final Exam
![[CS333 Final-01.jpg]]
![[CS333 Final-02.jpg]]
![[CS333 Final-03.jpg]]
![[CS333 Final-04.jpg]]
![[CS333 Final-05.jpg]]
![[CS333 Final-06.jpg]]
![[CS333 Final-07.jpg]]
![[CS333 Final-08.jpg]]
![[CS333 Final-09.jpg]]
![[CS333 Final-10.jpg]]
![[CS333 Final-11.jpg]]
![[CS333 Final-12.jpg]]
![[CS333 Final-13.jpg]]
## 1 - Z type error
We are considering an error of the form $\Large Z_\theta=\begin{pmatrix}1 & 0 \\ 0 & e^{i\theta}\end{pmatrix}$

This represents rotations on the [[Bloch Sphere]] around $\Large\pm\hat{z}$
(Answers in PDF attached)



## 2 - Quantum Entanglement
### a) - Probability of each outcome
(Answers in PDF attached)

### b) - Recreating original entangled state
(Answers in PDF attached)
## 3 - Phase kickback for non-boolean functions
### a) - Analyze circuit for each of f(x)
(Answers in PDF attached)

### b) - Measurement outcomes & probability
(Answers in PDF attached)

### c) - What algorithm is this similar to?
This circuit definitely reminds me of the period finding algorithm, and hidden subgroup style algorithms broadly as well. It does differ from the standard period finding algorithm in that it adds a QFT on the lower qubit as well. Functionally, there are a lot of similarities in what happens as the QFT is applied with a tensor-product of a superposition and basis state. In this circuit however we pass in a $\ket{1}$ along the bottom and apply a QFT, so the tensor product of the two states picks up a complex phase that differs for each superposition state of the bottom qubit.

## 4 - Are you ready to invest in a quantum computing startup?
I believe the only correct answer to this question is "it depends". There are definitely a number of areas I think are ripe for long term investment at the moment, but any such investment would need to come with an understanding that success will depend on advances in error correction, noise reduction and system stability that have not yet ocurred. This means there is uncertainty involved that an investor will have no control over, however this does not mean no investments in the space should be made. In fact, I would argue the exact opposite - this high level of risk will be what creates the largest possible investment returns. Besides this, the real payoff from an investment in quantum technology now would likely only happen in decades. Holding an asset for such a long time is burdensome and risky, however market theory indicates that this would also increase the potential return to investments that become successful, moreso than an investment held for a shorter period of time several times. 

Before investing in a startup in the quantum space, I would need to sit down and understand exactly how quantum effects create a competitive edge for the business and what would stop other companies like IBM from copying the idea and undercutting the startup. I would definitely want to see concrete advantages for this, such as how the period finding algorithm uses quantum effects that can help factor numbers, and as a result circumventing current cryptography. An algorithm like this wouldn't be enough to make me invest though, as there are a number of practical challenges I would need to hold conviction would be overcome before success becomes a possibility. 

Specifically, I would look at the current hurdles for quantum progress, namely error correction and system stability, and see how it affects the idea. Presumably, most good ideas wouldn't be quite possible with current technology and would require further progress before becoming viable. I would seek to quantify exactly how much progress is needed before the idea becomes plausible, and what exact developments would be required. This is probably the most challenging part of investing in the space, because it is impossible to know how much progress we will be able to make in the future. I am confident that better error correction is ahead of us and that system stability and maximum complexity will continue to improve. 

All of these criteria are very much procedural, and do not depend on the idea itself. I would use them to pre-screen ideas that I could more safely ignore to look only at the interesting ideas. The reason I would screen by feasibility is because there are certainly lots of interesting applications of quantum computing people haven't thought of yet, so I wouldn't want to box myself into set ideas. I would still require a concrete explanation of the potential quantum benefits.

In terms of existing ideas, I find applications of quantum to be most exciting for finance in the short term and artificial intelligence in the long term. For finance, there are clearly a number of areas where the ability to perform optimization in constant/ linear time opens lots of possibilities for algorithmic trading. I've been spending lots of time working on my algo system, and I can already see how being able to optimize huge datasets with multiple variables in realtime could provide an unimaginable edge to the decision making process. It would make it much easier to process larger amounts of data concurrently to make individual decisions, assuming powerful-enough quantum computers could be made. In the longer term, once quantum systems really improve, I think the quantum model of computation could hold huge promise for the creation of better learning systems. At a simple level this could be new and faster ML algorithms, but I also see potential for a system reminiscent of a neural network that relies on omnidirectional backpropogation/ reinforcement to actually learn more similarly to humans. There are lots of hurdles to overcome before this could be possible, but from what I have learned in this class I think it is possible. An investment now could easily take several decades to come to fruition, but I think within about 50 years the likelihood of these ideas becoming possible is relatively high. 

Finally, I think the surest way to make money in the quantum space right now is education, and specifically providing an education service that comes to firms and does training on quantum capabilities. I feel like one of the biggest hurdles for current investment is that hype clouds the judgement of those deciding where to allocate money, whereas even a morning session explaining the very basics of quantum states, possible operations and current roadblocks would be enough to make a signigficant positive impact. I bet a lot of firms would also want to use a service like this to help better understand if quantum is applicable to their field generally.

In conclusion I would invest in a quantum startup now if their idea was good enough, could be defended from competitors, was realistic, used quantum effects to create a business edge, and required a quantifiable amount of progress before viability. I would also understand that any such investment would be extremely risky and would likely require me to hold my money for decades before seeing the kinds of returns that are possible (and certainly there is a distinct probability that the quantum industry becomes a multi-trillion dollar industry).


I have neither given nor received unauthorized aid in this exam.
Michael Calvey[[CS333 Quantum Computing]]
# CS333 Final Grade Proposal
#school 

I believe I have accomplished the goals I set out for myself at the start of this semester for this class. I'm now going to go through each of my learning and process goals and discuss whether and how I accomplished them.

## Learning Goals
The goals I set out for myself at the start of this semester were:

### 1. Use math & proper terminology to describe quantum algorithms and systems
My biggest deliverable for this goal was to explain to someone in 60 second what quantum is, why it is powerful and what kinds of things are possible or impossible with quantum systems. I still need to refine the specifics, but I actually tried this out with my ML Professor a couple weeks ago and was really impressed with my ability to hit all the important topics (that I am aware of) and describe the topic in a pretty complete way. Obviously 60 seconds is more of an elevator pitch than an explanation, but I think this actually demonstrates a better grasp of the content than being able to deliver a longer presentation on the topic. Speaking well in 60 seconds shows an ability to condense an incredibly complex, wide-ranging topic down into it's most important elements.


Despite my accomplishing the deliverables I set out, the goal itself was definitely the most challenging for me because I lacked a lot of important fundamental experience of math that was needed to fully analyze the systems we learned about this semester. Nevertheless, I am incredibly pleased with my progress and how much my math skills have improved through working on quantum-related problems. By the end of the course, I feel confident enough in the math that it is no longer a major roadblock to my understanding, but also doesn't significantly improve my understanding. I still try to approach the concepts in a more visual or intuitive way, and then make sure the math aligns.

To aid my work on this course, I created a whole section of notes related to the mathematical component of the course:
- [[Linear Algebra TOC|Linear Algebra]] | [[Matrices]] | [[Inner Products]] | [[Outer Products]] | [[Tensor Products]] | [[Conjugate Transpose]] | [[Complex Numbers]] | [[Complex Conjugate]] | [[Vector Spaces]]
- [[Quantum Algorithms]] | [[Quantum Errors]] | [[Quantum Error Correction]] | [[Period Finding Alrogithm]] | [[Quantum Cryptography]] | [[Deutsch's Algorithm]]

I've definitely improved my understanding and use of proper quantum terminology throughout the semester from 0 to a decently competent level. I will discuss this in my justification of process goals too, but my discussions with my physics major roommates demonstrate to me that I know and understand enough jargon to uphold meaningful conversations even with people who are familliar with the topic.

### 2. Describe properties of quantum mechanics and explain why these properties give quantum systems an advantage over classical ones.
This was one of the goals I emphasized a bit over the others because it aligned with my longer-term desires around quantum. My biggest goal for this course was to come out knowing enough about quantum mechanics and their application to computing that I could carry discussions and make decisions on these topics. I feel like my intuitive understanding of quantum is as strong as it could be without a deeper understanding of the math. I put a lot of focus this semester on building up the theoretical part of my notes. Some of the notes I've made on this topic are:
- [[Quantum Mechanics TOC|Quantum Mechanics]] 
- [[Quantum Computing TOC|Quantum Computing]]
- [[Phase Kickback]]
- [[Quantum Computer Hardware Implementations]]
- [[Quantum States]] | [[Quantum Gates]] | [[Quantum Measurement]] | [[Partial Quantum Measurement]] | [[Projective Quantum Measurement]] | [[Hadamard Gates]] | [[Qubits]] | [[Qubit Normalization]] | [[Quantum Fourier Transform Gate]] | [[Quantum Phases]] | 
- [[Multi Qubit Systems]]
- [[Superposition]] | [[Entanglement]] | [[Quantum Interference]]
- [[EPR Paradox]] | [[Interpretations of Quantum Mechanics]] | [[Copenhagen Interpretation of Quantum Mechanics]]

The best thing about creating the notes in this way is the added benefit of having what I call "dense linking" between ideas. I will link ever occurrence of ideas as I create new notes, which strongly reinforces my spatial understanding of the relationships between the concepts. 

My main deliverable for achieving this was being able to sit down with my physics roommates and hold a conversation, and I have not only achieved but gone way beyond my expectations on this one. I have even been able to use intuition built in this course to help understand complex topics I've never come across academically, such as time evolutions for example or hamiltonian operators. Because I built my notes out in such a way as to see the relationships between concepts, I have been able to easily slot in new ideas into my framework of understanding. I honestly wish I hadn't just discovered this system as I leave college!

### 3. Appreciate the limits of quantum
This goal is one of the more important ones as well. I expect there is a chance in the future I will be in a decision making role where I need to consider whether to invest in quantum capabilities. If this happens, I want to make sure I not only understand the potential upsides offered by quantum computation, but also the risks and limitations of such systems. For this reason, I've focused my learning in this area into two discrete but related categories: limitations that exist today only because we haven't yet improved hardware implementations, or limitations that exist today because of the limits of quantum theory. These two categories of limitations are distinct and I wanted to make sure I separated them when thinking about what is or isn't possible. There's probably a better chance that we address the limitations that are more implementation rather than theory-based.
[[Quantum Hype]] | [[Quantum Errors]] | [[Quantum Computer Hardware Implementations]]

### 4. Learn to code for quantum systems
The tech ended up not being quite mature enough for me to do what I thought I wanted to do for my trading system, but I was able to successfully implement several algorithms and basic circuits that demonstrated what we were learning in class or problem sets. That ended up becoming a helpful way for me to understand the problems; I would write them out in Qiskit and see how changing individual pieces affected the overall outcome.

It was really cool to see the parallels between the concepts we learned about and what things would look like when implemented in code. I'm quite familiar with learning things as a coding language or framework and using trial and error to understand what is going on, and this was a very helpful way for me to better understand some of the ideas like measurement, superposition and entanglement. In the end this is exactly what I hoped I would get out of learning to code for quantum systems, as well as starting my understanding off with an appreciation for what is technically possible to do at the moment.

### 5. Working with others on a practical question
I was really hoping with this goal to implement some novel idea for my trading system, however for anything besides a proof of concept this wouldn't have been super useful. For this reason I ended up pivoting a bit away from what I originally intended with this goal (which was to learn about quantum systems by coding with others) and more towards a discussion-based probing of my understanding. I would do this by asking targeted questions and attempting to explain concepts to my roommates, and I feel I was very successful with this in the end. Not only did I learn by asking questions of others, I probed my own knowledge and filled in gaps by focusing on explaining things.

### 6. Practice active listening
Active listening was a great goal for me this semester. I spent lots of time employing it in my class group discussions as well as discussions with my roommates. Analyzing my discussion style at the end of the semester showed a big improvement in how much engagement I could get out of other people. It's really cool seeing people get excited about things you want to talk to them about, and this is a very subtle way to do just that. Interestingly, I've had three academic disciplines I've been fascinated by this semester; Quantum, machine learning and cognitive neuroscience. Active listening has lots of roots in the way people think, and it was cool to draw the parallel between the concept and other things I'd been learning about recently.


## Process Goals
### 1. Reading technical pieces on Arxiv and quanta
I really succeeded at this goal. I'm currently fully caught up on the recent physics and quantum articles on quanta, and I actually feel like I understand the topics the articles discuss. It has been incredibly gratifying going from all of these concepts being completely alien to me, to understanding the implications of things without needing to have them explained to me. To me, alongside my newfound topical discursive skills, this is the biggest sign that I've learned a ton from this course. I'm also happy I established this skill now before graduating, so I'll be able to keep up with the latest ideas and developments in the space. I've actually also created a slack group with a bunch of friends in related fields (everything from finance to tech to physics and engineering) where I hope we will all be able to share technical and field-specific updates with each other. I wouldn't have thought of doing that if it weren't for the technical conversations I started enjoying with my friends as a result of this course.

### 2. Linked Notes
As I've already talked about lots in my reflections, I've spent much of this semester developing my new Personal Knowledge Management system. Things we've learned about in this class have actually served as the pilot information for my system, which is a fitting place to start since Quantum is one of the foundations of reality. My note system has now matured hugely, with a total of 750 files and 150,000 words. Obviously quantum notes represent just a portion of this total, but I can already feel how building all of the things I learn and think about into a single web of information is a really powerful way of understanding and remembering. In my responses to some of the goals above, I included links to some relevant files that I've created for the content of this course. I believe this demonstrates significantly my engagement with the concepts, my desire to learn them for the long term and my relative grasp of the relationship between the concepts thus far. I still have an uncountably infinitely long way to go to actually learn all of this stuff, but I'm excited for the opportunity to try and grateful for the strong foundation of knowledge I already have. I've recently decided that my personal motto for life is "How and Why?", the two questions that motivate me the most in the world. I plan to keep expanding on these notes and building new relationships between ideas for as long as I am mentally able to. I've certainly opened up a whole new world of possibilities for understanding and thinking with these notes.

### 3. Speaking quantum
I've been very successful at achieving this goal throughout the semester, and have actually grown to really enjoy discussing technical concepts with friends and acquaintances.


# Overall Grade Proposal: A
Overall I believe I deserve an A in this course. I have clearly not followed the path people usually take when approaching the material in this course. I set out to learn quantum with several very specific goals in mind. I tried my best to shape the goals we had given to us in our learning plans to fit closer to what I needed from this course. Although the mathematical understanding is clearly important, I certainly won't be working in a job where I would be expected to actually create or develop quantum systems. I might however be working in a capacity where I would need to decide whether or not to invest in quantum capabilities, acquire a competitor or enter the space for example. For this, I still need to have a strong theoretical understanding of what is going on, how, and why, and what is possible to do as a result.

I believe I have succeeded at this as much as I could have given my mathematical background for this course. I am really happy with the depth and breadth of my understanding at the moment, and I believe I have built a detailed appreciation for the key concepts of quantum mechanics, how they enable quantum computing, what exact applications may be possible, and where these systems fall short of the impossible expectations people tend to put on them.[[CS333 Quantum Computing]] | [[CS333 Midterm]]
# CS333 Midterm Reflection
I was initially somewhat upset with my performance on this midterm, but upon deeper reflection I am happy with my attempt because I feel like I am generally understanding the questions and coming up with reasonable approaches to solve them. I clearly need to be more meticulous and methodical with my approach however, because while I was on the right track, I found myself often grasping upon one particular path to a solution, and then sticking with it regardless of how well it seemed to be going.

## 1a
I think my reasoning here was good, I should have just followed through with the math to arrive at the real solution. I think if I had grappled with the problem a little longer, I could have done this. I'm not sure why I am so scared of approaching the math, but I really need to get over that fear and start using the math to prove my logic. 

## 1b
I did not make the connection between this question and PS1#5. I should spend more time revisiting past problem sets, since I feel like my overall understanding of quantum systems has improved to the point where my old conclusions are not valid anymore and I could understand the earlier concepts of the course to a much deeper level. I actually spent some time the other day revisiting my early personal research on interpretations of quantum mechanics (looking at things like the Copenhagen interpretation, EPR paradox, Bell inequality and other problems in the space) and found tons of links and new understanding even embedded within my earlier notes. That is really cool, and I should give myself the opportunity to go through the same process with the concepts we learned in the beginning of the class. 

In this case again, I should have written out more of the math and at least tried to get towards an answer. I suppose I should have carried out the measurement by Eve, predicted the probabilities of her getting R or Q, figured out what she would then send Bob, and then use that information to calculate the chance Bob's measurement disagrees with Alice's state.

I think one of the things that threw me off for this question was all of the trig in the measurement bases.

## 1c
I just realized that the M basis is actually the negative of the bisecting angle. I need to think about this question and answer some more from the perspective of carrying out measurements (and maybe make several such measurements) and then draw a general conclusion.

## 2a
I completely omitted the normalization process from my approach to question 2. Ironically, as I was working on the question, I kept being very confused by the amplitudes I seemed to be getting as they did not sum to 1 consistently. I thought this was a byproduct of the question setup which in hindsight is pretty silly, as we must ensure our states are normalized ourselves. I feel like the work we've done since the midterm on Deutsch's algorithm has really solidified my understanding of partial quantum measurement, so I think I would be able to make a better attempt at this question now. 

At the very least, I'm happy that for this question I followed through the calculations and made an attempt to reach the answer. I am also quite encouraged that my intuition for what the answer was was pretty close.

I am also realizing that I didn't quite fully understand the process of recombining the qubit at the final beamsplitter, and deciding which way it would end up going under what probabilities. 

## 2b
Normalization really hurt me here again. I should have taken the time to rewrite the state in a more readable way. If I had, I would have probably noticed the states weren't properly normalized. I tried to do the grouping method as we did in class, but I put the $1/\sqrt{2}$ in the wrong place, and things fell apart after that.[[CS333 Quantum Computing]] | [[Quantum Measurement]] | [[CNOT Gates]] | [[Quantum Gates]] | [[Quantum States]] | [[Partial Quantum Measurement]]
# CS333 Midterm
[[midterm.pdf]]
## 1. 
### a)
This is an interestingly different implementation of CHSH. If Alice and Bob decide to only send one of two states randomly, and the states are not orthonormal, additional weaknesses will be introduced. Most importantly, this is because the measurement probabilities are now uneven since Bob will be choosing either $\ket{0}, \ket{1}$ or $\ket{+},\ket{-}$ to measure in. Clearly, the probabilities of various measurements are now no longer certain. $\ket{0}$ has a 50% chance of collapsing to the state $\ket{+}$ and $\ket{-}$, but a 100% chance of collapsing to $\ket{0}$ and 0% of collapsing to $\ket{1}$. Thus additional information about what qubit was sent is revealed to Eve when she eavesdrops.

### b)
Optimal measurement basis: 

$$\LARGE
M(-\pi/8)=\{\ket{R(-\frac{\pi}{8})},\ \ \ket{Q(-\frac{\pi}{8})}\}
$$

Honestly I've tried a bunch of ways but I'm not sure how to approach this. My thinking was to try to figure out the probabilities of each sent photon and then reason how often Eve's snooping causes a misalignment, but I couldn't quite get there.

### c)
Measurement basis $M(-\frac{\pi}{8})$ is the best for distinguishing $\ket{0},\ \ \ket{+}$. Since this exactly bisects the angle of the two desired measured states, I would posit that that relationship carries for all other pairs of non-circular polarized photon states.

## 2.
### a)
Let us, as before, track the state of the photon. It starts in $\ket{+}_P\ket{0}_V$.

It then goes through the first beamsplitter/CNOT gate and will now be in the state:

$$\LARGE
\frac{1}{\sqrt{2}}\ket{0}_P\ket{0}_V+\frac{1}{\sqrt{2}}\ket{1}_P\ket{1}_V
$$

Since the states are split by the beamsplitter in their appropriate direction based on polarization. The states are now in a superposition state.

The photon in the channel that was initially vertical goes through the 45* lens with the overall state now becoming:

$$\LARGE
\frac{1}{\sqrt{2}}\ket{0}_P\ket{0}_V+\frac{1}{\sqrt{2}}\ket{-}_P\ket{1}_V
$$

Then the final beamsplitter will reflect the $\ket{-}_P\ket{1}_V$ with probability 1/2 making it horizontal, and will transmit it with probability 1/2 emitting it vertically. The part $\ket{0}_P\ket{0}_V$ gets transmitted always, ending up going down. This leaves us with the state:

$$\LARGE
\frac{1}{\sqrt{2}}(\frac{1}{\sqrt{2}}(\ket{-}_P\ket{1}_V+\ket{-}_P\ket{0}_V)+(\ket{0}_P\ket{1}_V))
$$

With this exit state, I believe we will get a detection just 1/4 of the time, when the state collapses to $\ket{-}_P\ket{0}_V$. The probability the photon is not detected is the inverse, or logically the 1/4 probability that the state collapses to $\ket{-}_P\ket{1}_V$ or $\ket{0}_P\ket{1}_V$ with probability 1/2.

b)

If we don't get a detection, that means the photon went vertically. There are two possible options that lead to this outcome: the state collapsing to $\ket{-}_P\ket{1}_V$ or $\ket{0}_P\ket{1}_V$. In actuality, since no measurement occurs if the photon exits vertically, the output state will be the following superposition of $\ket{-}_P$ and $\ket{0}_P$:

$$\LARGE
\frac{1}{\sqrt{2}}((\ket{-}_P+\ket{0}_P)\ket{1}_V)
$$
[[CS333 Quantum Computing]] [[006 School MOC]]
#school #school/cs #school/quantum 
# CS333 PS1
[[CS333 PS2]]
[[CS333 Week 1]]
[[PS1sol.pdf]]
1. (Question posted in syllabus)
2. (Done)
3.  

m		s		m_bar
0		0		0
0		1		1
1		0		1
1		1		0

The reason XOR works for encryption is for a given output m_bar, m could be either a 1 OR a 0. This means the message is not possible to determine from the output alone, and requires the secret key to XOR back into a message.

4. Math Practice
	1. Since the process of conjugation takes complex numbers from the form *x = a + bi* to *x = a - bi*, we can consider this to be a mirroring around the line i = 0, or the x-axis.
	2. The complex conjugate of e^(iw) would be options 1 or 2, 
	`cos(w) - i*sin(w)` or `e^-(ix)`
	3. The magnitude of `e^(ix)` can be written as:
	`|e^(ix)| = sqrt(e^(ix) * conjugate(e^(ix)))`
	`= sqrt(e^(ix) * e^(-ix)) = sqrt(e^0) = 1`

	Alternatively:
	`|e^(ix)| = sqrt((cos(w) + isin(w)) * (cos(w) - isin(w))) = 1`

	4. A simpler expression is found using Euler’s formula:
	`cos(3pi/2) + isin(3pi/2)`
	5.  
```
[1	i][1] = 1 + i^2
		  [i]
```

```
[1	-i][ 1] = 1 + i^2
		   [-i]
```

1. The complex transpose of y multiplied by x, all transposed is the same as the complex conjugate of x multiplied by y, since the complex conjugate is a commutative property and can be distributed through the process of matrix multiplication

7. The inner product follows the distributive property because it is itself an artifact of the distributive nature of matrix multiplication.

9.  

5. 
*a.* Without multiplying the tensor products, I was able to continuously simplify this expression until I had [(1/2) [TENSOR PRODUCT] (1/2)], yielding 1/4.

*b.* 1/4 is not what I would expect with classical coins. Since the outcome of one of the coins is already determined (heads), I would argue that with classical coins the actual odds would be 1/2, since exactly half of all rerolls would result in a situation where the first coin is a tails and the second a heads. I’m not sure if I messed up my math along the way or probability works in a fundamentally different way, but I am sure I will check in the solutions  

6.
[[CS333 Final Exam]]

# CS333 PS10
#school 

## 1 - Quantum Hype

## 2 -  [[CS333 Quantum Computing]] [[006 School MOC]]
#school #school/cs #school/quantum 
# CS333 PS2
[[CS333 PS1]] | [[CS333 PS3]]
[[CS333 Week 2]]
[[PS2sol.pdf]]

1. The way I understood the protocol, the key element of quantum mechanics that makes quantum cryptography possible is the fact that observed quantum particles undergo wave function collapse, and their state changes on observation. These state changes can be tracked and discrepancies noted during the public sharing stage of the protocol, since any snooping activity would immediately be evident when the wrong numbers show up too often. 
2. 
	1. Scientists are focusing on satellites for quantum cryptography since they would enable people to send completely secure messages across long distances without needing to use key-based authentication, which relies on keys being shared ahead of time. Satellites work in particular because there are only several tens of miles of atmosphere before photons sent up to satellites are in space, and much less prone to errors.
	2. Given what we’ve learned about the BB84 protocol, I would say that it is important to have a low basis error rate because otherwise it becomes impossible to tell if the error comes from intrusion or random chance, and the protocol stops being useful.
3. If the light comes on, he knows for sure that the photon that was sent was not horizontally polarized, since that has a 0% chance of passing a vertical filter. He cannot tell for sure without more observations whether the photon was say diagonally vs vertically polarized. If the light does not come on, we know for sure that the photon did not have a vertical polarization, but can’t say for certain whether it was diagonally or horizontally polarized, since a diagonally polarized photon would pass through 50% of the time, and Bob could have just experienced a random outcome.
4. The photon will make it through 1/4 of the time and will be vertically polarized, since that is the polarization of the last filter.
5. This is a tough and interesting question. I don’t know enough yet to answer for sure, but I think I could reason like this:

If Eve gets a detection, she knows for certain that the photon was not horizontally polarized. Then, there is as much chance that it was vertically polarized as the chance of both diagonal polarizations combined, so a 50% error rate thus far. We know that Eve only measures with probability p, so we can then simply say that the overall probability that a given photon is wrong when Bob and Alice check is 0.5p.
[[CS333 Quantum Computing]] [[006 School MOC]]
#school #school/cs #school/quantum 
# CS333 PS3
[[CS333 PS2]] | [[CS333 PS4]]
[[CS333 Week 3]]
[[PS3qsol.pdf]]

1.  
a.  
c) In this problem, I feel like I have more intuitively understood the fact that orthonormal bases can be used in place of our traditional spin ket vectors as defined in class. I feel like the whole linear nature of this quantum algebra is also starting to make sense, although I am unsure if I am approaching the problem correctly. This means quantum states can always be written as a linear combination of orthonormal bases.
 
4.  
a) 
Sigma` must also represent a qubit state, since it is really just Sigma multiplied by the phase e^(iw). Since qubit states can be written as linear combinations of their constituent orthonormal bases (as noted above, but in this case using standard ket 0 and ket 1), we can simply multiply Sigma through by our phase, without changing the outcomes of observations.

b) 
Since we know the absolute value of the phase is 1, we can also say that for any measurement of quantum states, the phase will not have an impact on the resulting probabilities. Since we take the absolute value squared, the phase ends up not affecting our observations.

c)
I’m not sure how it would be possible to tell the difference between Sigma and Sigma`. Since measurements give the same predicted outcome probabilities, we cannot use measurement to try to tell the difference. Perhaps there is another technique, but I am unaware of it.

<!-- {BearID:9B0E3108-D002-42C2-990A-7543FCD8C4BE-1476-0000A3E0861D8580} -->
[[CS333 Quantum Computing]] [[006 School MOC]]
#school #school/cs #school/quantum 
# CS333 PS4
[[CS333 PS3]] | [[CS333 PS5]]
[[PS4qsol.pdf]]


## Questions 1 & 2:
![[PS4-1.jpg]]
![[PS4-2.jpg]]
![[PS4-3.jpg]]
![[PS4-4.jpg]]

## Question 4
i)
a) The case of Random.org clearly shows why the needs of true randomness vs secure randomness are different. Random.org uses atmospheric noise measurements to generate it's random numbers, which are much more close to seeming "random" than a pseudo-random classical RNG system on a computer. The issue with this type of randomness however is that it is not truly random - it rather relies on the fact that atmostpheric noise represents such a complex system that no known computational approach could make much sense of or even less predict it. That's not to say it isn't possible though - careful tracking of ongoing conditions would still allow a bad actor to act in a malicious way.
	
ii)
a)
	
b)
	(INCOMPLETE)[[CS333 Quantum Computing]] | [[Beam Splitter]] | [[Interferometer]] | [[Partial Quantum Measurement]] | [[Multi Qubit Systems]] | [[Quantum Measurement]] | [[Superposition]] | [[Entanglement]] | [[Relative Phases]]
# CS333 PS5
[[CS333 PS4]] | [[CS333 PS6]]
[[PS5qsol.pdf]]
#school 


1.  
a)

This is a tricky one and I'm not totally sure, but let's start with the start state $\ket{\psi}$
$\ket{\psi}=\ket{+}_P\ket{H}_A$ goes into the beam splitter. 

Since we are not conducting a measurement, the photon remains in a superposition state with the polarization and arm position entangled, as $$\frac{1}{\sqrt{2}}\ket{0}_P\ket{H}_A+\frac{1}{\sqrt{2}}\ket{1}_P\ket{V}_A$$

The horizontal portion then encounters the mirror, becoming $-\frac{1}{\sqrt{2}}\ket{0}_P\ket{H}_A$ leaving us with:
$$
\frac{1}{\sqrt{2}}\ket{1}_P\ket{V}_A-\frac{1}{\sqrt{2}}\ket{0}_P\ket{H}_A
$$

Now the entangled photon goes through the mirrors, becoming $\frac{1}{\sqrt{2}}\ket{1}_P\ket{H}_A-\frac{1}{\sqrt{2}}\ket{0}_P\ket{V}_A$.

It now enters the final beamsplitter, recombining to become 
$$\frac{1}{\sqrt{2}}\ket{1}_P\ket{V}_A-\frac{1}{\sqrt{2}}\ket{0}_P\ket{V}_A$$
$$=\frac{1}{\sqrt{2}}(\ket{1}-\ket{0})\ket{V}$$

This is no longer our orthonormal basis for $\ket{+}!$


b)
In this case, the states are different because the gate is no longer creating a [[Global Phases]], as it would if it affected the whole photon. Since it acts on only one arm, it is a new state. 

3.

Ahsoka has $a\ket{0}+b\ket{1}$

Ahsoka and Boba share entangled state 
$$\frac{1}{\sqrt{2}}(\ket{00}\ket{11})_{A_2B}
$$
So the total state is:
$$(a\ket{0}+b\ket{1})_{A_1}\otimes\frac{1}{\sqrt{2}}(\ket{00}\ket{11})_{A_2B}
$$
$$
=\frac{1}{\sqrt{2}}(a\ket{000}_{A_1A_2B}+a\ket{011}_{A_1A_2B}+b\ket{100}_{A_1A_2B}+b\ket{111}_{A_1A_2B})
$$


Ahsoka now applies her gate on each of her two qubits leaving her with:
$$
=\frac{1}{\sqrt{2}}
(\frac{1}{\sqrt{2}}a\left(\ket{00}_{A_1A_2}+\ket{01}_{A_1A_2}\right)\ket{0}_{B}
$$
$$
+\frac{1}{\sqrt{2}}a\left(\ket{10}_{A_1A_2}+\ket{11}_{A_1A_2}\right)\ket{1}_{B}
$$
$$
+\frac{1}{\sqrt{2}}b\left(\ket{10}_{A_1A_2}-\ket{11}_{A_1A_2}\right)\ket{0}_{B}
$$
$$
+\frac{1}{\sqrt{2}}b\left(\ket{00}_{A_1A_2}-\ket{01}_{A_1A_2}\right)\ket{1}_{B})
$$

Ahsoka measures her qubits $A_1$ and $A_2$ with the standard basis $\{\ket{00},\ket{01},\ket{10},\ket{11}\}$ getting the final state:

$1/8$ chance state collapses to each of *(note: 01 and 11 cancel out and do not occur)*:
* $a\ket{000}_{A_1A_2B}\rightarrow$ Ahsoka sends 00
* $b\ket{001}_{A_1A_2B}\rightarrow$ Ahsoka sends 00
* $b\ket{010}_{A_1A_2B}\rightarrow$ Ahsoka sends 01
* $b\ket{011}_{A_1A_2B}\rightarrow$ Ahsoka sends 01
* $a\ket{100}_{A_1A_2B}\rightarrow$ Ahsoka sends 10
* $b\ket{101}_{A_1A_2B}\rightarrow$ Ahsoka sends 10
* $a\ket{110}_{A_1A_2B}\rightarrow$ Ahsoka sends 11
* $b\ket{111}_{A_1A_2B}\rightarrow$ Ahsoka sends 11

Now for Bob there are only four equally likely outcomes. He applies the gates on each, generating the state:

$$
=\frac{1}{\sqrt{4}}
(\frac{1}{\sqrt{2}}(a\ket{0}+b\ket{1})
+\frac{1}{\sqrt{2}}(a\ket{0}-b\ket{1}
+\frac{1}{\sqrt{2}}(a\ket{1}+b\ket{0}
+\frac{1}{\sqrt{2}}(ai\ket{1}-bi\ket{0}
)
$$

NOTE: I feel like I am getting very close, but I'm not getting that final breakthrough to get this to match up. I feel like I'm not correctly doing the multi-qubit measurements.[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[CS333 Quantum Computing]] | [[Quantum States]] | [[Quantum Measurement]] | [[Quantum Gates]]
# PS6
[[CS333 PS5]] | [[CS333 PS7]]
[[PS6sol.pdf]]
#school 

1. a)
	I understand the vector $\vec{z}$ as corresponding to $\ket{0}$ because we can think of $\vec{z}$, as the northern direction, being equal to $|\psi(0,\phi)⟩=\begin{pmatrix}cos(0) \\ e^{i\phi}sin(0)\end{pmatrix}=\begin{pmatrix}1 \\ 0 \end{pmatrix}$. This is our $\ket{0}$. The same but reverse is true for $\ket{1}$
	
	b)
	Note to self: x, y and z are orthonormal in the bloch sphere. I wonder how this translates to standard basis states? I've converted all of the below to the $\theta'$ variant. In this case all 4 points lie on the equator.
	
	I posit that orthonormal vectors equate to orthonormal bases, while negative vectors equate to the matching pair, so for $\ket{0}$ $\ket{1}$ is the inverse, and we know these map to $\vec{z}$ and $\vec{-z}$
	
	$\vec{x}=\ket{\psi(\pi/4,\pi/4)}=\begin{pmatrix}cos(\pi/4)\\ e^{i\pi/4}sin(\pi/4)\end{pmatrix}=\ket{+}$
	
	$\vec{y}=\ket{\psi(\pi/4,7\pi/8)}=\begin{pmatrix}cos(\pi/4)\\ e^{i7\pi/8}sin(\pi/4)\end{pmatrix}=\ket{i}$
	
	$\vec{-x}=\ket{\psi(\pi/4,3\pi/4)}=\begin{pmatrix}cos(\pi/4)\\ e^{i3\pi/4}sin(\pi/4)\end{pmatrix}=\ket{-}$
	
	$\vec{-y}=\ket{\psi(\pi/4,3\pi/8)}=\begin{pmatrix}cos(\pi/4)\\ e^{i3\pi/8}sin(\pi/4)\end{pmatrix}=\ket{-i}$
	
	c) Since we know now which quantum states each of these 6 vectors refers to, we can use existing examples to deduce what happens when we take the absolute value squared of the inner product of two orthogonal vectors and two opposite vectors.
	
	With two orthogonal vectors, we should get a vector exactly bisecting the two vectors in question. I believe this would correspond a quantum state having being in a probabilistic superposition of the two measurement basis states.
	
	With two opposite vectors, I think the result is 0. 
	
	After writing that out, that doesn't really seem right, or at least I'm still missing something.
	
	d)
	I think taking a qubit measurement using the bloch sphere is the same as taking square absolute value of the inner product of the two vectors, as we did above. This makes sense, since for example the probability of any of x, -x, y or -y collapsing to $\ket{0}$ = 1/2, and the same for $\ket{1}$.
	
	e)
	i) I think the I rotation will exactly preserve any input vectors. If we think of it as a matrix multiplication problem (which the actual problem at hand is parallel to) then multiplying by I just maintains the state of each component.
	$I=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}$ 
	Perhaps any non-1 multiple I matrices would rotate around the north-south axis. 
	
	
	ii) $X=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}$
	I believe this unitary corresponds to flipping rotating around the right diagonal axis, or more conveniently, $\vec{x}$, since that vector would not be changed. In this case, the angle of rotation would be a 180* flip, I believe.
	
	iii) $Z=\begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$
	This will, conveniently, rotate around the $\vec{z}$ vector. 
	
	iv) $Y=\begin{pmatrix}0 & -i \\ i & 0\end{pmatrix}$
	This appears to be a rotation around $\vec{y}$.
	
	v) $\frac{1}{\sqrt{2}}\begin{pmatrix}1 & i \\ i & 1\end{pmatrix}$
	
2. 
(This is missing because I was focusing on finishing on time. I will work through this problem when I have a moment soon)
3. 
	In our view, ‘supremacy’ has overtones of violence, neocolonialism and racism through its association with ‘white supremacy’. - Nature letter
	
	It dumbs down understanding of language: word meanings are conventions, not spells with magical powers, and all words have multiple senses, which are distinguished in context. - Steven Pinker
	
	there’s little I despise more than a meaty scientific debate devolving into a pointless semantic one
	
	
	I think the question of connotations in the English language quickly becomes indecipherable because of the inherent nature of both history in general as well as the fact that the foundation of practically all of "western civilization" is empire. I very much understand both sides in the debate around language, but personally (and some would state that as a white male my say is less important, which I understand but disagree with) I believe that we must work to acknowledge and bring to light our and our ancestors' transgressions. Humanity has done incredible things, evil things, and everything in between. I understand why certain individuals may feel ostracized by the use of language, but I don't think simply changing terms such as "quantum supremacy" to something along the lines of "quantum domination" would do much if anything to truly deal with the issues in our society. We evidently have massive issues, but changing the name and pretending like nothing happened is not at all the right way to go about improving ourselves as a people. 
	
	I do not want this to come off at all as me saying that we don't need improvement. I just hold the personal view that unless things can be acknowledged, understood and fully appreciated, we can't really move on as a society. I do somewhat disagree with the line where Aaronson says "it dumbs down understanding of language: word meanings are conventions, not spells with magical powers, and all words have multibple senses, which are distinguished in context," although Aaronson does have a point. One of my favorite books ever for stirring thought is actually quite an old texts titled "the tyranny of words" by Stuart Chase. The most incredible thing is that the book was written in 1937 and still establishes truths I believe hold to this day. In one of the chapters, Chase talks about Einstein's theory of relativity which had rocked the world about a couple decades before the publication of the book. Specifically, Chase states that the beauty of much of Einstein's work was that he "derived a picture of the world relatively undeflected by the human senses." (Chase 1937). Granted, as may be evident from the date of publication of *The Tyranny of Words,* Chase is much less concerned about the implications of the choices of our words, moreso focusing on making some very interesting observations about the relative and individual nature of semantics. His core argument is that everyone constructs their own meaning for every word they hear based on their past experiences with that word. The big lesson from the book is a note of caution - what you say is not what people hear. I think this rings particularly true in modern-day discussions of cancel culture, and must be discussed in the open. 
	
	In their letter to the editors of *Nature*, the co-authors wrote "In our view, ‘supremacy’ has overtones of violence, neocolonialism and racism through its association with ‘white supremacy." I would counter this by noting that literally all of western civilization has these overtones. Our society has grown out of that past. That does not legitimize it, nor does it imply that the current state of society is fully developed. We've come a long way, but we perhaps have even longer to go from here. Semantics are clearly relevant and important, but we have the power to control language. It may be difficult to control it directly, but in the end it is shared understanding that determines meaning in the broadest sense. I agree when Aaronson states that the term quantum supremacy could be a good way to reclaim the word supremacy. I think that, alongside a necessary discussion on the roots of the word and why it is problematic, would be the best way forward. At a certain point there are so many different people on this planet, that someone will surely have strong negative connotations with all kinds of words. Pretending they don't exist does not solve the problems they had, and only serves to severely limit an already rather poor language. 
	
	I'd like to finish with the oxford english dictionary of "supremacy": `noun; the state or condition of being superior to all others in authority, power, or status: the supremacy of the king.` I think this is *literally* the most specific and correct term that could be used in the context of quantum machines being able to tackle problems beyond the capabilities of classical systems.[[CS333 Quantum Computing]] | [[Quantum States]] | [[Phase Kickback]] | [[Deutsch's Algorithm]]
# CS333 PS7
#school 

## 1
Assume $\large \{\ket{\psi_{00}},\ket{\psi_{01}},\ket{\psi_{10}},\ket{\psi_{11}}\}$ is an orthonormal basis. This tells us that using superpositions of these 4 states, we can describe the entire state space of any two qubit system. We know that the same is true for any set of 2-qubit orthonormal bases, including the standard bases $\large \{\ket{00},\ket{01},\ket{10},\ket{11}\}$. 

From this, we can rewrite the state $\large \ket{\psi_i}=A\ket{i}$ meaning the state is now written as some unitary A acting on a standard basis state. Let us further define the state $\ket{\psi}=\begin{pmatrix}\alpha \\ \beta\end{pmatrix}$.

We now know that each of our initially given orthonormal basis states i can be written as the unitary A postmultiplied by $\ket{i}$.

This is shown by:
$$
\large \ket{\psi_i}=A\ket{i}=A_{00}\ket{0}\braket{0|\psi_i}+A_{01}\ket{0}\braket{1|\psi_i}+A_{10}\ket{1}\braket{0|\psi_i}+A_{11}\ket{1}\braket{1|\psi_i}
$$
$$
\large
=(A_{00}\alpha+A_{01}\beta)\ket{0}+(A_{10}\alpha+A_{11}\beta)\ket{1}=\begin{pmatrix}A_{00}\alpha & A_{01}\beta \\ A_{10}\alpha & A_{11}\beta\end{pmatrix}
$$

___
Since we know that $\large I=\sum_i{\ket{i}\bra{i}}$, we can define a unitary matrix A such that: $$AI=\sum_i{A_i\ket{i}\bra{i}}=\sum_i{\ket{\phi_i}\bra{\phi_i}}$$

Since A is a unitary gate, there also exists $A\dagger$ that acts as desired, converting $\ket{\psi_i}$ into $\ket{i}$.

## 2
### a)
### i) NOT:
$$
X=\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
$$
	
### ii) TOFFOLI:
$$
CCNOT=\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0
\end{bmatrix}
$$

### b + c)
I'm not certain if I'm going in the right direction here, but I think both of the matrices I wrote out above are left stochastic: all columns and rows add to 1 indicating it could be interpreted as whole probabilities. They are also valid unitaries, since you could invert them. This suggests to me that probabilistic computers could simulate deterministic computers, however there would be a huge computational overhead since 2^n bits are needed. A quantum computer could certainly simulate a classical computer, it would just do so incredibly slowly.

## 3
### a)

| x   | y   | $f_{00}(x_1, x_2)$ | $f_{01}(x_1, x_2)$ | $f_{10}(x_1, x_2)$ | $f_{11}(x_1, x_2)$ |
| --- | --- | ------------------ | ------------------ | ------------------ | ------------------ |
| 0   | 0   | True               | False              | False              | False              |
| 0   | 1   | False              | True               | False              | False              |
| 1   | 0   | False              | False              | True               | False              |
| 1   | 1   | False              | False              | False              | True               |

### b)
$n - 1$ or $4-1=3$

### c)

### i)
Immediately the unitary $U_f$ we have had two $\ket{0}$ states acted on by hadamards, and one $\ket{1}$ state acted on by a hadamard. This will give us the overall state $\ket{++-}$ or $\ket{++}\ket{-}$.

We can then rewrite this as:
$$
\frac{1}{2\sqrt{2}}\sum_{x_1,x_2\in\{0,1\}}\ket{x_1,x_2}\ket{-}
$$

This is equivalent since it is really summing all the different variants of $\ket{++}$, but leaves the state easier to work in.

### ii)
Our unitary is defined as 
$$\Large
U_f\ket{x_1,x_2}\ket{y}=\ket{x_1,x_2}\ket{y\oplus f(x_1,x_2)}
$$

We can substitute in the state above and expand the summation to calculate the state after $U_f$:
$$\huge
U_f[\Large\sum_{x_1,x_2\in\{0,1\}}\frac{1}{2\sqrt{2}}\ket{x_1,x_2}\ket{-}\huge]
$$

$$\large
=\frac{1}{2\sqrt{2}}U_f\ket{0,0}\ket{-}+\frac{1}{2\sqrt{2}}U_f\ket{0,1}\ket{-}+\frac{1}{2\sqrt{2}}U_f\ket{1,0}\ket{-}+\frac{1}{2\sqrt{2}}U_f\ket{1,1}\ket{-}
$$

$$\large
=\frac{1}{2\sqrt{2}}\ket{0,0}\ket{-\oplus f(0, 0)}+\frac{1}{2\sqrt{2}}\ket{0,1}\ket{-\oplus f(0, 1)}+\frac{1}{2\sqrt{2}}\ket{1,0}\ket{-\oplus f(1, 0)}+\frac{1}{2\sqrt{2}}\ket{1,1}\ket{-\oplus f(1,1)}
$$

This demonstrates how we get phase kickback. The outcome phase depends on the definition of $f(x_1,x_2)$. In the next question, we shall examine the 4 possible states, one for each potential definition of f.

### iii)
**Case 1: $f(0,0)=1$:**
$$\large
=\frac{1}{2\sqrt{2}}\ket{0,0}\ket{-\oplus 1}+\frac{1}{2\sqrt{2}}\ket{0,1}\ket{-\oplus0}+\frac{1}{2\sqrt{2}}\ket{1,0}\ket{-\oplus0}+\frac{1}{2\sqrt{2}}\ket{1,1}\ket{-\oplus0}
$$

I am struggling to make the connection with how the phase kickback actually happens here. I am also realizing I'm not entirely clear on what the modulus operator does when applied to the hadamard state $\ket{-}$.

We know each of the four resulting states is orthonormal because 1: the states are orthogonal, as they differ in exactly one element, 2: the states are normalized, since the sum of absolute value of the squared probabilities will be 1.

### iv)
All I know for this question is that we can ignore the third qubit once we apply $U_f$ because all of the necessary information will now be contained in the first two qubits via phase kickback. The last qubit will remain in state $\ket{-}$

### v)
I believe I have a rough but not complete understanding of how this algorithm works. To begin with, the two plain hadamard and one x-hadamard are applied, putting the three input qubits into a superposition state. This allows us to execute $U_f$ once and retrieve the result using phase kickback. Once $U_f$ is applied, the resulting measurements on kets 1 and 2 will tell us the exact definition of $U_f$.[[CS333 Quantum Computing]] | [[Phase Kickback]]
# PS8
#school 


## 1
### a
#### $H^{\otimes n}\ket{0}^{\otimes n}$
The resulting state from applying this operation should be $\Large\frac{1}{2^{n/2}}\sum_{y\in \{0,1\}^n}{\ket{y}}$.

This is equivalent to a no-phased superposition of all possible combinations of n 0 or 1 qubits.

#### $H^{\otimes n}\ket{1}^{\otimes n}$
The resulting state from applying this operation should be $\Large\frac{1}{2^{n/2}}\sum_{y\in \{0,1\}^n}{(-1)^{|y|}\ket{y}}$.

This is similar to the state above, except any states with an odd number of 1s will have a negative phase. 

### c
#### i)
In this case, I believe the worst-case (which given the prompt it would always be) query complexity will be $(n/2)+1$. We know this is the case since after querying $n/2$ times, a malicious f designer would put all 0s for the first n/2 items, and all 1s for the last n/2 items, thus forcing us to test one more item.

#### ii)
I am not 100% certain with my answer for this, but I would approach this problem by creating an algorithm that guesses different combinations of $\{0,1\}^n$. If we could truly guess randomly, we would only need to try 2/3 of the combinations of the deterministic algorithm to have a 2/3 certainty in the outcome, so we can extrapolate from that to calculate the overall probabilistic query complexity of $2/3((n/2)+1)$

### d
#### i)
To calculate the state of the system after $U_f$ acts, we will follow the system through step by step to see where we end up.

We start with 
$$\ket{\psi_0}=\ket{0}_A\ket{-}_B$$

Then, the H gates act on our $\ket{0}$ leaving us with:

$$
\ket{\psi_1}=\ket{+}_A\ket{-}_B
$$

Now, $U_f$ acts on our state $\ket{\psi_1}:$

$$
\ket{\psi_2}=U_f\ket{+}_A\ket{-}_B=U_f(\frac{1}{\sqrt{2}}\ket{0}+\frac{1}{\sqrt{2}}\ket{1})^{\otimes n}\ket{-}
$$

$$
=(\frac{1}{\sqrt{2}}\ket{0}+\frac{1}{\sqrt{2}}\ket{1})^{\otimes n}\ket{-\oplus f(\frac{1}{\sqrt{2}}\ket{0}+\frac{1}{\sqrt{2}}\ket{1})^{\otimes n}}
$$

Thus, since we are passing hadamard states to $U_f$, phase kickback will happen and the action of $U_f$ will be preserved in the first $n$ qubits.

#### ii)
#### iii)
#### iv)
## 2
### a)
We know that simulating this quantum algorithm will take O(Tn) bits because we will need to have a bit for each possible standard basis state for each qubit.

Furthermore, we can evaluate the time complexity by considering several factors. First, the time will scale linearly with the number of unitaries involved, namely M. Second, we will have to simulate every possible combination of superposition states, or $T^2$. Finally we know that we will have to do this for every qubit n, leaving us with the time complexity equation of $O(MT^2n)$

### b)
Based on my answer above, I would suppose that the critical area for time complexity gains is the $T^2$ component, or in other words the the standard basis states out of which our superposition is created. We could put states into superposition and thus check all basis state combinations in time O(1). This yields an exponential speed up for our algorithm and demonstrates the value of quantum systems as opposed to classical ones. 

Additionally, the n component of our time complexity equation is another potential area of optimization. If we parallelized our algorithm, we would get a linear speedup. 

## 3
### a)

### b)
-   Quantum computing is well-suited to solve optimization problems—sorting through vast potential solutions to arrive at the best decision.
	-   This is a problematic statement because it implies that we can always get the information we desire out of a quantum system. In reality, we know there are observables and unobservables, and furthermore the act of observation destroys quantum information, making it difficult to actually "sort through vast potential solutions." We have workarounds to this using clever quantum algorithms such as Deutch's Algorithm, however these use techniques such as phase kickback to extract a bit of extra information, rather than "sorting through" all of the potential solutions.
-   Vivek Wadhwa piece:
	-   This piece is problematic for several reasons. First off, the author compares the runtime of a travelling salesman solution between *A LAPTOP* and a multi-million dollar quantum system. This makes no sense, especially when you realize how expensive and complex a quantum computer that could do this "in seconds" would be. 
	-   This piece also (perhaps understandably, although falsely) simplifies the complexity of retrieving quantum information. When we put states into superposition and measure with respect to the bases being superimposed, we can never get out all of the quantum information. 
	-   I also don't like this being compared to Y2K because although I wasn't around, it seems like it wasn't a very big deal and was a pretty great example of a time when people shouldn't have been super worried about technological change.
	-   Finally people always talk in vague numbers about the difference between current capabilities and what would be required to actually solve many of these problems, and neglect to mention how far we are from building a quantum computer that could compute prime factorizations for numbers hundreds of digits long, for example. [[006 School MOC]] [[005 Active MOC]]
# CS333 Quantum Computing
#school/cs #school/quantum 
#school/class 
#concepts/cs/quantum


## Refs
[[005 Active MOC]]
[CS 333](https://www.cs.middlebury.edu/~skimmel/Courses/333S21/) - Course Website
[Class OneNote](https://middleburycollege-my.sharepoint.com/personal/skimmel_middlebury_edu/_layouts/15/Doc.aspx?sourcedoc={99047f54-8694-465e-bdd6-5d7a9105f25e}&action=view&wd=target%28Welcome.one%7C0cbdf7c3-331d-4b26-a066-b2d477bbc713%2FWelcome%20to%20Class%20Notebook%7C00ce158a-6083-49c2-9fc2-1a17de01dc6d%2F%29)
 
## Review
[[Linear Algebra TOC]]
[[Information Theory]]
[[Quantum Mechanics TOC]]
[[Quantum Computing TOC|Quantum Computing]]
Quantitative finance q algos [Portfolio Optimization — Qiskit 0.23.6 documentation](https://qiskit.org/documentation/tutorials/finance/01_portfolio_optimization.html)

## Pending Homework
* [[CS333 Quiz 4]]!!!! DUe years ago boi! 
* [[CS333 PS9]]
* [[CS333 PS10]]
* [[CS333 PS11]]
* [[CS333 Final Exam]]


## Completed Homework
[[CS333 PS1]]
[[CS333 PS2]]
[[CS333 PS3]]
[[CS333 PS4]]
[[CS333 PS5]]
[[CS333 PS6]]
[[CS333 PS7]]
[[CS333 PS8]]



## Topics
1. Single [[Qubits]], [[Quantum Cryptography]], [[Quantum Measurement]]
2. Multiple Qubits (tensors product and entanglement) [[Quantum Bomb Game]], [[Partial Quantum Measurement]]
3. Teleportation
4. [[Quantum Gates]] and Circuits
5. [[Deutsch's Algorithm]]
7. [[Period Finding Alrogithm]]
8. Search
9. Quantum Error Correction

## Schedule
* Pset assigned Monday
* Reflection on past due Tuesday night.
* Rough draft due Friday
* Due Monday
* Exit ticket after class!!!!

## Classes
* [[CS333 Week 1]]
* [[CS333 Week 2]]
* [[CS333 Week 3]]
* [[CS333 Week 4]]
* [[CS333 Week 5]]# CS333 Quiz 1
#school #school/cs #school/quantum
[[CS333 Week 2]]
1. 
We say a basis is an orthonormal basis when all the inner product of every combination of inner crosses produces 0 unless the same vector is used, in which case the inner cross should be 1. We can use this to reason that alpha should be some complex number that makes this true. 
 
2. The dimension of this vector where the tensor product is taken with itself will always be the same as the initial vector, since the tensor product operation preserves the dimensions of whatever the first input vector is for the output.

<!-- {BearID:53385163-E28F-484A-831C-FEF1726C0FF2-3006-0000B7F3AEC80EC9} -->
[[006 School MOC]]
# CS333 Quiz 2
#school #school/cs #school/quantum 
[[CS333 Quantum Computing]]
1. This strategy would not work well since it becomes possible to determine with certainty what polarization the photon sent by Bob had. Eve would be able to measure, and if she doesn’t register a pass, she knows her polarization basis was off by 90 degrees - vertical vs horizontal or vice versa. In eliminating private information (since she could never fully learn what was sent by Alice before) the protocol becomes insecure.
2. This strategy makes it clear that the fact that photons allow us to generate secret keys due to the fact that they can be sent in any polarization, and then probabilistically pass measurement. This occurs because of the superposition state the photon is in until observation. If there were only two possible states, as above, the protocol would not be secure. It is only because Eve has to guess from nearly infinite (but practically 6) options when picking her basis, and thus she is unable to consistently measure the photon.

<!-- {BearID:B002836B-4797-4135-A29C-E54AFB62CC16-574-000003EED5063326} -->
[[CS333 Quantum Computing]]
# CS333 Quiz 4
1. I believe this state is entangled because # CS333 Week 1
#school #school/cs #school/quantum 
[[CS333 Quantum Computing]]

[file:DA3C4DD2-F8E7-4DED-A68F-73AE3E7DE48E-2534-0000B2964BC38A9E/MathPractice.pdf]


[file:F87A9B6E-ACF5-442A-B195-BE82193B8B77-2534-0000B2AC4D9BE6F1/PS1.pdf]

[[CS333 PS1]][[Linear Algebra TOC]]

<!-- {BearID:0D0A261D-FBCA-4E62-8AC3-F5D707775226-2534-0000B28D56151086} -->
# CS333 Week 2
#school #school/cs #school/quantum 
[[CS333 Quantum Computing]]
[[Quantum Measurement]]
[[Standard Model TOC]]
[[Quantum Cryptography]]

[file:FA8452D0-E362-4BE5-8DFB-3CC1CDFE86A5-375-00000408DF6331D3/PS2.pdf]
[[CS333 Quiz 1]]
[[CS333 PS2]] 
<!-- {BearID:B2DD91CF-6AAF-47FD-B78E-5DC5275E41CE-375-0000028B84963BCF} -->
# CS333 Week 3
#school
[[CS333 Quantum Computing]]

# Class 1
BB84 algorithm [Quantum Cryptography - BB84 Cryptography Protocol](bear://x-callback-url/open-note?id=66C432F6-CF7F-4D8F-84E0-9A77E896ED54-2534-0000D1C11E55F802&header=BB84%20Cryptography%20Protocol)

*Questions:*
	* What makes it secure vs failing loudly?
	* Error correction

*Secret Sauce:*
	* Measurement - when you measure, you collapse state, if any info comes out, she alters polarization
	* Can’t copy state

[[CS333 PS3]]

<!-- {BearID:C3DAD9D9-76B2-43E2-AFF2-EFF552FFE010-3006-000130202C90844D} -->
[[CS333 Quantum Computing]]
# CS333 Week 4
#school #school/quantum


<!-- {BearID:87B5C850-8E7B-4E53-B068-F37B256B0E32-574-00013BCFC65BD8A8} -->
[[CS333 Quantum Computing]]
# CS 333 Week 5
[[Quantum Bomb Game]] [[Quantum Gates]]
Check out the class notebook!
Need to take more notes on single and double qubit states!

![[Pasted image 20210323100538.png]]
What happens! A lens?!

$\LARGE\ket{0}_P\ket{H}_A\rightarrow\LARGE\ket{+}_P\ket{H}_A$
As the input goes through a 45* lens to start with, basically makes state:

goes to a superposition/ gets beamsplit, research more about this
$$\LARGE
\frac{1}{\sqrt{2}}(\ket{0}_P\ket{H}_A+\ket{1}_P\ket{H}_A
$$

The top line goes through a 90* lens so the 
$$\LARGE
\frac{1}{\sqrt{2}}\ket{0}_P\ket{H}_A
$$
to $\ket{1}$

Then we have that the upper track hits the mirror and that part goes to: 
And finally the output state will be: 
$$\LARGE
\frac{1}{\sqrt{2}}\ket{1}_P(\ket{H}_A\ket{V}_A)
$$ 
The photon is still travelling in two discrete directions! If you then take a measurement, [[Quantum Measurement]] occurs and collapse happens

Skill to learn is [[Partial Quantum Measurement]]

Research Hidden variables![[006 School MOC]] [[005 Active MOC]]
# CS451 Machine Learning
#concepts/cs/ml
#school/ml #school/class #school/quantum


## Refs

Textbook - [A Course in Machine Learning](http://ciml.info)
Probabilistic ML - https://probml.github.io/pml-book/book1.html 

[[Machine Learning TOC]]
![[Pasted image 20210427171153.png]]
# Topics
1. [[Decision Trees]]
2. [[Binary Classification]]
3. [[Linear Regression]]
4. [[Multi-Class Classification]]

# Schedule
*Always watch the ahead of time videos!!!*
*One activity/ project due weekly!*

4-March (Thurs): Project possible direction
11-March: P - task definition
23-March: P - task set
		* Make 100 labels
		* Reflect on whether task is well defined
6-Apr: P - training set
		* Build up training set, ~300 more labels
		* Start solving the problem
13-Apr: Nothing!
22-Apr: P - analysis
		* Figure out which models to use, finish a few solutions
6-May: P - exploration
		* Zoom, enhance - expand!
20-May: P - 	Reflection
		* Pitch it to bossman!	

# Course Project
[[CS451 Machine Learning]]
# CS451 Project Exploration
#school 

## Task
I want to explore more deeply some of the custom algorithm implementations in this course. Given the fantastic success I've had so far with random forests, I would like to try to implement a custom RF Classifier/ Regressor, and see how close I can get it to SK's model.

## Implementation
To achieve this deeper exploration, I decided to create my own weighted ensemble that weights 100 DecisionTreeClassifiers and takes the average to arrive finally at a regression approximation. I initially experimented with using DTreeRegressors, but interestingly the final result is better taking the average vote of the Classifier rather than the regressor. 

I had to do a decent number of modifications on the code we were given in p14-ensembles.py to make it work with my data. First off, I had to go from binary classification to multi-class classification. This was pretty simple but involved a number of changes to get things to work. For a while my forest's accuracy was horrible while the trees were doing well, and I realized it was because I was trying to predict two classes with my forest while predicting 5 with my trees. Not sure how that even ran.


### Code
```py
@dataclass  
class WeightedEnsemble(RegressorMixin):  
  
    members: T.List\[T.Tuple\[float, T.Any\]\] \= field(default\_factory\=list)  
  
    def insert(self, weight: float, model: T.Any):  
        self.members.append((weight, model))  
  
    def predict\_one(self, x: np.ndarray) -> float:  
        vote\_sum = 0  
 for weight, clf in self.members:  
            vote\_sum += clf.predict(x)\[0\] \* weight  
        vote\_avg = vote\_sum / float(len(self.members))  
        return vote\_avg  
  
    def predict(self, X: np.ndarray) -> np.ndarray:  
        (N, \_) = X.shape  
        class\_votes = np.zeros((N, 1))  
        for weight, clf in self.members:  
            ys = clf.predict(X)  
            for i, y in enumerate(ys):  
                class\_votes\[i\] += y \* weight  
        class\_votes = class\_votes / len(self.members)  
        return class\_votes
```

```py
forest = WeightedEnsemble()  
(N, D) = X\_train.shape  
  
params = {'max\_depth': 25}  
  
for i in range(100):  
    \# bootstrap sample the training data  
 X\_sample, y\_sample = resample(X\_train, y\_train)  \# type:ignore  
  
 tree = DecisionTreeClassifier(\*\*params)  
    tree.fit(X\_sample, y\_sample)  
  
    weight = 1  
  
 \# hold onto it for voting  
 forest.insert(weight, tree)  
  
    tree\_num.append(i)  
    tree\_vali.append(tree.score(X\_test, y\_test))  
    forest\_vali.append(forest.score(X\_test, y\_test))  
    if i % 5 \== 0:  
        print("Tree\[{}\] = {:.3}".format(i, tree\_vali\[-1\]))  
        print("Forest\[{}\] = {:.3}".format(i, forest\_vali\[-1\]))
```

## Custom Implementation Learning Curve
I wanted to see how my model was doing when compared with the regular SKlearn implementation. Here is my custom learning curve:

![[Pasted image 20210516142419.png]]

This compares pretty well with the sklearn learning curve (Using same parameters where possible):

![[Pasted image 20210516143159.png]]

## Conclusion
I think I've succeeded in implementing a pretty effective random forest. Training my model takes significantly longer than training the sklearn model, but I could probably parallelize Dtree creation to make the process faster. I was honestly quite surprised my model ended up about as good as the sklearn model from an accuracy perspective.

Going through this process of creating my own ensemble ML implementation is has been incredibly helpful at building my intuition on how RFs make decisions. It also ended up improving some of the explainability for my main project model, since I could examine components a little more easily with a custom implementation. [[CS451 Machine Learning]]
# CS451 Project Feature Analysis
#school 

## Feature performance analysis
I accidentally conducted this analysis for the last checkpoint.
![[tree_importances.png]]
![[feature_removal_analysis.png]]

I've spent some time now playing with my features, and have pretty much set on the current picks. I removed a couple that led to easy gains. I have also now thought more about removing `ROE`, however I think this would be a negative change based on my analysis of the optimal model's learning curve with and without the variable. Performance is marginally worse with `ROE` included, however the learning curve suggests potential further gains from increasing my sample size. This is interesting, since my model had already seemingly hit that point before I removed a couple erroneous features.


## Appropriate Models 
I've done some thinking about exactly why the [[Random Forest]] [[Regression]] model performs the best and has the most consistently improving learning curve, and while I may be falling into the trap of misattributing causality (bad trap), I think the reason is because the way [[Decision Trees]] pick their turning points is quite similar to the way I intuitively labelled my data, so in effect, the RF model isn't necessarily the best at predicting company quality, it is the best at predicting *what I would predict* company quality to be.

This also raises a critical question about how accurate it is possible to get, given potential mislabelings on my part. I am now a little concerned that my mental state (eg excited/ tired/ concentrated) may have systematically influenced my judgements. I quite often have to make calls where I decide where a label will fall if it is on the boundary (e.g. something that could feasibly be 0 or 1). I probably am not perfectly consistent with this, and I bet this introduces some "intrinsic inaccuracy" that no advanced model would be able to eliminate. I don't think this is a solvable (or even particularly terrible) problem, however it does raise interesting questions about quality of subjective data and what kind of impact that can have when interacting with deterministic or quantitative analysis.

I am still keeping tabs of the hyperparameters for three other models besides this one, but at larger sample sizes improvements really leveled off so I am not considering these seriously anymore:
* Random Forest Classification
* MLPRegressor
* MLPClassifier

Linear models seem to be hardly better than random guessing. I'm not sure if I wasn't able to set them up correctly, but as I have tried a bunch and had no good results while having promising non-linear results, I've decided to not pursue these further.

## Proposal of Exploration Task
I am now going to try to do what I've been trying to do with this model all along, and see whether this predictive power I've nurtured is actually a valuable metric for analyzing companies. I already have most of the infrastructure in place to execute a portfolio of the highest quality companies.

I am now realizing that perhaps a more interesting metric than standalone company quality may be the derivative of quality, or change over time. I bet this is where more interesting and useful analysis could be conducted, but this is just a hunch so far, and it appears to be a difficult one to test without a considerable amount of additional work. I will see if my data allows me to do this, and try if possible.
# CURLF
#concepts/finance #inputs/companies
/Curaleaf is the largest US cannabis MSO, with operations in over 20 US states. We like Curaleaf for its focus on creating a geographically diversified, cost-competitive CPG business in the cannabis industry, with a top notch management./

[[XCAP 3Q20 CURLF]]
3Q end price: $7.32
Current price: $12.75
Current mkt cap: $8.435bn
top price: $9.43
mkt cap: $6.23bn
bottom price: $2.75
mkt cap: $1.81bn

##
[[Cannabis Arbitrage]]
# Calls

## Adil
- Freshie
	- SIC
	- Exploring lots of tracks

## Alex Demoly
- Junior
	- Worked PE
	- MATH neurosci
# Cannabis Arbitrage
<!-- #finance/algo -->
[[Quantitative Finance]]

Working on a pair trade on TCNNF and CURLF. Made some decent progress so far, I have a script that can run a backtest on historical data of stocks, and another script that manages pulling and saving market data from an API. 

# Variants
Successful result on the backtest run for *Cannabis Arbitrage2*

## Strategy:
Strategies are encoded using a defined JSON format:
```
strategy = {
	{
		"name": Strategy name
		"params": {
			"diff": {
				"stock1_threshold": 0.02,
				"stock1_risk": 1,
				"stock2_threshold": 0.02,
				"stock2_risk": 1,
			}
		}
	}
}
```


### Load data from file:
[[Data Manager]]  <- See this for more info on how data is *obtained* and *stored*.
```
def load_data_from_file(ticker1, ticker2):
		# loads data from a json text file into a pandas dataframe for further manipulation
```


### Process data:
```
def process_data(raw_data_dataframe):
		# Processes data and calculates percent change, SMAs etc. All statistics and data manipulation happen here.
```


### Run strategy backtest:
```
def backtest_strategy(strategy, data_dataframe):
		# Runs a backtest of the strategy on given data and outputs data object with returns calculated.
```
[[Radioactivity]]
# Carbon Dating
#concepts 
The practice of tracking ratios of Carbon-14 to Carbon-12 to determine the age of something. 
**Half life of C-14:** 5730 years

C-14 is just like C-12, but with two extra [[Neutrons]]. The two are [[Isotopes]]. C-14 is radioactive, so one neutron will eventually explode, emitting [[Electrons]] and a neutrino and becoming a nitrogen atom. We expect half of all C-14 to decay into C-12 in 5730 years, the half life.

[[063 Economics TOC]] [[200 Outputs MOC]]
# Carbon Tax

Absolutely carbon tax. I think the most important (and least european) aspect about a carbon tax as I see it would be to really put the focus on creating an open market for carbon credits and allow them to be priced fairly. This would also serve the double purpose of incentivizing private citizens to “wall off” certain portions of forest land (or eventually even plant trees etc) and create a sustainable, fair and equitable way for companies to transition away from carbon intensive processes, especially where it comes to agriculture and industry. Realistically though, if I proposed a carbon tax the EU would go along and figure out some way to “mandate” some abstract quantity, have it cost vast sums of money and create a new “biggest government agency ever” to oversee the process (where all it would have taken was a couple of companies to maintain a platform, and I know microsoft is currently working on an idea along these lines, and act as independent certifiers of “carbon-credit”worthy amounts of land. The financial incentive of the cost of the credits would itself create the market, in the process rewarding the very small-time rural landowners most hurt by current approaches to climate change policy.)

<!-- {BearID:57A0AA56-84CC-4E4D-A080-88470920782A-37181-0003AD041B8E8C13} -->

# Carbonomics Project
[[INTD 1242 Carbonomics]]
#school

* Talking lots about “Midd Energy 2028”  
## Project 1 - Data Analysis Application
Use existing reports nd research literature to create a model to estimate the soil organic carbon content of land lots that Midd leases to local farmers.
* Blue source forest management plan (forest example)
* Regen network - wilmot project (field example
* Middlebury College - Valley lands ecological evaluation

## Project 2 - Energy/ carbon footprint reduction
* Conduct an assessment of the various sub sources of Midd’s energy and carbon footprints and their relative controls to those footprints and to rank their impacts
* Research college’s history
* Review internal carbon tax proposal

## Project 3 - Asset Allocation (Reporting and Portfolio Management)
* Review existing reporting framework
* Introduce use of infographics and best practice
* AS IS vs TO BE
* Articulate carbon asset & investment portfolio recommendations in line with 2028 carbon neutrality
* Where can we reallocate to focus more on ESG?
	*
	
* Define what it means to have sustainable exposure!
* Discuss reporting characteristics, what is our footprint rn, how do we effectively track and mitigate?

* Linked with what Andrea is doing
* *Deliverables:*
	* Document in 2 weeks time
		* Audience up to trustees, Patton
	* Presentation
		* Class
	* Infographic
		* Expression of intent, directed at students 

* 2500 acres

Net value ~ $220k first 5 years, $150k after
cost: $3,000,000

* 40% leakage fee
* 20% management fee

$1k / acre of land






By wednesday: table of contents, sources, rough plan + team breakdown for that


* ESG exposure? 
* How should we track against a benchmark



# TODO:
* As is - financial statement/ reporting stuff

<!-- {BearID:E9C7A34F-F5E5-4F46-8AA3-95D135E0BB43-2637-000014B16C40BCA5} -->
[[071 XCAP MOC]]
# Carl Closing Letter

Dear Carl,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |            |
| -------------------- | ---------- |
| Amount Invested      | $700.00    |
| Shares Purchased     | 674.35     |
| Cash-out Share Price | $2.16      |
| Cash-out Value       | $ 1,456.60 |
| Return on Investment | 108%       |

- Amount Invested: $700.00
- Shares Purchased: 674.35
- Cash out share price: $2.16
- Cash out value: $1,456.60
- Return on Investment: 108%


Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL[[010 Mind MOC]]
# Causes of Stupidity
#concepts 

It may make more sense to try to avoid common causes of stupidity than to seek brilliance, in some cases.

## We're unintentionally stupid
See [[Cognitive Biases TOC]] - making consistent errors in judgement!

## Wrong information
We make judgements with an incomplete or incorrect grasp of a reality

## Wrong model
We use the wrong [[011 Mental Models MOC]] to judge situations

## Fail to learn
Not reflecting and improving application of models, choice of models

## "Looking" over "doing good"
Evolutionarily programmed to do what's easy vs what's right. Easier to signal being virtuous than to be virtuous. 
[[052 Physics TOC]]
# Chain reactions, nuclear reactors and atom bombs
#🌱 #concepts

Reactor vessel: Contains control rods, pressurizer heat transfer system
Containment Vessel: Contains the radioactive material in a nuclear reactor. Designed to contain all radioactivity. Big concrete sometimes
Fuel Rod: Contains radioactive material to fuel fission process
Control rod: prevents neutrons from causing too many fission reactions. Can be adjusted between the fuel rods to slow the reaction. Ideal number of neutrons absorbed is 1, otherwise it will start doubling and NUKE. U235 concentration is below critical mass (3% vs 70%) so could never truly become a nuke.
Moderator: Slows down neutrons so they are easier to force to collide. Tends to be water! [[Neutrons]]
*	Best way to slow down a particle is to have it collide with an equal mass particle

Chernobyl had a non-water moderator system, bad control rods and no containment system!

Nuclear cooling pools: Where rods are cooled down, block most of the radiation from coming out, help capture heat
#todo
- [x] [[Nucleus]]


# Charlie
[[200 Outputs MOC]]
Emphasize software and moat
How do we continue down the line?

software must upgrade!
manufact
Deal with spotify - drive usage of their service. Pandora apple etc.

Explain that we need value added from VC for value add with more than just selling a piece of equipment

<!-- {BearID:319DEF29-39DE-424E-8B7A-9B5A77A42838-11122-00001FC8696CEEF5} -->
[[071 XCAP MOC]]
# China Allocation Breakdown

- TCEHY -  0.9%
- BABA - 3.35%
- CNYA - 3.9%
	- TCEHY - 0%
	- BABA - 0%
- CQQQ - 2.17%
	- TCEHY - 10.38%
		- .225%
	- BABA 0%
		- 0%
- MCHI - 1.16%
	- TCEHY - 15.48%
		- .179%
	- BABA - 15.15%
		- .175%

- BABA Total: 3.52%
- TCEHY Total: 1.31%

After investing 8% extra in ESPO
- 8.77% TCEHY
	- +.69%
	- Net exposure up to 2% of portfolio
- 5.62% ATVI
	- +.48%
	- 6.51% currently
	- Up to 7% overall
[[053 Computer Science TOC]]

# Chomsky Hierarchy
#concepts/cs #TOC
The Chomsky Hierarchy is a way of grouping computational classes in order of the complexity required to solve them. 
[[CS301 Final Exam Core Reference]]
[[CS301 Quiz 10]]

## Hierarchy, in order of broadest to narrowest:
For a more detailed analysis of this, see [[Decidability TOC]], where properties of TMs correspond to [[Recursive Sets]], [[Recursively Enumerable Sets]], [[Co-RE Sets]].

### Neither Co-RE, nor RE
To say a set is neither Co-RE nor RE, means it isn’t possible to create a total UTM that accepts or recognizes these sets.
Can answer neither yes, nor no
	* `ALL = {M | L(M) = Sigma*}`
	* `FIN = {M | L(M) finite}`
	* `INF = {M | L(M) infinite} = FIN_not`
	* `TOTAL = {M | M is total}`


### Co-RE
Cannot answer yes, but can answer no
Set A is Co-RE if it’s complement `Sigma* - A RE`
	* `HP_bar = {M#x | M loops on x}`
	* `MP_bar = {M#x | M does not accept x}`
	* `EMPTY_bar = {M | L(M) not empty}`


### Recursively Enumerable [Turing Machines]
[[Recursively Enumerable Sets]] [[Turing Machines TOC]]
Set is RE if it is accepted by TM (Can answer yes)
RE sets are NOT closed under complement (See Recursive sets below).
We say that `set P semi-decidable if {x | P(x)} is RE`
	* `HP = {M#x | M halts on x}`
	* `MP = {M#x | M accepts x}`
	* `EMPTY = {M | L(M) empty}`


### Recursive Sets
[[Recursive Sets]]
Can say yes or no
A set is recursive if it is accepted by a total TM (a TM that always halts). 
We say that a `set P is decidable if {x | P(x)} recursive`
Recursive sets are closed under complement, so  `A recursive -> Sigma* - A recursive `
`A RE and A Co-RE -> A recursive`
	* `count the states in an encoding`
	* `M#x halts after i steps`
	* `takes more than I steps on some input`
	* `takes more than i steps on all inputs`
	* `ever moves its head more than i tape cells from tape start on inputk`
	
### Context Sensitive [Linear Bounded Automata]
[[Context-Sensitive Languages]]
	* Think of these as being impossible to calculate with a single stack
	* `{a^n b^n c^n | n > 0}`
	* `{0^n 1^m 0^n 1^m | n, m > 0}`
	* `{ww | w in {a,b}*}`
	* `{a^p | p is prime}`

### Context Free Language [Pushdown Automata & CFG]
[[Context-Free Language]]
	* Think of these as things you can do keeping track of one variable (or set of variables) in a stack, via pushing and popping only
	* `a^n b^n`
	* `{a,b}* - {ww | w in {a,b}*}`
		* `{x in {a,b}* | x is not a palindrome}`
	* `{a^m b^n | m, n >= 0, 5m - 3n = 24}`
	* `{x in {a,b}* | #a(x) > #b(x)}`
	* `{a^I b^j c^k | i,j,k >= 0, I=j or j=k}`
	* `{a^n b^n c^m d^m | n, m > 0}`

### Regular Expressions [DFAs / NFAs & regex]
[[Regular Expressions]]
	* Problems must be solvable in a finite number of states, without using a stack
	* `a* b*`
	* `{a^p | p is the smallest prime greater than 100000}`
	* `{0^m 1^n | m+n = 301}`
	* `{a^m b^n | m, n >= 0, 5m + 3n = 24}`

<!-- {BearID:DE53E066-1FCE-4568-B1CF-85963BDC4309-2136-000018EDFD3C0956} -->
[[062 Religion TOC]]
# Christianity
#concepts 
---
aliases: ["Classical Liberalist"]
---
[[010 Me MOC|Me]] | [[064 Politics TOC]] | [[063 Economics TOC]] | [[061 Philosophy TOC]]
# Classical Liberalism
#concepts 

A political ideology. Advocates for civil liberties under the rule of law, with an emphasis on economic freedom. The ideology builds on [[Classical Economics]] as defined by [[Adam Smith]] in the [[Smith - The Wealth Of Nations]].

The British Tradition of Classical Liberalism emphasises empiricism and focus on the common law, as well as endorsing the claim that traditions and instutitions evolve spontaneously and are not perfectly understood. 

Most of my view on this topic is shaped by the views and writings of [[Friedrich Hayek]]. 

It seems that a major focus of the ideology is the preservation of [[Individualism]] and personal freedom. It's interesting to see the overlap between what is now [[Conservatism]] and CL - almost startling how similar the concepts are tbh. At the same time, they are diametrically opposed in several ways. The first and foremost one that comes to mind is the absence of the rigidity to change in CL that we observe in conservatism. It's fantastically unhelpful 


## Decentralized Democratic Techno-Liberalism
Interestingly, I think [[Technology]] may somewhat reframe this whole debate. Obviously nothing is ever "different this time", however I think through technology we can achieve a more [[Decentralization]] society that in effect is more socialized, although it would actually achieve this through an increased focus on [[Individualism]] rather than [[Collectivism]], and the fact that technology can empower [[Systems Theory TOC|Systems]] through [[Emergence]]. VERY interestingly, this actually seems to be one of the core arguments made by [[Friedrich Hayek|Hayek]] more broadly, at least with respect to emergent institutions (As argued in the ideology of [[Classical Liberalism]]), especially as seen in [[Markets]].


## Notable [[030 People MOC]]
[[Friedrich Hayek]]
[[John Locke]]
[[Thomas Malthus]]
[[Adam Smith]]
[[David Ricardo]]# Classical Mechanics
[[000 Life MOC]]
<!-- #Todo -->
[[PHYS106 Physics for Educated Citizens]]
[[Newton’s Laws]]

## Circular Acceleration
Acceleration for constant speed around a circle: `a = (v^2)/r`

## Archimedes’ Principle
Archimedes’ principle states that the upward buoyant force that is exerted on a body immersed in a fluid, whether fully or partially, is equal to the weight of the fluid that the body displaces.

<!-- {BearID:FA050B51-A53E-4769-BAD7-6714961088C7-3006-0000CB1AF90DC9E6} -->
[[300 Inputs MOC]]
# [[James Clear]] - Atomic Habits
#inputs/books 

Do this![[071 XCAP MOC]]
# Closing Letter Template
#projects/old/xcap 

Dear _____,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |     |
| -------------------- | --- |
| Amount Invested      |     |
| Shares Purchased     |     |
| Cash-out Share Price |     |
| Cash-out Value       |     |
| Return on Investment |     |


Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL[[053 Computer Science TOC]]
# Co-RE Sets
#concepts/cs

Another member of the [[Chomsky Hierarchy TOC]], these sets are the complement of [[Recursively Enumerable Sets]]. These sets are not closed under complement.

## Examples
These are generally the inverse of sets we know to be RE, for example:
	* NULL: `{M | L(M) = empty set}`
	* MP_not: `{M#x | M does not accept x}`
	* HP_not: `{M#x | M loops on x} = {M#x | M does not halt on x}`

<!-- {BearID:554C58EE-AB99-448D-A694-609F753B1AA8-5857-00012F6B45030440} -->
[[090 Lists MOC]]
# Codes
Fic- 090517
Fic makerspace- 101870

<!-- {BearID:F4DB4B97-2F29-4B93-A67B-9BDA0D1BAE56-754-000000920EA0DF4A} -->
[[011 Mental Models MOC]] | [[054 Neuroscience MOC]] | [[056 Psychology TOC]] | [[Heuristics and Biases]]
# Cognitive Biases MOC
#concepts/mental_models 

Cognitive biases are the gaps between theoretically optimal decision making, and the actual decisions humans make. Many of these are direct results of [[Heuristics TOC]]. 

Cognitive biases can be broadly attributed to [[Biologically Innate Human Tendencies]], which in turn are the results of a combination of [[Evolution]] and ongoing [[Social Influence]] or pressures.

There is a very close connection between the idea of cognitive biases, and [[Heuristics and Biases]], which would be a slightly broader category. I need to do a bit more mental organization between these two principles so they're a bit more separated. 

## List of Biases
- [[Recency Bias]]
- [[Survivorship Bias]]
- [[Selection Bias]]
- [[Confirmation Bias]]
- [[Substitution Bias]]
- [[Overconfidence Bias]]
- [[Overoptimism Bias]]
- [[WYSIATI Bias]]
- [[Representativeness Bias]]
- [[Fundamental Attribution Bias]]
- [[Framing]] and [[Priming]] - closely linked
- [[Planning Fallacy]]
[[Cognitive Biases TOC]]
# Cognitive Ease
#concepts/mental_models #concepts/cognitive_biases 

Cognitive ease is a measure of how easy it is for our brains to process information, and that by extension we will always prefer the easier way initially. Being forced to go a harder route (Often by having to switch from [[System 1]] to [[System 2]]) causes alarm and suspicion. 

This is partially attributable to the [[Energy Saving Brain]].

[[061 Philosophy TOC]]
# Collectivism
#concepts #concepts/philosophy #🌲 

Collectivism is the favoring of the group over the individualism. It is the opposite of [[Individualism]]. Collectivism entails the type of [[Central Planning]] necessary to achieve given distributive ideals. 

Collectivism, according to [[Friedrich Hayek|Hayek]], refers to some of the numerous methods that can be used of attaining the ends of [[Socialism]]. Interestingly, he goes on to argue that it is in fact collectivism that leads to the erosion of [[Freedom]], rather than the ultimate goal of [[Socialism]]. However, since collectivism happens to be a necessary method through which socialism is said to be achieved, this in effect makes the path to socialism untenable. 

## Decentralized Democratic Techno-Liberalism
Interestingly, I think [[Technology]] may somewhat reframe this whole debate. Obviously nothing is ever completely "different this time", however I think through technology we can achieve a more [[Decentralization|Decentralized]] society that in effect is more socialized, although it would actually achieve this through an increased focus on [[Individualism]] rather than [[Collectivism]], and the fact that technology can empower [[Systems Theory TOC|Systems]] through [[Emergence]]. VERY interestingly, this actually seems to be one of the core arguments made by [[Friedrich Hayek|Hayek]] more broadly, at least with respect to emergent institutions (As argued in the ideology of [[Classical Liberalism]]), especially as seen in [[Markets]]. 

This is a core thought, I should ponder and learn about this more. There is already a running idea called technoliberalism that this aligns with fairly closely. In fact, the decentralized and democratic aspects are mostly implied.[[051 Math TOC]]
# Combinatorial Explosion
#concepts/math

This concept concerns the rapid growth in the complexity of a problem due to how the [[Combinatorics]] of the problem are affected by inputs, bounds and constraints of said problem.

[[071 XCAP MOC]]
# Commitments
#projects/old/xcap
- Cam: 600
- Josh: 500
- Raf: 1000
- Guo: 500
- Henry: 100
- Sergio: 500


Total: $3200

<!-- {BearID:9B5EEA69-B400-4586-B78E-2A9CF423A92A-303-0000154C0866F6DB} -->
[[Algorithms And Complexity]]
# Comp Sci overall reflection
#school #school/cs
I learned a significant amount about the functions and processes of a number of algorithmic approaches. A lot of my understandings of discrete math changed a lot, as well as my ability to apply abstract problems to real solutions with real uses. The content we studied really allowed me to get a feel for the broader applicability of a lot of the content we covered like dynamic programming, closest points and graph theory. I’m already thinking about how to start working these ideas into projects I’m working on with Henry Mound from our class!

I had a number of concrete methodology and skill takeaways from the course that I will take with me. Firstly, I now have a solid grasp of dynamic programming. I feel like I struggled with writing invariants to prove my algorithms to start with, but after applying a more measured approach to thinking about the problems, I was able to focus on the overall meaning and thus begin to write better invariants. 

Second, I now have a great intuitive feel for graphs and how to traverse them. It took me a while to begin to fully understand the benefits and drawbacks of each of the search algorithms we learned, especially in terms of connecting the theoretical benefits with the algorithms’ constructions. I feel like I was able to soundly grasp this however, and I will be able to make me own decisions when implementing these types of algorithms to ensure I am using the optimal solution for my use case.

Third, my proof skills in general have improved greatly. When I started this course in the winter, I was not confident with consistently writing inductive proofs. Now, I just realized that invariants seemed easy to learn when I thought of them in terms of inductive proofs! It feels awesome to have such an intuitive understanding of the way you first create a base assumption to go off of, then just prove you can always build on it or break it down to prove whatever it is you want to prove. This is not something that made such clear sense to me in any of my prior computer science or math courses. 

My process for approaching problems also changed a lot. I used to rush into my solutions when I started, without thinking about alternative solutions or different ways of thinking about the problem. This often resulted in me getting derailed doing something I shouldn’t be. I have learned to take a more thoughtful approach when planning my answers, and I now just allow my ideas to create outside-the-box solutions to some problems, without completely derailing the correct way of thinking.

Overall I progressed significantly as a computer scientist and student in general. I am grateful for the constant work, even though it may have been unpleasant at the time, as I now can truly say I feel confident with a variety of techniques and approaches to solving problems.

<!-- {BearID:251D6CFA-6D79-4E33-9CB6-80A4AFC12E10-1155-000039576D46F97A} -->
[[070 Finance MOC]]
# Company Research
## Known companies
- [[TCNNF]]
- [[CURLF]]
- [[NMIH]]


* Aerofarms [[Aerofarms]]
* Aramesco - solar company?
* Iberdrola
* Nextera
* Enel 
* Fintech and stripe - funding carbon capture
	* limeworks
	* carboncure
	* project vesta
* Nuscale - small size nuclear
* Skeleton - patent for graphene batteries 

* Who will be the next winners of China’s growing middle class??

* DNMR - Danimer Scientific
	* Renewable plastic maker
	* Probably levered cyclical af
	* Run up???
	* w

<!-- {BearID:3746BDA2-53DF-4BF0-9506-7A05D2000323-13443-000007A37B76D204} -->
[[051 Math TOC]] | [[Linear Algebra TOC]]
# Complex Conjugate
#concepts/cs/ml 

In mathematics, the **complex conjugate** of a [[Complex Numbers|Complex]] number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign.

## Definition
If
$$
\ket{\phi}=\begin{pmatrix}\alpha \\ \beta\end{pmatrix}
$$
Then:
$$
\bra{\phi}=\begin{pmatrix}\alpha* & \beta*\end{pmatrix}
$$

This is used for the [[Conjugate Transpose]] matrix operation, and seen in the [[Inner Products]] and [[Outer Products]] operations.
---
aliases: ["Complex"]
---
[[051 Math TOC]] | [[Quantum Mechanics TOC]]
# Complex Numbers
#concepts
Complex numbers make up the broadest set of numbers. The set is closed under all arithmetic operations.

Complex numbers are made up of two elements, a real part and an imaginary part: `C = a + bi`

## Complex Conjugate
The complex conjugate, or conjugate transpose, for a given d-dimensional column vector $x= a+bi$ is a d-dimensional row vector $x^T=a-bi$

## Euler's Formula
Euler's formula allows us to represent any complex number as a complex exponential:
$$e^{iw}=cos(w)+isin(w)$$[[053 Computer Science TOC]]
# Complexity Classes
#concepts/cs #TOC 

## P
[[Deterministic Polynomial Time]]
If there exists a polynomial-time DTM M such that L(M) = A
*Examples:*
	* Sorting is O(nlogn)
	* Matrix multiplication is under O(n^3)
	* CKY is O(n^3)
	* Finding an Euler Tour of a graph G = (V, E), ie a tour that traverses every edge of G exactly once is O(|E|)

## NP
[[Nondeterministic Polynomial Time]]
If there exists a polynomial-time NTM M such that L(M) = A
*Examples:*
	* Graph isomorphism - if two graphs are identical, but relabeled and repositioned.
	* Prime factorization

## NP-hard
B is NP-hard if for all A in NP, A <= B via a polynomial time reduction
See [[Reductions]]. Most of the reductions we’ve learned aren’t done in polynomial time.
*Examples:*
	* Traveling salesperson *optimization* problem - given a graph with distances between each pair of vertices, what is the minimum cost tour that visits every vertex once?

## NP-complete
If B is in NP and B is NP-hard. All NP complete problems are equally hard.
*Examples:*
	* Hamiltonian circuit - is there a cycle for a given graph where each vertex is traversed exactly once?
	* Boolean satisfiability 
	* Vertex cover decision problem - a subset of the vertices of a graph that contains at least one endpoint of every edge.
	* Traveling salesperson *decision* problem - is there a circuit that visits every vertex at least once with cost at most B for some bound B?
[[Todo]] #todo Transcribe into LaTeX from bear image![[054 Biology MOC]] | [[053 Computer Science TOC]]
# Computational Biology
#concepts/biology #concepts/cs 

- [[DNA]] sequencing/ inference, as seen with [[Minimum Edit Distance Problem]].


---
aliases: ["Concepts", "Concept"]
---
[[001 Meta MOC]]
# Concept Notes
#concepts #concepts/definition 

## Concept Note Definition
Concept notes are the most atomic unit of information, knowledge or wisdom. They should live and breathe, linking to other concepts and linked & expanded upon by [[Bridge Notes]]. Concepts should be [[Evergreen Notes]][[Heuristics and Biases]] | [[Cognitive Biases TOC]]
# Confirmation Bias
#concepts/cognitive_biases 

Confirmation bias is a [[Biologically Innate Human Tendencies]] that leads people to search for information that confirms their existing views. It can cause us to ignoreimportant information
[[051 Math TOC]] | [[Linear Algebra TOC]]
# Conjugate Transpose
#concepts/cs/ml 

The conjugate transpose is an operation on [[Matrices]] that takes the [[Transpose]] of a matrix and takes the [[Complex Conjugate]] of each item.
[[2021-04-13]]---
aliases: ["Consciously"]
---
[[000 Life MOC|Life]] | [[010 Mind MOC]]
# Consciousness
#concepts #🌲 

Consciousness is a crazy thing that I need to delve into and try to analyze. Wikipedia has it aptly captured as being "at once the most familiar and most mysterious aspect of [[000 Life MOC|Life]]." 

My model of consciousness is a little hazy and still in development, but I believe consciousness arises when all the *Mental Systems* work in tandem. I need to think about each and try to understand them better.

- [[Experiencing Self]]
- [[Remembering Self]]
- [[]][[053 Computer Science TOC]]
# Context-Free Grammars
#concepts/cs

An element of the [[Chomsky Hierarchy TOC]] that can be used to define a [[Context-Free Language]]

The goal of grammars is to systematically define a way to start with a start string S and systematically derive a string of all terminal symbols, or a CFL.

Note, that as proven in our section on [[Reductions]], we cannot create a UTM that determines whether a language L(M) for some TM M is Context-Free. For more on this, see [[Turing Machines TOC]][[Decidability TOC]]

## Formal Definitions
A context-free grammar (CFG) is a tuple 
`G = (N, Sigma, P, S)` where:
	* N is a finite set of *nonterminals*
	* Sigma is a finite set of *terminals*
	* P are *productions*, a set of rules where each production maps a symbol of N to a sequence of nonterminals and terminals that describe the form the nonterminal on the LHS can take.
	* S in N is the *start symbol*

## Conventions
	* Nonterminals A, B, C in N
	* Terminals a, b, c in Sigma
	* Sentential forms alpha, beta, gamma in (N u Sigma)*
	* Sentence w, x, y, z, strings in Sigma*

[[Normal Forms]]

<!-- {BearID:579EBF80-C128-4B22-A3BE-966A9C48EA0A-37181-00039465005E526C} -->
[[053 Computer Science TOC]]
# Context-Free Language
#concepts/cs

A CFL is a member of the [[Chomsky Hierarchy TOC]]
For describing CFLs, see [[Context-Free Grammars]]

All CFLs are also [[Context-Sensitive Languages]] and are equivalent to Nondeterministic Pushdown Automata (but not Deterministic Pushdown Automata), with DPDAs only accepting a proper subset of CFLs: deterministic/ unambiguous grammars.

## Closure Properties of CFLs
CFLs *are* closed under:
	* union
	* reversal
	* concatenation
	* Kleene star *

CFLs are *not* closed under intersection!! Can intersect two valid CFLs, and as a result get too many vars to track.

## Proving Context Free (or not)
Use pumping lemma!

## Examples
```
{ 0^i 1^j 2^k | i, j, k >= 0, i=j or j=k
{0^n 1^n 1^m 0^m | n, m > 0}

```

<!-- {BearID:791E8EAC-F608-4E29-8F6F-CD46F8D034DD-2136-00001B77220F8C4B} -->
[[053 Computer Science TOC]]
# Context-Sensitive Languages
#concepts/cs

A member of the [[Chomsky Hierarchy TOC]]

## Examples
```
{0^n 1^m 0^n 1^m | n, m > 0}


```

[[Machine Learning TOC]]
# Coordinate Ascent
#concepts/cs/ml 

Simple ML training model

Seeks to optimize things, need Loss function
Not very fast since it compares countless hyperplanes

[[Stochastic Gradient Descent]] is a more powerful variant# Copenhagen Interpretation of Quantum Mechanics
<!-- #_reference/science -->
[[000 Life MOC]]
* This is one of the prevailing [Interpretations](bear://x-callback-url/open-note?id=BA5746A1-0EFC-4D51-8538-8E7ABF2C6DCA-37181-0002BEA513EB67D1) of [[Quantum Mechanics TOC]] that states that, on a microscopic level, material objects do not have definite properties prior to measurement. Rather, they are described by probabilistic wave functions that collapse on observation into one of the potential outcomes. The act of measurement, under this theory, affects the observed system itself.

<!-- {BearID:756A4C6B-F897-453A-8665-F93B6C443EDD-37181-0002BFC442DB76FB} -->
# Core Investment Ideology
[[XCAP Writing]]
#projects/old/xcap

The core purpose of the fund is to take advantage of our understanding of the secular shifts in society to find investments that will benefit from the ongoing changes. In particular, we try to identify companies that have a good potential to leverage technological progress to do new and exciting things that will benefit from the demographic and socio-economic changes happening around us. We have identified a number of core business models that we believe have a particular chance to outperform including financial services, e-commerce platforms, information services and high-tech manufacturing. 

We specialize in understanding industries that will be changed by the increase in influence of GenX/Y/Z in the buying power and political capital of the United States. A preference for online interaction and presence mean a very different breed of consumer is starting to earn enough money to have a serious impact. This change will enable a number of exciting growth industries that will change the way people lead their lives in significant ways. We aim to identify the companies that will invariably benefit from this over the next decade. 

Every investment should aim to identify a strong growth opportunity in a  an industry with strong exposure to these secular shifts. There should be a preference for high quality names that have an indispensable value-add that can be resilient to changing market conditions.

<!-- {BearID:8856D553-60D3-47CF-9E91-8D2D364F01FB-304-00001A9CD89D7961} -->
[[010 Mind MOC]] | [[061 Philosophy TOC]]
# Cosmogeny
#concepts/philosophy #concepts

Any model concerning the origin of the cosmos or universe. I should really delve into this more. [[God]] has something to do with it. [[Quantum Mechanics TOC]] also does, in another way.

I could probably also eventually build a connection to [[Decision Machines]], although that would be a concrete implementation, or rather cosmogeny could serve as part of the philosophical component to making such a machine.

[[053 Computer Science TOC]]
# Cryptography
#concepts


# Unconditionally Secure - Secret Key Protocol
0.  A + B share secret key s in {0, 1}^n
1. A creates encoded message m_bar by XORing the message with secret key s
	1. m_bar = m_i (XOR) s_i
2. A sends m_bar to Bob
3. B decrypts m_bar by setting
	1. m_i = m_bar (XOR) s_i

This process is very secure, however it is *tough to get secret key out*! Can’t do it with unknown actor!
Qubits handle step 0

Solution? *QUANTUM!!!* [[Quantum Cryptography]]
# Daily Log 2020/10/14
#outputs/log
*Next log:* [[Daily Log 2020\/10\/15]]
	* Decision engine theory, self-learning on impulse [[Decision Machines]]
## New Matrix Algebra page
[[Statistics TOC]]
Wrote a great new personal reference on matrix algebra [[Linear Algebra TOC]] [[Linear Regression]]
Gotta keep it in my mind as I move through this semester…
I’m pretty excited to get a better grasp of a lot of this stuff. It’s really exciting to see especially where this is applied in metrics and stats. I intend to create sheets of this quality explaining concepts from all my classes for future personal reference, and also as a study aid. 

## Bear VS physical note taking
[[Zettelkasten]]
[[Thoughts on note taking]]
This is a big conversation I’ve been having with myself that I still can’t get to the bottom of. The big issue at hand is that writing things is better for learning than typing, so I sacrifice some ultimate efficiency for the convenience of universal access. Honestly I think for a lot of this referential stuff like above this isn’t too big of an issue, but it might be a problem for learning things generally. 

The only thing I can think is that the upshot is really good synchronization with the way the mind stores and retrieves information, linking together common concepts in the process. This might be a good way for me to start building my “train” metaphor, in fact perhaps a much more effective way than I intended. Perhaps the physical act of me thinking, planning, articulating and most importantly *linking* concepts will counteract the loss from experiential learning.

I suppose only time will tell, but I will be revisiting this question in the future. Future me: did it work? I’m sure that in reality (unfortunately lol) some mix of the two media is the ideal way forward. The question is where to draw the line, and for my mental state’s sake, I hope that line is really clear in differentiating between:

* Bear
* Notability
* Goodnotes
* Notebooks

Honestly not an ideal setup as things stand. 

Perhaps I can plan to streamline a workflow out of this, so that I say take class and meeting notes in notability, then transcribe them to bear for mental categorization. Goodnotes and bear seem to occupy a somewhat overlapping use case, with the overwhelming deficiency of the lack of a referential organization system for GN. Notebooks certainly have their place, and I shall perhaps reserve my more personal observations for them.

## Reference core document
Honestly might also be a good idea to try to create a core or home document for the whole referencing system, where I can have a big appendix of all the ideas in my train of thought for easy accessing. Could be a whole deep index within an index wiki type of deal, would be neat. I wonder what the capacity of a note is, prolly pretty large. I’ll get to this tomorrow tho, have exam now lol.

<!-- {BearID:305B5582-F758-4518-8BE9-903224516EA2-37181-000239F40B716611} -->
# Daily Log 2020/10/16

*Last log:* [[Daily Log 2020\/10\/15]]
*Next log:* 
Lovely Friday today, finished up my exams yesterday and today. Decided to spend a bit of time hanging out in my room expanding my knowledge of [[Quantum Mechanics TOC]]. Wrote a whole bunch of nice new notes on this, It’s quite fun linking it all together. I guess at this point I’m gonna try creating a real, big [[Zettelkasten]] system here in bear, and use that to both categorize my thoughts, come to grips with new concepts, and create a large knowledge reference for myself linked in a way that resembles the real cognitive links of the content in question. Let’s see how this goes…




## Notes created today:
[[Quantum Mechanics TOC]]
	[[Interpretations of Quantum Mechanics]]
	[[Bell’s Theorem]]
	[[Superdeterminism]]
[[Systems Theory TOC]]

<!-- {BearID:2B72E8EE-887C-42E8-9117-BE1861B3273D-37181-0002BF22D1820AAA} -->
[[095 Journals MOC]]
# Daily Log <% tp.file.title %>
#outputs/log

[[<%tp.date.now("YYYY-MM-DD",-1)%>]] < + > [[<%tp.date.now("YYYY-MM-DD",+1)%>]]



## Tasks
#todo 
- [ ]

## Notes Worked On
- [ ]

#🌱 #concepts
# Daily Notes

I should talk some more about exactly what I think should go into these daily notes. Lots of power here, but I gotta keep things consistent and organized.
# Daily Plan 2021/02/01
[[095 Journals MOC]]

Lots of work to do today.

1. Give feedback on strategy reviews for xcap
2. Fix portfolio + subsystem allocation in the system
3. Double check subsystem works rn

<!-- {BearID:D4B2926D-D8F6-41E8-AD4F-16EBC431CD44-2637-00008DA0D4D10427} -->
[[030 People MOC]]
# Daniel Kahneman
#people 

## Notable Work
[[Kahneman - Thinking Fast and Slow]]
[[Prospect Theory]][[095 Journals MOC]]
# PRJCT Ams order

Dear Alexandro,

I’ve thought about it and decided the items I would like. 

1.  The blue herringbone pants on the far right of the ones you sent, and the grey one second from the left
2. The striped black pants like in the photo you posted on April 22, I’ll attach a picture, maybe it’s called the scuba herringbone. 
3. The grey pinstripe pants sent later as we texted about
4. Dark blue suede shorts
5. Silver suede shorts
6. 3 suede T-shirt’s, both of the beiges and dark blue
7. 2 T-shirts in that super light synthetic style, not the one in elastin! In beige and white
8. Last I would like to get 3 T-shirt’s for my girlfriend. She’s a size extra small, and I want to get her some different choices that you would recommend. If you have one in blue that would be great as part of that.

<!-- {BearID:DA46A176-DAAF-417F-8EB3-042B008A8559-2499-000002A611672E61} -->
---
aliases: ["Decentralized"]
---
[[011 Mental Models MOC]] | [[Systems Theory TOC]]
# Decentralization
#concepts# Decidability
#concepts/cs #TOC
For a computation hierarchy of these sets and more, see the [[Chomsky Hierarchy TOC]]

## Decidable properties
We say properties of TMs are decidable, we mean there exists a total, universal TM that can decide whether any arbitrary TM has that property (can answer yes/ no). Properties of systems are decidable when we can say either yes or no to them. Sets of decidable properties are also [[Recursive Sets]]

A quick way to think about this is that these properties usually pertain to the TM in question’s actual operation, rather than asking about the set described by the TM.
*Examples:*
Whether a TM:
	* Has at least 301 states
	* Takes more than i steps on input x
	* Takes more than i steps on some input
	* Takes more than i steps on all inputs
	* Ever moves its head more than i tape cells away from tape start on input null string E

## Semi-decidable properties
Properties of systems are semi-decidable when we can say yes, but not necessarily no. 
Sets of semi-decidable properties are also [[Recursively Enumerable Sets]]. The complements of these sets are called [[Co-RE Sets]]. Semi-decidable properties may also be decidable or undecidable.
See [[Turing Machines TOC]]
*Examples:*
	* Does a given TM accept any strings?  
	* Does a given TM accept the null string?
	* Is a given input string the encoding of a TM M and a string x where M accepts/ halts on x?
		* See [Turing Machines - Universal Turing Machine](bear://x-callback-url/open-note?id=148A123B-B831-4683-A9B7-B902DDE5742B-5857-00012BA642B359C9&header=Universal%20Turing%20Machine)
		* A UTM can simulate M#x and say whether M accepts or halts on x, but not whether it does not accept or does not halt on x - making these properties semi-decidable because there is no way to tell if the UTM is looping - see [[Halting Problem]]

## Undecidable properties
	Questions that cannot be truly answered by an algorithm. We would generally consider semi-decidable properties to be undecidable, unless they are explicitly decidable. These exist on a number of levels, and require meta-thinking to understand. We will also be thinking in higher levels of abstractions to study undecidability, considering sets of [[Turing Machines]] rather than sets of strings.

	Undecidability occurs when TMs are not able to give a definitive answer to a question. 

	These questions can be determined as sets that are neither [[Recursively Enumerable Sets]] nor [[Co-RE Sets]].

	* *When we say a property of TMs is undecidable:* We mean that there is no total UTM that can answer yes or no for some encoding M#x
	
Our usual undecidable baseline is the [[Halting Problem]]. See that page for the proof that it is undecidable via diagonalization. For others we prove undecidability through [[Reductions

*HACK:* We can think of this to mean that any non-trivial properties of RE sets are undecidable! This means these ideas are related more to the properties of the set themselves, rather than the TM that accepts the RE set. There can be many TMs that accept the same RE set! Note that all RE sets are accepted by TMs.

Note that it is impossible to truly deduce aspects of sets by just observing the TMs that accept them, for example:
``` python
def TM1(x):
	return x[0] == 1

def TM2(x):
	if x[0] == 1:
		return True
	while True:
		pass
```
These TMs both accept the same set {x | x[0] == 1}, but only the first rejects all other strings, the other loops. 

This is an application of *Rice’s Theorem*
*Examples of undecidable properties of RE sets:*
Whether a TM:
	* Accepts the null string E
	* Accepts any string at all
	* Accepts every string
	* Accepts an infinite set
	* Accepts a regular set
	* Accepts a CFL
	* Accepts a recursive set

*Proofs:* We use [[Reductions]] to prove properties are equivalent, and simply show the property in question reduces to one we know is undecidable to prove.

<!-- {BearID:A95B9AEC-4AE3-49DE-934D-A26FABB92339-5857-00013869E55DA986} -->
[[011 Mental Models MOC]] | [[054 Biology MOC]] | [[053 Computer Science TOC]]
# Decision Machines
[[053 Computer Science TOC]] [[061 Philosophy TOC]]
#outputs/thoughts 
#concepts/cs
#🌲 

I’ve spent a lot of time thinking about how to create a machine that thinks for itself, and one of my ideas so far has been making decisions based on trying to minimize discomfort (maximize comfort). I have a sense that a lot of what I will learn by connecting [[011 Mental Models MOC]]

## Components:
I think a component-based system that blends necessary elements will be the most successful approach. In general, by going this route, I think I will be drawing an aweful lot of inspiration from humans, in a sense. I am definitely thinking about this in a bottom-up, [[Emergence]] sort of way, where I expect that a number of "simple" systems working in conjunction could create an incredibly powerful result.

- Input stream monitoring for changes to state. This should be from multiple "sensors". We have our senses.
- Grouping, simplification and abstraction of actions
- Simulation of reality (like imagination?)
	- Yes but actually this can be broken into making simple predictions. I really like the connection here with prediction-based [[Neural Networks]]. For more on this, see [[Learning Algorithms#Predictive Coding]]
- Attention. Areas of the neural system that are brought "to attention" should be both more malleable and more responsible for actions.

I think a very key thing here is what is discussed in the Simons Institute's work on computational neuroscience, and simulating neural systems from first principles. If I can define a rough structure, I could probably start creating groupings (and allow others to emerge naturally) that would lead to the necessary learning processes happening.


Could this be a way to make true AI? The key would be to enable it to learn and understand in an emergent way. Giving it the goal of minimizing discomfort allows for the machine to learn towards something


* Definitely need to learn a lot more about systems theory. [[Systems Theory TOC]]

## Emergent Properties
- Awareness of possible actions - this is an emergent property actually, not a component.
- Gradual learning of the ramifications of actions. Linking [[Machine Learning TOC]][[CS451 Machine Learning]] | [[ML Models TOC]]
# Decision Trees
#concepts/cs/ml


Really robust method of AI for some applications - eg survival % of passengers on titanic based on gender, age and class

Decision trees are non linear models - they can answer questions like the XOR problem! Grouping things that can’t be divided with a single straight line.

## Speed
O(Nlog(D)) decisions occur - tree structure!!!

## Possible Features
Really easy to use all kinds of feature types - very extensible, all sort of works intuitively. 

* Bools
* Enum
* Int/ float
* Missing data can be treated as a separate feature - makes things more accurate


# Core Vocabulary
* Classification Trees
* Regression Trees
* CART Trees - Classification And Regression Trees


# How to learn a tree recursively(data):
```
def learn_recursively(data):
	is all the data the same label?
		predict that label
		DONE
	for each feature:
		for each possible split point in data:
			if this split is the best so far:
				hold onto it
	split the data at the best point into left and right
		learn  a tree recursively(left)
		learn a tree recursively(right)

```
Must define what best means: using loss function
	* A least squares (L2) loss function makes this the same as a regression!
	
This gives a [[Binary Classification]] Tree! But can be generalized to [[Multi-Class Classification]].

# Implementation - sklearn
```
# Import classifier
from sklearn.tree import DecisionTreeClassifier

# Create classifier instance
f = DecisionTreeClassifier(
    splitter="best",
    max_features=None,
    criterion="gini",
    max_depth=None,
    random_state=13,
)  # type:ignore

# train the tree!
f.fit(train_X, train_y)
```

[[005 Active MOC]]
# Degree Plan
[[006 School MOC]]

So how TF do I graduate?

## Distro reqs - both
- PHL - Philosophy or Religious studies
- SCI - Physical and life sciences

## Geog reqs - 1 more
- SOA
- NOA
- MDE
- SAF


## CS major
- Elective 1 - OOP & GUI
- Elective 2 - ML
- Elective 3 - Advanced study? Otherwise drone robotics
- Senior Seminar - FML

## Econ major
- DOnezo boi!~!~!!!!


Ideas:

*RELI 0254*
*RELI 0228*

<!-- {BearID:F5F23800-B159-4D07-B090-13FFD3DA825F-5857-0000283585AEC3E8} -->
[[030 People MOC]]
# Demis Hassabis
#people 

Interesting guy. Don't know much about him. Should read more abotu his other work.

## Work
[[Hassabis - Neuroscience Inspired AI]]
- Really loved this piece. Great summary of the field and lots of references for further reading. Need to go parse them.[[053 Computer Science TOC]]
# Deterministic Polynomial Time
#concepts/cs

The set of problems solvable by a DTM in polynomial time, or in other words problems where the length of computation is some polynomial function of the input length. For more on DTMs, see [Turing Machines - Time bounded computation](bear://x-callback-url/open-note?id=148A123B-B831-4683-A9B7-B902DDE5742B-5857-00012BA642B359C9&header=Time%20bounded%20computation)

<!-- {BearID:650F6D79-C550-4450-8A72-E0A49F961796-5857-00014B242E15C90E} -->
[[Quantum Computing TOC|Quantum Computing]] | [[Quantum Algorithms]]
# Deutsch's Algorithm
#concepts
[[Quantum Gates]]

## Problem
Problem: Given one-bit function f, decide if f is even or balanced.
With quantum can solve with one f-gate using [[Superposition]] and [[Phase Kickback]], classically we need 2.

| X   | f(x) |
| --- | ---- |
| 0   | y    |
| 0   | y    |
Four different possible one-bitters:
![[Pasted image 20210401101338.png|600]]

## Classical Solution


Design soln with gates!

Classical soln:
![[Pasted image 20210401101804.png|600]]
[[Probabilistic Computation]]

## Quantum solution
Uses [[Hadamard Gates]]
![[Pasted image 20210413161500.png|250]]
Need to make a reversible quantum f! Need to keep info about what was input.
![[Pasted image 20210401103414.png|250]]

Bigger $U_f$ solves our problem:

![[Pasted image 20210401104840.png|450]]
$\ket{x}, \ket{y}$ must be standard basis states.
$\ket{y\oplus f(x)}$ - add mod 2
Can interpret function as:
![[Pasted image 20210401105103.png|400]]

### Full Diagram

![[Pasted image 20210426204530.png]]

Note that the phase now tells us if $U_f$ is even! [[Phase Kickback]]. The phases in this case are [[Global Phases]]. This happens because of the two [[Hadamard Gates]] on the first qubit. See below for a breakdown of the state at each point of the algorithm

### States
$\LARGE\ket{\psi_0}=(H\otimes X)\ket{00}=(H\ket{0})(X\ket{0})=\ket{+}\ket{1}$

$\LARGE\ket{\psi_1}=(I\otimes H)\ket{+}\ket{1}=\ket{+}\ket{-}$

$\LARGE\ket{\psi_2}=U_f\ket{+}\ket{-}=U_f(\frac{1}{\sqrt{2}}\ket{0}+\frac{1}{\sqrt{2}}\ket{1})\ket{-}$

$\Large=U_f(\frac{1}{\sqrt{2}}\ket{0}\ket{-}+\frac{1}{\sqrt{2}}\ket{1}\ket{-})$

$\Large=\frac{1}{\sqrt{2}}U_f\ket{0}\ket{-}+\frac{1}{\sqrt{2}}U_f\ket{1}\ket{-}$

$\Large=\frac{1}{\sqrt{2}}(-1)^{f(0)}\ket{0}\ket{-}+\frac{1}{\sqrt{2}}(-1)^{f(1)}\ket{1}\ket{-}$

#### **Even f(0)=f(1)=0,1:**
$\Large(-1)^{f(0)}(\frac{1}{\sqrt{2}}\ket{0}+\frac{1}{\sqrt{2}}\ket{1})\ket{-}$

$\Large=(-1)^{f(0)}\ket{+}\ket{-}$

**Apply H gate**

$\LARGE\ket{\psi_3}=(-1)^{f(0)}\ket{0}\ket{-}$

**Measure first qubit and get outcome 0**

#### **Balanced f(0) != f(1)**
$\Large(\frac{1}{\sqrt{2}}\ket{0}-\frac{1}{\sqrt{2}}\ket{1})\ket{-}$ or $\Large-(\frac{1}{\sqrt{2}}\ket{0}-\frac{1}{\sqrt{2}}\ket{1})\ket{-}$

$\Large=(-1)\ket{1}\ket{-}$

**Apply H gate**

$\LARGE\ket{\psi_3}=(-1)\ket{1}\ket{-}$

**Measure first qubit and get outcome 1**

### Path Integral Formulation
Path integrals offer another way of exploring the behaviour of quantum algorithms. Specifically, they trace the likelihood of the machine being in different states at different points in the algorithm.

![[Pasted image 20210426144602.png]]# Dickey Fuller Test
#concepts #concepts/math/stats
[[Statistics TOC]]
A test to see whether [[Time Series Data]] is stationary or not.


The ADF test essentially tests whether the Yt value is directly influenced by the Yt-1 value.

`ΔYt = [P1 - 1]Yt-1 + Et` /(same as 13.5 from textbook)/
`H0: [P1 - 1] = 0`
`H1: [P1 - 1] < 0` -> we use a one tailed since greater than 0 would be BIG SOUP. If the coefficient is positive, need to beef the test up

* H0 -> if accept null, data non-stationary
* H1 -> if reject null/ accept alt, series Stationary

To run this test in eviews: quick -> series -> unit root test

# To improve robustness
Can add an intercept to the equation:
`ΔYt = B1 + [Pt - 1]Yt-1 + Et`

Can also add a time trend! This basically adds variables starting at 1 for each time period to the regression:
`ΔYt = B1 + B2t + [Pt - 1]Yt-1 + Et`

# Augmented Dickey Fuller Test
`ΔYt = B1 + B2t + [P1 - 1]Yt-1 + Sum[j = 1 -> m] (Bj + ΔYt-j) + Et` where m = max lags

To see whether going into first difference space, can redo the ADF test as follows:

`Δ2Yt = B1 + B2t + [P1 - 1]ΔYt-1 + Sum[j = 1 -> m] (Bj + ΔYt-j) + Et`



For Wunnava tval 275, coefficient 1.08

<!-- {BearID:9F56AF0C-3F5E-4AAD-A563-C4FFC803F0D2-2136-00003B0653FCD231} -->
[[052 Physics TOC]] | [[Waves]] | [[Light]] 
# Diffraction
#concepts/physics

Diffraction is the process by which any wave that passes through an opening tends to spread out when it emerges on the other side. The equation that describes by how much is:

$$
\LARGE S=\frac{L}{D}R
$$

where:
* S: The amount of spreading
* L: Wavelength
* D: Diameter of opening
* R: Distance beyond opening

## Resolution
A slightly adapted version of the diffraction equation above can be used to calculate the maximum resolution of a lens, camera or eye.

$$
\large B=\frac{L}{D}R
$$

Where everything is the same except B is the separation of two objects and R is the distance to the objects in question.

### Examples
- Pinhole camera
- Waves in a harbor[[Systems Theory TOC]] | [[Machine Learning TOC|Machine Learning]]
# Dimensionality Reduction
[[Reading List TOC]]
# Don Juan - Lord Byron
#inputs/books

* Interesting comedic aspects - contradicts usual style of poetry
* Juan is a very interesting character - passionate and human. Interesting combination of good and bad
	* Lover = warrior?
		* Don the impulsive/ digressive
	* Saving the orphan girl during the siege of Ismael
* The character of Suvorov
[[Waves]]
# Doppler Shift
#concepts 
When an object is approaching you, you hear it at a higher frequency since the object is closer each time it emits a trough than last time, so you hear them closer together. The reverse is also true.# Drinking Games
* Dudo - dice game 
* Taps
* [[Verbier Curling]]

<!-- {BearID:F02AFE4A-B7DA-4CEF-BD76-8AF22121E0CC-14789-0000082ED9EEF9B6} -->
# Driving Hype

[[Playlists TOC]]
<!-- #_reference/music -->


## Track List
Knocking on heaven’s door
November Rain
Sweet Child o mine
Bad to the bone
Spiritual but not religious
no hands
roses
my generation
my kinda girl
the other side
get over it
goin down (live)
metal and mayhem 
eyes of a panther
750-4
wet dream
Call me the breeze
Faint
Wit yo bae

<!-- {BearID:5D0AE40B-ECB1-4F61-8D07-65F1C773DD29-1476-0000A8D305EA5A59} -->
[[011 Mental Models MOC]] | [[054 Biology MOC]]
# Dunbar's Number
#🌱 
[[Evolution]] 

Dunbar's number theory states that people's social circles are empirically limited by brain physiology, so that people can generally have only about 150 meaningful contacts. This meshes well with my theory of social systems and biological imperative reasons for social behavior. Also very interesting relationship to the [[Tribal Theory of Evolution]] I've been building for some time. We can explain a lot about ourselves today by looking at (and attempting to understand) the way hunter-gatherers lived their lives.

I don't take this as a hard rule, but rather a general theorem if you will. This is an example of biological [[Constraints]] in action. 

[[2021-05-09]]#concepts/math/stats #🌱  #concepts
[[Statistics TOC]][[053 Computer Science TOC]] | [[Algorithms TOC]][[ECON411 Applied Econometrics]]
# EC411 HW3
#school 

*viii)*
Since our F-statistic (17.1869) is larger than our F-critical value of 4.24, we can reject the null hypothesis that our coefficients are equal to 0, concluding that our model predicting total profits as a function of sales is statistically significant overall.

*ix)*
Since our t-statistic for the estimated slope coefficient 4.15 is larger than our t-critical threshold of 2.060, we can reject the null that the coefficient is equal to 0 and therefore conclude that the slope coefficient is individually significant.

*B.*
Given our equation *Y = B0 + B1X*
The coefficient reported for our constant Bhat0 is 83.57528. We can interpret this theoretically as the fixed amount of profit made by a company without any sales, however obviously there is no economic intuition for this value. Furthermore, the reported t-observed value for a t-test testing the individual significance of the coefficient is 0.71, meaning we cannot reject the null that Bhat0 is not statistically significant individually.

Our reported coefficient for Bhat1 is 18.43376. We can interpret this value as how many millions of dollars profit will increase by for each additional billion dollars of sales. The economic theory for this makes sense, as we would expect companies with larger total sales to be able to make more profit overall, however as a result incurring greater costs of business and thus not realizing all revenues as profit. We calculate the t-observed value for the relevant test of individual significance above and demonstrate that we can reject the null that our coefficient is not statistically different from 0, leading us to conclude that this coefficient is statistically significant.

Furthermore, our R-squared value of .4074 is not very high, implying there may be a case of omitted variable bias happening - economically speaking, this makes perfect sense since there is clearly much more that goes into the determination of profits than just sales (costs, taxes etc).

<!-- {BearID:47E03907-9A39-4680-8815-916D4195CF74-1286-000054F423253E15} -->
# EC411 HW7 Notes
#school [[Deprecated]]
 
<!-- {BearID:A59B0ABE-EA26-4F6B-81B0-7C6183A66A1F-37181-000445644B351FB0} -->
# EC411 Paper Skeleton
<!-- #school/Economics/ec411 -->
[[ECON411 Final Paper]]

# Abstract
This paper seeks to investigate the relationship between mortgage default and housing prices on the level of an individual US mortgage insurer. National Mortgage Insurance Holdings, referred to hereafter as NMI, is a small-cap public US company that writes insurance on batches of mortgages issued by the GSEs such as Fannie Mae and Freddie Mac. 

# Introduction and statement of research objective
This paper seeks to investigate the relationship between mortgage default and housing prices on the level of an individual US mortgage insurer. National Mortgage Insurance Holdings, referred to hereafter as NMI, is a small-cap public US company that writes insurance on batches of mortgages issued by the GSEs such as Fannie Mae and Freddie Mac. Entities like NMI were created in the aftermath of the 2008 financial crisis to help distribute risk and manage it in separate, private entities where risk overall becomes easier to compartmentalize and track. Part of the motivation also lies in getting private investors to knowingly take on some of the broader housing market risk, in exchange for a significant reward when things go well. Their success and ongoing stability are both directly tied to the performance of the underlying mortgages on their books, therefore it is important to consider methods of forecasting default before defaults actually happen.

Although this specific application of our understanding of mortgage default has not been researched, there already exists a considerable amount of literature devoted to studying mortgage defaults more broadly. This paper will focus on whether it is possible to predict defaults within a smaller entity that may not completely reflect the overall US housing market, and have varying exposures in different geographies and risk segments. 

# Existing literature review
Much study has already been conducted on the nature of mortgage default in the US. The existing literature identifies several clear factors that have a statistically significant effect on future defaults. The core of this article builds on the seminal paper by Case & Schiller (1996) that develops a model for hedging mortgage default risk based on regional housing price moves. Their paper proves highly useful to the specific application of NMI since NMI closely mimics the studied format of regional concentrations of risk, to which we can as a result apply housing price changes to help understand the incidence of residential mortgage default.

Case & Schiller’s work

# Econometric model
Given the above review of the literature, this paper will use a slightly modified model to investigate mortgage default at NMI. The final model used for the paper is a time-series Linear Probability Model (LPM):
`lossratio_t = B0 + B1*new_rif_t + B2*unemployment_t + B3*total_t + E_t`
Where we define:
	* lossratio: this value is obtained by dividing insurance claims and claim expenses by total premiums earned, and can be used as a proxy for the profitability ratio of the company’s mortgages. This is the key statistic that determines the level of losses from defaults. This variable is added as a result of Case & Schiller’s study on mortgage performance. 
	* new_rif: The number of new mortgages added to NMI’s total portfolio in the current quarter. This variable is added as a proxy for overall mortgage age. This variable is added as a result of a much older study by (von Fürstenberg & Green 1974) that finds an age pattern to delinquency. We would expect the coefficient on this variable to be negative, since von Furstenberg and Green find that generally defaults increase as mortgages age until about 5 years into their existence, at which point they plateau and gradually come down. We choose not to model the second part of this effect, since NMI’s growth in quarterly issuance has thus far been large enough to mitigate the impacts of older loans having differing characteristics from medium-aged loans.
	* unemployment: the L3 unemployment rate in each quarter in the US. This is added as a proxy for overall economic health, and helps control for some of the exogenous impacts of the coronavirus pandemic. This variable is added as a result of another seminal study, this time by (Campbell & Dietrich 1983) that links mortgage performance with borrowers’ ability to pay, as captured in the rate of overall employment. We expect this variable to have a positive coefficient, since higher unemployment rates translate to a lower ability to pay, and thus higher losses.
	* total: this is our created variable that we use to capture the effects of regional housing price changes on NMI’s loan losses. This data point is created by multiplying through the percentage of risk NMI has in each state by the quarterly change in the housing price of that state, and summing the resulting values into a single aggregated total index. The appendix includes several alternative models that disaggregate this value and use a panel method to estimate the outcomes, however with most states contributing relatively small amounts to the total (<4%) individual significances were very low, so the aggregated model is preferred. A formal definition for /total/ is given below in the description of the data.

This model might be a surprise to some readers, given the previously referenced literature’s emphasis on panel estimation and disaggregated effects as well as the potential to use a Logit model. Since we only care about the overall national loss in loans for each quarter, we have reasoned that individual effects do not bear much relevance to our final outcome, as long as we can create a model that fits our theoretical expectations, the literature and is statistically significant. Thus we use an aggregated approach for considering states. Furthermore, there is no theoretical reason for the loss ratio in this case to be a bounded probability, since the loss ratio can be below or above one. This leads us to disregard the option of running a LOGIT regression.

# Description of the data
This paper will use several key data sources for the bulk of the data. Unemployment rate data and the federal funds target rate come from FRED. Quarterly housing data is extracted from FHFA’s monthly HPI (Housing Price Index) survey, which is the largest publicly available database on change in housing price based on home sales in the US. 

All of the company-specific data on NMI used in this paper comes from NMI’s investor resources. The loss ratio is made public each quarter in NMI’s quarterly reports, where it is generally reported in a standalone table reflecting credit performance for the quarter. Details on the regional concentration of NMI’s mortgages are similarly reported in a single separate table in the quarterly results, where the ten states with the highest concentrations of the company’s total risk are listed, as well as the percentage of total risk in each of the ten states. This presents an inherent statistical difficulty for this project, whereby we are working with an incomplete panel, since the top ten US states often jockey for position and move on and off the league tables. There is also a definite, gradual shift over time in where the company’s mortgages are located. Since this method of reporting is only available from the second quarter of 2015 onwards (end of June 2015), we must limit our study to the period between 2Q15 and 3Q20. This leaves us with a total of 21 time observations for a total of 14 states that make the top ten ranking in that period. 

This lack of consistency in the dataset paired with a relatively low number of observations certainly complicates statistical analysis, in particular making panel estimation difficult and somewhat unreliable, since we have a very incomplete panel. Since we are not particularly concerned with the differences between the contributions of individual states, we therefore opt to create a new aggregated variable we term /total/ that we shall use to capture the overall combined effect of changes in NMI’s regional risk distribution as well as the relevant changes in housing price in each respective state. We formally define /total/ to be:
`total_t = SUM(i = 0 -> 14)[NMI_concentration_t * quarterly_pct_HPI_change]`
Thus for each time slice of our sample, we calculate a single value that captures the combination of changes in NMI’s main distribution of risk as well as the change in each respective state’s housing price. 

## Summary table
```
. estat sum
  Estimation sample regress                Number of obs =         21
  -------------------------------------------------------------------
      Variable |         Mean      Std. Dev.         Min          Max
  -------------+-----------------------------------------------------
     lossratio |     .0414762      .0713321            0         .347
       new_rif |      1201.81      474.5838           46         2288
  unemployment |     4.968572      2.178446        3.644       13.069
         total |     .0070797      .0019502     .0026384     .0093258
  -------------------------------------------------------------------
```

## Correlogram
```
             | lossra~o  new_rif unempl~t    total
-------------+------------------------------------
   lossratio |   1.0000
     new_rif |  -0.5357   1.0000
unemployment |   0.4833  -0.4340   1.0000
       total |  -0.5103  -0.1038   0.1117   1.0000
```

The move correlogram demonstrates relatively low degrees of correlations between our variables. None of the values are especially concerning, and all correlations are supported by our theoretical explanations: the largest correlation between new_rif and loss ratio can be explained via our reasoning in the section presenting our econometric model above.

# Presentation of the econometric results + econometric challenges
Given our above discussions surrounding some of the statistical difficulties involved in estimating the data (with alternatives presented and rejected in Appendix A) as well as the fact that we do not care about the individual differences between states (emphasized by lack of focus on this in the literature), we choose to run a simple time-series Linear Probability Model (LPM) regression estimating total loss ratio in a given quarter using the following stata command:
`reg lossratio new_rif unemployment total, robust`

Notice we use robust standard errors to be doubly sure our results are valid. The reported results for this regression are:
```
Linear regression                               Number of obs     =         21
                                                F(3, 17)          =       3.74
                                                Prob > F          =     0.0314
                                                R-squared         =     0.7141
                                                Root MSE          =     .04137
------------------------------------------------------------------------------
             |               Robust
   lossratio |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     new_rif |  -.0000665   .0000264    -2.51   0.022    -.0001223   -.0000107
unemployment |   .0117245   .0054804     2.14   0.047     .0001619    .0232871
       total |  -21.80602   7.683918    -2.84   0.011    -38.01767   -5.594372
       _cons |   .2174742   .0826563     2.63   0.018     .0430846    .3918638
------------------------------------------------------------------------------
```
These results tell an interesting and revealing story about the dependence of the performance of NMI’s loan portfolio on several easily quantifiable metrics. Let us now interpret and discuss these results.

## Interpretation
	* *new_rif:* Our results report a negative coefficient for new_rif of -.0000665, which can be interpreted to mean that for every additional million dollars of risk added to NMIH’s balance sheet in a given quarter, we would expect the loss ratio to decrease by .0065% in the subsequent quarter. It is perhaps easier to think of this in larger terms, given new issuance is typically in the billions: for every additional billion dollars of risk, the company’s loss ratio falls by 6.5% the subsequent quarter. Although we do not capture the full time-effect of mortgages as described in Von Furstenberg and Green’s 1974 paper, we still find that this result aligns with our theoretical expectations of temporarily decreasing the loss ratio with new issuance. Given the wide variability in new issuance quarter to quarter ()
	* *unemployment:* Our results for unemployment show a positive coefficient of 0.117245, which we can interpret to mean that for every additional unit increase in the unemployment rate, we would expect a corresponding increase in the loss ratio of 1.2%. The reported coefficient matches our theoretical expectations of a positive value, as confirmed in the literature outlined by Campbell & Dietrich.
	* *total:* This is the real variable of interest, with the other two acting as controls to improve model specification. Our reported coefficient of -21.8 is a little difficult to interpret in a very quantifiable way, but we can start by confirming our theoretical expectations that this coefficient will be negative; which makes sense since positive values for total mean that the weighted average housing price in the largest concentration geographies has increased. Specifically, we can say that for each 1% increase in the weighted housing price in a specific geography in a given quarter, we would expect a 21.8% decrease in the loss ratio. In practice, the actual variation in total is all between .0026384 and .0093258, or .26% and .93%. One concern for us in this model is the lack of historical data covering periods where the overall housing price in the US declined, since NMI only started operations in 2014, with the last major national decline in the housing price happening in 2004-2009.

## Discussion of Significance


# Hypothesis testing + other econometric analysis

# Interpretation of the results and relevance to policy making

# Conclusion

# Bibliography

# Appendix

<!-- {BearID:F9D57733-5645-4CF9-890F-6FD35A4125AD-16983-000172AEEE988246} -->
[[ECON411 Applied Econometrics]]
# EC411 Presentation Review
#school 
* What I learned
* Think of a question
* Any suggestions
* 0/100?

LASTNAME_SUMMARY

# Sean - Homelessness
	* I learned a lot about the determinants of homelessness in the US. I had never considered that there might be regional differences based on levels of urbanization, and it was particularly interesting to see the impacts of cannabis legalization on homelessness in the US. It was interesting to see that the variable was not significant. I think homelessness is a huge issue, and it is great to see a recent study on this that takes into account modern considerations. I was quite surprised to see some of the results Sean reported, showing that it is crucial for us to study and understand the causes of homelessness. I think it would be interesting to expand the paper to thinking about ways to solve the problem without housing people, especially at the costs estimated in the paper. I feel like it would be possible to solve these issues more cheaply, so it would be interesting to see if that is true. I would also be curious to see whether ideas such as reskilling programs could have a positive impact by retraining discouraged workers and encouraging productive people to reenter the workforce. I think the role of benefits is also super important, and another avenue for further questioning.
	* I would be curious to learn about the ties between rural-urban migration and urban homelessness. 
	* I would be interested to see as above how domestic and international migration would affect the outcomes.
	* 98

# Matt - Foreign aid and economic growth
	* Pretty cool to learn about the determinants of economic growth regarding foreign aid. Interesting to see the controls used as well. 
	* I am curious about how effectiveness of government and openness to investment factor into it.
	* I would add some more qualitative data on governance to see the effects of that on growth. I’d bet they could be very significant
	* 65/100

# Eliza - The impact of internet connectivity on unemployment
	* This is a really cool study, I’ve actually wondered quite a lot about whether the internet helps or hinders job finding. While it may be easier to find a job, it is also easier to get replaced, so I could definitely see the outcome going either way. I liked the thoroughness of the model considered, although it may have been a little dense at times. 
	* I would be curious to see the additional effects of internet connectivity on hiring and firing, rather than maybe unemployment itself
	* I would simplify and clarify the model, as well as making the presentation a little more clarified. Perhaps my model was just too simple, but I think making things too complicated confuses the results a little.
	* 75

# Emily - Social determinants of health on access to health insurance
	* Very pertinent study given all of the thinking recently about single payer health systems. I think this study was really thorough and included lots of great control variables. I think the relationship between poverty, income and health insurance is interesting. It would be curious to extend the study outside of the US, obviously there would then be issues of it not being a ceteris paribus comparison, but I think it is relevant to compare. Maybe an aggregate-level comparison between countries, taking into account different political ideology?
	* I don’t really understand how it is possible to separate out different types of insurance when many people, as noted in the presentation, have more than one type. What is the effect of people having some private, some employer insurance for example? Also WOW that is a crazy Chi2 - is there a risk that there is some heteroskedasticity involved?
	* I would be curious to see an extension of this paper that considers different political ideologies, for example comparing the American system to the Russian system and British system, and how health outcomes differ as a result.
	* 94

# Max - How can we predict seemingly random mortality differences between countries as a result of COVID?
	* Very timely study that addresses an important ongoing issue. I think part of the difficulty with studying this topic is that data is really difficult if not almost impossible to compare between countries. 
	* I am curious if death classification/ general case classification differs between countries? I think this could have a significant impact overall on the outcome.  I would also like to learn more about people’s willingness to follow rules as a society. For example Americans traditionally do not trust the government or listen to authority. I don’t know how you could quantify this, but it would be interesting to try.
	* I would be curious to extend the model of effectiveness of healthcare a little to see how this actually stacks up. For example, I know a doctor down in Texas working with a new machine called ECKMO that significantly improves patient outcomes, much more so than just number of hospital beds.
	* 93

# Thomas - Major League baseball
	* I don’t really know anything about baseball, so it was quite interesting to watch this presentation. I didn’t realize there had been this much thought or study put into sport, to be honest, so a little door has been opened for me I guess. It was interesting to see how the game can be reduced to a very statistical situation.
	* I don’t really know enough about baseball or honestly any sports to have a very interesting question for this one. Ironically this is more complex to me than a complicated economics paper, at least I know what the terms mean in economics. 
	* I’d be curious to see how well you can predict actual game performance based on how different players’ characteristics are matched in each game.
	* 90. Great analysis but completely over my head, so tough for me to judge

# Liam - Measuring economic costs of obesity
	* This is a very relevant issue, and something I have been worried is getting lost in the narrative around obesity in the US. It’s interesting to see the different lines of thought on the economic costs over time, as well as the different things that go into the cost. It was quite surprising to see how mixed the results were, especially given how objective in my view many of the costs are. I suppose it is really hard to actually disaggregate the effects and understand how to quantify costs.
	* How does the research change over time? Are we seeing constant upward revisions in the costs of obesity, indicating an increasing strain on the economy?
	* I’d be interested to see how social factors play into the question of obesity. I feel like a lot of these things are ingrained in people socially over the course of thousands of years, with minute social cues being conveyed by things such as weight and skin tone. It would also be interesting to see how these things change over time.
	* 92

# Anthony - Family income and athletic participation
	* This is an interesting study, and certainly makes me think more deeply about alternate ways wealth inequality manifests itself in our society. I liked the focus in this study on family income rather than just school - I think it is quite obvious that private schools will have a larger budget for and thus focus on athletics.
	* I wonder if there are alternative ways to make up for these differences in income without increasing everyones income? I mean like ways to equalize this outcome.
	* It would be interesting to learn more about the cultural effects of this as opposed to the actual financial effects. Could you maybe argue that say wealthy families expose their kids to sports more than poor families, not because of wealth but just historical precedent? 
	* 94

# Jack F - Does payroll in MLB have an effect on win %
	* It’s quite interesting to consider the impacts of salary on sports performance. I think athletes are terribly overpaid and the whole thing is essentially a scam, so it’s interesting to bring that idea to bear with the question of financial incentives on sports performance. It also clearly “drives revenue through ticket sales” as Jack would say. SOGOOD variable naming was quite funny. I again didn’t really understand what this was all about though, I don’t really get the fuss about sports to be honest, especially baseball. I also don’t really like the team at Midd, but that’s besides the point. This presentation was pretty interesting. It totally confirmed my assumptions to see that higher pay results in worse performance, although this does contradict the economic theory we would assume. 
	* How would the results change if you controlled for age or number of years playing? I feel like this is a very critical factor. Jack mentions “rookies,” I don’t know exactly what these are, but I feel like he should account for this. 
	* Same as above
	* 60


# Michelle - Random walk in the US stock market
	* Finally a more interesting, relevant topic. I personally vehemently deny that the EMH holds, although it is still very interesting to analyze it. It’s cool to see such a rigorous statistical approach as well,  loved seeing the unit root tests. I also liked how multiple tests are used, Zivot & Lee. This makes the tests more relevant. I absolutely and categorically disagree with Michelle’s /a priori/ expectations. I think there are enough examples of people making consistently better-than-market returns to show that the market is not efficient. I also know enough people who make worse than market returns, meaning there are people on other sides of those trades winning. For example, Cathy Wood runs an actively traded ETF that consistently outperforms. Warren Buffett is another example, although you could argue he buys private assets (although he didn’t start this way, rather starting the same way as everyone else). Really interesting analysis nonetheless and a very very interesting approach to examining markets. I love seeing different opinions and seeing proofs for why I may be wrong in my assumptions. Either way, what a phenomenally interesting and comprehensive analysis. Although I quite strongly disagree with the motivations and some of the conclusions, it is so interesting to see this.
	* I would be curious to see if these results would hold when examining the performance of individual fund managers, and what the relevant conclusions would show. I (perhaps naively) absolutely believe it is possible to beat the market, since really you need to anticipate human psychology, which is inherently predictable. I think for example the medallion fund by Jim Simons & Renaissance Technologies proves this unequivocally. 
		* I would also be curious to see how ETF weighting affects the results of this test. Indices tend to be market cap weighted, meaning the biggest best players have an outsize affect on the outcomes.
	* I think it’s interesting to think more about the implications of EMH not holding. If the EMH did hold, even in weak form, a lot LOT of things in the financial world would be pretty screwed, and I would argue that this is not a form of “suffering” as suggested by the paper. Part of capitalism is that people put their money behind their beliefs, and overall after a long time the money speaks for itself. On an individual basis efficiency is not a good thing and shouldn’t be hoped for.
	* Note: I am quite upset we spent so much time discussing the sports results, and did not have the chance to ask a single question about this paper, which was more impressive, more comprehensive and more interesting than probably any sports-related question you could ask. 
	* 100/100

# Abdoul - Effects of ECOWAS trade area on regional growth
	* This was an interesting paper, and something I hadn’t learned about or known about until Abdoul presented on it. I am still personally grappling with the right way to implement regional economic alignment, so it’s really interesting for me to consider more than just the example of the EU. I liked the statistical rigor of this paper. Cool to see that the results were strongly significant. I was very very surprised with the variable of economic freedom. I wonder if perhaps controlling more closely for the strength of domestic institutions in addition to economic freedom could help fix this, since I am quite convinced more economic freedom creates more growth, as long as there are good institutions that support and enable fair growth to happen.
	* Is there a way to expand this paper into the idea of governance and institutions in a quantitative way? These variables are obviously difficult to quantify, but it would be interesting to try. I’d bet corruption and similar issues plague countries that may still be more open economically or financially. This would especially help by allowing international capital to enter the country more effectively.
	* It would be interesting to try to understand why variables were not individually significant. 
	* 92%

<!-- {BearID:3931CDFE-6BA4-4BC8-B368-D78DA7E8F389-5857-00019A5F80BE0429} -->
[[063 Economics TOC]]
# ECON211 Regression
#school/class 
For the concept, see: [[Linear Regression]]

[[Linear Regression Lab 2]]
[[Regression HW5]]
[[Linear Regression HW6]]
[[Linear Regression HW7]]
[[Regression HW8]]
[[Regression HW9]]
[[Regression HW10]][[006 School MOC]] [[063 Economics TOC]]
# ECON411 - Applied Econometrics
#school/class #concepts/econ #school #school/Economics 
Taken with Wunnava

[[ECON411 HW4]] [[ECON411 HW4-5 Notes]]
[[ECON411 HW5]] 
[[ECON411 HW6]] 
[[ECON411 HW7]] [[ECON411 HW7 Notes]]
[[ECON411 HW8]] [[ECON411 HW8 Notes]]
[[ECON411 HW9]] 
[[ECON411 HW10]]

[[ECON411 MIDTERM]]
[[ECON411 Final Exam]]
[[ECON411 Final Paper]]# ECON411 Final Exam
#school
[[005 Active MOC]]

# Todo
* Make a list of all the topics covered in the course
* Create a reference page for each concept! Try to link to homework

<!-- {BearID:4A973BD9-76EE-4141-80F6-EB289D87D5F8-2897-000010846D0C181A} -->
# ECON411 Final Paper
#school
[[Quantitative Finance]] [[Residential Mortgages]]


[[EC411 Paper Skeleton]]
Summary due noon tuesday!
Dont forget *estat sum* after running regression! Summary stats of effective sample
	*  Want skeleton!


# Remaining to complete WELL estimates:
- Abstract ~15 mins (only results)
- Intro ~ 0.5 hours (want maybe another page or so)
- Existing literature review ~1 hours
- Econometric model ~30 mins
- Econometric challenges ~2.5 hours
- Discussion of significance ~45 mins
- Hypothesis testing + other metrics ~2 hours
- Interpretation of the results and relevance to policy ~1 hour
- Conclusion ~1 hour
- Bibliography ~30 mins
- Appendix ~30 mins
- Cleaning up ~1 hour

TOTAL: 14 hours


## Potential state groupings
* *West:* CA, AZ, CO
* *Central:* TX, IL, MI
* *East:* FL, PA, VA, MD


# Panel test
state_code:
	1. CA
	2. TX
	3. FL
	4. VA
	5. AZ
	6. PA
	7. IL
	8. CO
	9. MI
	10. MD
	11. WA
	12. MA
	13. NC
	14. NJ

^ Gonna use these!!!

Testing RE vs FE with special F-test, restricting fixed effects to 0:
((.724093-.7513261)/(18-4))/((.7513261)/(210-18)) = -0.4970977506

F(4, 192)
H0: FE coefficients = 0
H1: H0 false

F critical value: 1.94486

Cannot reject null that FE coefficients are 0, so we choose RE model!!

Is this a Hausman test?




Remaining:
1, 2, 3, 4, 5, 6, 8, 9, 10
	1. CA
	2. TX
	3. FL
	4. VA
	5. AZ
	6. PA
	7. CO
	8. MI
	9. MD


# Topic
“So I would say that historically speaking, house price paths — house prices have been the single most important driver of residential mortgage credit performance.” - Adam Pollitzer, CFO at NMI Holdings

Using housing price data to predict the level of delinquencies in the portfolio of mortgages of a Private Mortgage Insurer (PMI) known as National Mortgage Insurance Holdings (NMIH or NMI), and using this information to create signals for a stock trading system based off of a flow of economic house-price data. The scope of this idea is quite broad but this is a topic I find wildly fascinating, and I have already spent quite some time putting thought into much of the technical infrastructure for the project. If this seems like too detailed of a study, I could perhaps try to pull a level back from creating a profitable trading system, and just see if I can accurately predict the monthly level of defaults based on their exposures in different states and changing macro data. 

I have been studying a number of models created after 2008 to simulate the effects of housing prices on mortgages, and rapid housing price moves appear to be the most common contributor to default rates rising (Case & Schiller). Interestingly, their paper was one of the key pieces of research used to establish the PMIs after the recession - they are mentioned briefly in the methodology as a potential solution to taxpayers having to hold the risks of these mortgages.

# Details of the Data
It appears, based on a lot of the literature below, that each individual component of this project should be quite achievable, if somewhat difficult. I’m currently in the latter stages of scraping and cleaning data on NMI’s risk concentrations over the past decade, which will be one of the independent variables in my eventual model. I already found a good source of monthly housing price data, and also have a source for the monthly level of defaults at NMI. 

# Bibliography
von Furstenberg, and R. J. Green (1974). “Home Mortgage Delinquencies: A Cohort Analysis,” /Journal of Finance/, 29(5), 1545-1548.

* /Mortgage insurance: Comparing private sector and government-subsidized approaches: Hearing before the Subcommittee on Housing and Insurance of the Committee on Financial Services, U.S. House of Representatives, One Hundred Thirteenth Congress, first session, March 13, 2013/. (2013). Washington: U.S. Government Printing Office.
	* This is a really interesting source that discusses in detail the formation of the PMI industry and how it was created to help manage the taxpayer risk of default - in effect, PMIs act to take some of this risk from taxpayers in exchange for an additional insurance premium. 
* Mayer, C., Pence, K., & Sherlund, S. M. (2009). The Rise in Mortgage Defaults. /Journal of Economic Perspectives/, /23/(1), 27.
	* Ties rising house prices to lower default levels as buyers can refinance. This is part of my core economic theory I will use to describe a model for NMI.
	* Says complex products not inherently at fault, however once underwriting standards improve, inability to refinance means borrowers who have issues with these are unable to refi.
* Daglish, T. (2009). What motivates a subprime borrower to default? /Journal of Banking and Finance/, /33/(4), 681–693. https://doi.org/10.1016/j.jbankfin.2008.11.012
	* Builds a model for predicting default decisions, especially among subprime borrowers, as a factor of interest rates and housing prices. This will be the core of my argument/ model.
	* Extends model to variable rate mortgages, and how these were so dangerous for subprime borrowers, since rates went up and suddenly everyone had to pay more.
* Kau, J. B., Keenan, D. C., Lyubimov, C., & Carlos Slawson, V. (2011). Subprime mortgage default. /Journal of Urban Economics/, /70/(2), 75–87. https://doi.org/10.1016/j.jue.2011.05.001
	* Associating 2009 mortgage crash with mostly a fall in housing prices, since borrower quality degraded, but it was not a surprise and happened in 2004/5
	* Creates a model for mortgage default
* Zhu, J. ( 1 ), Janowiak, J. ( 2 ), Ji, L. ( 3 ), Karamon, K. ( 4 ), & McManus, D. ( 4 ). (n.d.). The Effect of Mortgage Payment Reduction on Default: Evidence from the Home Affordable Refinance Program. /Real Estate Economics/, /43/(4), 1035–1054. https://doi.org/10.1111/1540-6229.12104
	* Details again the effect of payment reduction on lowering default risk, 10% reduction in mortgage payment results in a 10-11% monthly default hazard.
* Wang, H., Lu, S., & Zhao, J. (2018). /Aggregating multiple types of complex data in stock market prediction: A model-independent framework/
	* Model-agnostic frameworks for quantitative analysis.
* Hautsch, N., Hess, D., & Müller, C. (2012). Price adjustment to news with uncertain precision. /Journal of International Money & Finance/, /31/(2), 337–355. https://doi.org/10.1016/j.jimonfin.2011.11.013
	* Uncertainty in the reaction of stock prices to news releases, mostly focused on macro news. This is useful for me since I am also looking at news releases for macro data (housing price info).
* Brenner, M., Pasquariello, P., & Subrahmanyam, M. (2009). On the Volatility and Comovement of U.S. Financial Markets around Macroeconomic News Announcements. /Journal of Financial & Quantitative Analysis/, /44/(6), 1265–1289. https://doi.org/10.1017/S002210900999038X
	* Volatility and comovement in financial markets after surprise macro data announcements.
* Kurov, A., Sancetta, A., Strasser, G., & Wolfe, M. H. (2019). Price Drift Before U.S. Macroeconomic News: Private Information about Public Announcements? /Journal of Financial & Quantitative Analysis/, /54/(1), 449–479. https://doi.org/10.1017/S0022109018000625
	* Price drift before economic news, data shows that stocks start moving in the correct direction 30 minutes before an economic announcement, and generally make 40% of their gains before data are announced.
* Cagliero, L., Garza, P., Attanasio, G., & Baralis, E. (2020). Training ensembles of faceted classification models for quantitative stock trading. /Computing/, /102/(5), 1213.
	* Training ensemble ML models to predict stock price movements - combining and filtering separate signals to create unified recommendation.
* Miller, Norm & Sklarz, Michael. (1986). A Note on Leading Indicators of Housing Market Price Trends. Journal of Real Estate Research. 1. 99-109. 
	* Paper on shorter-term housing price predictions based off of other Instrumental Data
* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.7536&rep=rep1&type=pdf
	* AWESOME article laying out a model for predicting mortgage defaults based on changes in the housing price. Presents a great starting point for me with my model, and even mentions PMIs!

# Core books
These are the books I am referencing for my trading systems. This is obviously an ancillary part of the project since it isn’t strictly an application of statistics, however I wouldn’t be as interested without this component, so I can include this portion in the appendix or something like that, and focus more on the economic theory tying housing prices to mortgage defaults.
* Clenow, A. F. (2019). /Trading evolved: Anyone can build killer trading strategies in Python/. Zurich, Switzerland: Andrew F. Clenow.
	* Used to build code architecture used to test models and data.
* Carver, R. (2015). /Systematic trading: A unique new method for designing trading and investing systems/. Petersfield: Harriman House.

<!-- {BearID:C683F631-E774-47CE-953D-F0D7B48815AE-37181-00039117F75FAC96} -->
# ECON411 HW10
#school 

# I)
Given the LOGIT model: `ln[P_i/(1-P_i)] = B_0 + B1*X_i + E_i` we can differentiate it with respect to X_i as follows:
First, raise both sides to the power of e:
`[P_i/(1-P_i)] = e^B_0 + e^B_1*X_i + e^E_i`
`P_i = (e^B_0 + e^B_1*X_i + e^E_i)(1-P_i)`
`ln(P_i) = (B_0 + B_1*X_i + E_i)(1-P_i)`
`[dP/dX_i] = 1/P_i = 0 + B_1 - B_1*P_i = B_1[P_i(1 - P_i)]`

# II)
a)
Model:
```
. logit four_yr_college grades faminc female black

Iteration 0:   log likelihood = -691.68847  
Iteration 1:   log likelihood = -545.88582  
Iteration 2:   log likelihood = -544.62221  
Iteration 3:   log likelihood = -544.62008  
Iteration 4:   log likelihood = -544.62008  

Logistic regression                             Number of obs     =      1,000
                                                LR chi2(4)        =     294.14
                                                Prob > chi2       =     0.0000
Log likelihood = -544.62008                     Pseudo R2         =     0.2126

---------------------------------------------------------------------------------
four_yr_college |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
----------------+----------------------------------------------------------------
         grades |  -.5326649   .0406259   -13.11   0.000    -.6122902   -.4530395
         faminc |   .0121119   .0023642     5.12   0.000     .0074782    .0167457
         female |  -.2027396   .1507377    -1.34   0.179      -.49818    .0927009
          black |    1.24378   .3208465     3.88   0.000     .6149329    1.872628
          _cons |   3.068816   .3225864     9.51   0.000     2.436558    3.701074
---------------------------------------------------------------------------------
```

## Overall significance:
The results from this regression are somewhat of a mixed bag. Our Pseudo-R2 is quite low at .2126, suggesting a low degree of explanation of our dependent variables by our independent variables. On the other hand, our LR Chi2(4) is very high at almost 300, corresponding to a P-value of 0.0000. These results suggest that while we go some way to explaining the variation in our dependent variables, we are either suffering from an incorrect model to represent the data, or are otherwise somehow misspecifying the results, perhaps through OVB.
 
## Interpretation:
	* *Grades:* we would expect students with better grades (a lower number using our data) to have a higher chance of going to 4 year college, so we would expect the coefficient to be negative. The reported coefficient matches our expectations.
	
	Furthermore, we know that the coefficient is highly significant given the reported P-value of 0.000, which indicates a very low chance of committing type 1 error if we assume the sign is correct. 
	(H0: coefficient = 0, H1: H0 false)

	* *Faminc:* We would expect students with a higher family income to haver a higher chance of going to 4 year college, and the positive reported coefficient supports this theoretical view. 

	We again determine that the coefficient is highly individually significant using the same null and alternative hypotheses as above, with a reported P-value of 0.000 again, leading to the same conclusion of low chance of committing type 1 error.

	* *Female:* We would not expect women to be less likely than men to get a 4 year college degree, so we would expect results not statistically different from 0. We have a negative coefficient that is not statistically significant. The reported P-value is 0.179, meaning we are forced to accept our null that this result is not statistically significant (H0: coefficient = 0, H1: H0 false)

	* *Black:* In my opinion, we wouldn’t expect any statistical difference between black people and white people in getting a 4 year degree, once we’ve controlled for family income and prior grades, which I would argue already capture much of the bias making it more difficult for black people to get a degree. The results do not align with this however, in fact showing that black people are more likely than white people to get a 4 year college degree.

	Statistically, this result is quite significant, with a P-value of 0.000, suggesting a low chance of committing type-1 error if we reject the null that the coefficient is statistically different from 0 (H0: coefficient = 0, H1: H0 false)


b)
```
. mfx, at (female=0, black=0)

warning: no value assigned in at() for variables grades faminc;
   means used for grades faminc

Marginal effects after logit
      y  = Pr(four_yr_college) (predict)
         =  .55299559
------------------------------------------------------------------------------
variable |      dy/dx    Std. Err.     z    P>|z|  [    95% C.I.   ]      X
---------+--------------------------------------------------------------------
  grades |  -.1316702      .00992  -13.28   0.000  -.151105 -.112235   6.53039
  faminc |    .002994      .00058    5.14   0.000   .001852  .004136   51.3935
  female*|  -.0504851      .03747   -1.35   0.178  -.123927  .022956         0
   black*|   .2580107      .05162    5.00   0.000    .15684  .359182         0
------------------------------------------------------------------------------
(*) dy/dx is for discrete change of dummy variable from 0 to 1
```
We can calculate the probability ratio, also known as the odds ratio, using the formula `P_i/(1-P_i)`
This gives us a resulting value of 1.2371143945, meaning a white male student is 1.24 times more likely to attend a four-year college than not attend a college.

# III)
a)
```
. mlogit psechoice grades faminc female black, base(1)

Iteration 0:   log likelihood = -1018.6575  
Iteration 1:   log likelihood = -858.70649  
Iteration 2:   log likelihood = -846.95257  
Iteration 3:   log likelihood = -846.75617  
Iteration 4:   log likelihood = -846.75602  
Iteration 5:   log likelihood = -846.75602  

Multinomial logistic regression                 Number of obs     =      1,000
                                                LR chi2(8)        =     343.80
                                                Prob > chi2       =     0.0000
Log likelihood = -846.75602                     Pseudo R2         =     0.1688

------------------------------------------------------------------------------
   psechoice |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
1            |  (base outcome)
-------------+----------------------------------------------------------------
2            |
      grades |  -.3089701   .0552152    -5.60   0.000    -.4171899   -.2007503
      faminc |   .0118943    .003928     3.03   0.002     .0041956    .0195931
      female |   .1169483   .1949887     0.60   0.549    -.2652224    .4991191
       black |   .5679813   .4295461     1.32   0.186    -.2739136    1.409876
       _cons |   1.937035    .491135     3.94   0.000     .9744285    2.899642
-------------+----------------------------------------------------------------
3            |
      grades |  -.7272638   .0566698   -12.83   0.000    -.8383346   -.6161929
      faminc |   .0204678   .0038319     5.34   0.000     .0129574    .0279781
      female |  -.1337162   .1932327    -0.69   0.489    -.5124453    .2450128
       black |   1.607127   .4079379     3.94   0.000      .807583     2.40667
       _cons |   4.962637   .4744651    10.46   0.000     4.032702    5.892571
------------------------------------------------------------------------------
```
Overall significance for this regression is also somewhat of a mixed bag. First off, we see that our Pseudo R2 is just 0.1688, which is quite low. This suggests the model does a poor job of explaining the variation in our dependent variable. Our LR Chi2(8) value is again very high at 343.8, giving us a very high resulting P-value of 0.0000. 

## Individual significance
To test individual significances, to avoid repeating the same null and alternative 8 times, we will use a simple t-test with the hypotheses (H0: coefficient = 0; H1: H0 false).

*PSECHOICE = 2*
We can soundly reject our null hypothesis for grades and faminc, with corresponding P-values of 0.000 and 0.002, suggesting very low chances of committing type 1 error if we reject the null. Female and black on the other hand, are not statistically significance, with P-values of 0.549 and 0.186, meaning we can not reject those respective nulls. 


*PSECHOICE = 3*
Using the same hypotheses and general framework as above, we can easily reject the null hypotheses for grades, faminc and black, with P-values of 0.000 for each. This means the chance of committing type 1 error is very low, meaning the coefficients are statistically different from 0, or in other words individually significant. Female on the other hand, has a very large P-value of 0.489, meaning we cannot reject the null that the coefficient for female is statistically different from 0.

b)
```
. mfx, predict(outcome(3)) at (grades = 6.64, faminc = 42.5, black = 0, female = 0)

Marginal effects after mlogit
      y  = Pr(psechoice==3) (predict, outcome(3))
         =  .52394656
------------------------------------------------------------------------------
variable |      dy/dx    Std. Err.     z    P>|z|  [    95% C.I.   ]      X
---------+--------------------------------------------------------------------
  grades |  -.1354279      .01023  -13.24   0.000  -.155482 -.115374      6.64
  faminc |   .0033355      .00061    5.51   0.000    .00215  .004521      42.5
  female*|  -.0512323      .03761   -1.36   0.173  -.124952  .022488         0
   black*|   .2664279      .05661    4.71   0.000   .155468  .377388         0
------------------------------------------------------------------------------
(*) dy/dx is for discrete change of dummy variable from 0 to 1
```
From the above marginal effects prediction, we see that the chance of a white male student with median grades and family income attending 4 year college is 52.4%.


c)
Values for attending no college:
```
. mfx, predict(outcome(1)) at (grades = 6.64, faminc = 42.5, black = 0, female = 0)

Marginal effects after mlogit
      y  = Pr(psechoice==1) (predict, outcome(1))
         =  .19207835
------------------------------------------------------------------------------
variable |      dy/dx    Std. Err.     z    P>|z|  [    95% C.I.   ]      X
---------+--------------------------------------------------------------------
  grades |   .0900439      .00798   11.29   0.000   .074413  .105675      6.64
  faminc |  -.0027086       .0006   -4.49   0.000   -.00389 -.001527      42.5
  female*|   .0060116       .0276    0.22   0.828  -.048083  .060106         0
   black*|  -.1339941      .02618   -5.12   0.000  -.185313 -.082675         0
------------------------------------------------------------------------------
(*) dy/dx is for discrete change of dummy variable from 0 to 1
```
So from here to compute the desired probability ratio, we divide the estimated probability of a white male student with median values of grades and faminc attending a four year college (as above in question b) by the chance that the same student does not attend any college (as in the mfx above), giving us the following probability ratio (odds ratio) of  2.7277752022.


d)
25th percentile white male four-year college attendance result.
```
. mfx, predict(outcome(3)) at (grades = 4.905, faminc = 42.5, black = 0, female = 0)

Marginal effects after mlogit
      y  = Pr(psechoice==3) (predict, outcome(3))
         =  .73200748
------------------------------------------------------------------------------
variable |      dy/dx    Std. Err.     z    P>|z|  [    95% C.I.   ]      X
---------+--------------------------------------------------------------------
  grades |  -.0992425      .00695  -14.27   0.000  -.112872 -.085613     4.905
  faminc |   .0023434      .00049    4.74   0.000   .001375  .003312      42.5
  female*|  -.0450444      .03111   -1.45   0.148  -.106015  .015926         0
   black*|   .1659805      .03434    4.83   0.000   .098666  .233295         0
------------------------------------------------------------------------------
(*) dy/dx is for discrete change of dummy variable from 0 to 1
```

To calculate the difference in probabilities, we simply subtract from the value above our earlier computed value with median grades, `.732-.5239 = .2081` (Note that this is different from the result given in the hints, but I believe there may have been a typo including a leading 2 in that result, besides which the two outcomes are the same).

<!-- {BearID:450AFB11-320E-4AE2-B95D-10F837D30389-5857-00003206B52C8484} -->
[[ECON411 HW4]] [[ECON411 HW5]]
# ECON411 HW4/5 Notes
#school

 # ECON411 HW4
#school

# a) 
### i) 
*lnwage:* All else equal, we might argue that this should be positive, since higher wages might incentivize working more hours.
*educ:* I would expect people with more hours of education to work fewer hours, so the sign on this would be negative. I think this is the case because wives with more education might have less of a need to work as they would necessarily be affiliated with wealthier families.
*age:* I would expect this to be negative, as older people are likely not able to work the same hours as their younger selves.
*kidsl6:* Large negative, as wives in 1975 were probably overwhelmingly the ones who took care of young children.
*kids618:* Smaller negative, since the effect from kidsl6 would likely continue, albeit less prominently as children grow up and become more independent.
*nwifeinc:* I would expect this to be negative, as the higher the income of a wife’s family without her contribution, the less need for her to work.


### ii)
*lnwage:*
We can interpret the coefficient of -17.407 for lnwage to mean that for every 1% increase in a wife’s wage in 1975, we would expect a 17.41 hour reduction in the number of hours worked.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: -.32
T-critical (5%): 1.96

Since |t-critical| > |t-observed| we cannot reject our null hypothesis that the coefficient is equal to zero, meaning the coefficient lnwage in our structural equation is not statistically significant at the 5% level. 

*educ:* We can interpret the coefficient of educ to mean that for every additional year of education a married woman had in 1975, we would expect her to work 14.44 less hours annually.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: -0.8
T-critical (5%): 1.96

Since |t-critical| > |t-observed| we cannot reject our null hypothesis that the coefficient is equal to zero, meaning the coefficient educ in our structural equation is not statistically significant at the 5% level. 


*age:* We can interpret the coefficient of age to mean that for every additional year a married woman in 1975 had lived.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: -1.4
T-critical (5%): 1.96

Since |t-critica| > |t-observed| we cannot reject our null hypothesis that the coefficient is equal to zero, meaning the coefficient lnwage in our structural equation is not statistically significant at the 5% level. 


*kidsl6:* We can interpret the coefficient for this variable to mean that for every additional kid less than 6 years old in the household, a married woman in 1975 would typically work 342.5 fewer hours a year.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: 3.42
T-critical (5%): 1.96

Since |t-critical| < |t-observed|, we can reject our null hypothesis that the coefficient is equal to zero and thus conclude that the coefficient kidsl6 is statistically significant at the 5% level.


*kids618:* We can interpret the coefficient for this variable to mean that for every additional kid between the ages of 6 and 18 in the household, a married woman from 1975 would typically work 115 fewer hours a year.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: -3.74
T-critical (5%): 1.96

Since t-critical < t-observed we can reject our hypothesis that the coefficient is equal to zero. In other words, we know that the coefficient kids618 is statistically significant to this model.


*nwifeinc:* We can interpret the coefficient for this variable to mean “for every additional dollar the household of a wife in 1975 made minus her contribution, the wife would typically work .004 fewer hours a year.

To test the significance of this result we use a t-test:
H0: coefficient = 0
H1: H0 false

T-observed: -1.16
T-critical (5%): 1.96

Since |t-critical| > |t-observed| we cannot reject our hypothesis that the coefficient is equal to zero. In other words, we cannot conclude that the nwifeinc coefficient is statistically significant.


*Concerns:* although this structural model appears to report a reasonable overall significance (F: 5.04), the R-squared value is really low suggesting that the model does not accurately explain the underlying phenomena in wives’ work habits. More explicitly, looking at our reported significances for our coefficients, there is reason to suspect endogeneity or simultaneity among several of our variables. Lnwage seems the most likely to be endogenous given its very low t-value, but educ also seems weak.


# b)
The requested first-stage equation allows us to predict which factors affect our aforementioned variable lnwage. We are in effect regressing lnwage against all of the same other independent variables, with the addition of two new variables: exper and exper2, women’s labor market experience and labor market experience squared.

### i) Joint Significance:
In order to test the joint significance of exper and exper2 in stata, we type:
```
test exper exper2
```
Which tests:
H0: exper = exper2 = 0
H1: H0 false

Internally, state uses the Chow/Wald special F test, resulting in an F statistic of 9.33. This result is ok, as we would ideally want to see an f-value reported over 10. This allows us to reject our null hypothesis and thus conclude that the two variables exper and exper2 are jointly significant, even though the individual significance of exper2 lies slightly above the range in which we would accept it as individually significant.


### ii) Instrument Adequacy:
These instruments definitely seem like a good initial addition to our original structural equation. Given the concerns of endogeneity for lnwage, we have gone out and estimated two additional variables that appear to improve the significance of the overall model, but most importantly, improve the individual significance of key variables.

To be more specific, our R-squared value is now more than twice as large at 0.14 vs 0.067 in the original structural model. Our overall significance F-statistic has also doubled from 5.04 to 9.71, with both metrics pointing to a much more accurate model specification when including exper and exper2.

Perhaps most significantly however,  we appear to have fixed our endogeneity on the lnwage variable; with the addition of the two new explanatory variables, repeating the test for individual significance as above,  the t-observed value now becomes 5.76. As this is much larger than our required t-critical value we can now firmly reject the null hypothesis that lnwage is not individually significant.  


# c)
Given the fact that we now have good reason to suspect the coefficient for lnwage is endogenous and that exper and exper2 are suitable instruments for correcting this, we can use a Hausman test to affirm the validity of our new model, regressing hours against wage, educ, age, kidsl6, kids618, nwifeinc, and vhat. 

To test this thesis we conduct an individual significance test using:
H0: coefficient of vhat = 0
H1: H0 false

This test gives us a t-observed of -5.94, allowing us to reject the null hypothesis that the coefficient of vhat is 0. We can thus conclude that lnwage is in fact endogenous.

# d)

<!-- {BearID:007FABCC-38F7-4657-81FC-F879B37EE964-19669-000111E3F765EAE3} -->
# ECON411 HW5
#school

Consider the following structural equation for hours worked by a married woman in 1975:
```
HOURSi = B0 + B1ln(WAGE)i + B2EDUCi + B3AGEi + B4KIDSL6i + B5KIDS618i + B6NWIFEINCi + Ei
```

# a) 
This question of the homework proposes that we could improve our structural equation by taking into account that EDUC as well as LNWAGE may be endogenous. To account for this, the question proposes several new Instruments (IVs) that we could use in place of our EDUC variable, to account for the fact that years of education does not account for innate ability - what of the idea that smarter people self-select into studying more?!

Furthermore, we know we cannot keep adding IVs forever since the unnecessary surplus instruments would ruin the validity of our model, therefore we must try to select one or potentially several of the options below to use in our ultimate corrected model.

The IVs proposed to be added to our structural equation are:
### MOTHEREDUC: Wife’s mother’s education level
*Correlation with EDUC = 0.39*
This could potentially be an instrument we could use, along with Father’s education level, to control for the years of education a woman receives. We can reason that all else equal, having a more educated mother would result in her daughter also receiving more of an education.

The reported correlation of MOTHEREDUC with EDUC is reasonable but not amazing at 0.39. This suggests that while we can use this as an IV, it is probably not capturing all of the true variation in wives’ educations.

### FATHEREDUC: Wife’s father’s education level
*Correlation with EDUC = 0.42*
Similarly to MOTHEREDUC, this can be used as an instrument for years of a wife’s education, since all else equal, a woman with a more educated father could reasonably be expected to have a higher degree of education.

The reported correlation for FATHEREDUC is slightly higher than that of MOTHEREDUC at 0.42, however it is still not a very large value (corr > 0.5)

### HEDUC: Husband’s education level
*Correlation with EDUC = 0.59*
When used in conjunction with the above two IVs, this instrument could help us separate innate ability from learned education, since we could reasonably expect partners of comparable innate ability levels to become married, as once we have controlled for number of years of education with the first two IVs, this one can now control for innate ability.

In fact, the reported correlation of 0.59 is high and supports the idea that this could be a useful instrument for fixing problems of endogeneity with EDUC. 

### SIBLINGS: Wife’s number of siblings
*Correlation with EDUC = .05* 
Finally, we could try using the wife’s number of siblings as an instrument for a wife’s educational attainment. This could make sense theoretically since we could try to argue that, once we have already controlled for wealth, the wife having additional siblings would be detrimental to her educational attainment.

While the theory seems sound, our initial reported correlation value is very low at .05. This does not inherently mean this cannot be a useful instrument for EDUC, but rather just that we will need to test it more thoroughly before considering it a valid instrument.

# b) Estimate first-stage equations for EDUC and LNWAGE and test the significance of instruments used
### EDUC First-Stage Equation:
`EDUCi = B0 + B1 MOTHEREDUCi + B2 FATHEREDUCi + B3 HEDUCi + B4 SIBLINGSi`
We can run our first stage equation on education as above via bootleg (OLS) to obtain the following results:
*MOTHEREDUC*: the reported coefficient of 0.11 in this case suggests that for every additional level of a wife’s mother’s education, a wife’s educational attainment would be expected to increase by 0.11 years.
		* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of 3.57. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is larger than this, we can safely reject the null hypothesis without taking more than 5% risk of committing type-1 error.

*FATHEREDUC*: the reported coefficient of 0.10 suggests that for every additional level of a wife’s father’s education, the wife’s educational attainment would be expected to increase by 0.1 years.
		* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of 3.55. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is larger than this, we can safely reject the null hypothesis without taking more than 5% risk of committing type-1 error.
		
*HEDUC*: the reported coefficient of 12.59 suggests that for every additional year of educational attainment a wife’s husband achieves, the wife would be expected to have 0.37 additional years of education.
		* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of 3.57. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is larger than this, we can safely reject the null hypothesis without taking more than 5% risk of committing type-1 error.
	
*SIBLINGS*: the reported coefficient of 0.03 suggests that for every additional sibling, a wife would be expected to receive 0.03 extra years of education. 
		* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of 0.97. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is smaller than this, we cannot safely reject the null hypothesis without taking more than 5% risk of committing type-1 error, therefore we cannot conclude that the instrumental variable SIBLINGS is statistically significant on an individual basis.

### Testing Overall Significance:
To test overall significance of our IVs in this first-stage regression (MOTHEREDUC, FATHEREDUC, HEDUC, SIBLINGS), we can conduct a Chow/ Wald F-test:

H0: MOTHEREDUC = FATHEREDUC = HEDUC = SIBLINGS = 0
H1: H0 false
Running the above test in Stata using the command `test mothereduc fathereduc heduc siblings` gives us a reported F-value of *78.25*. Our general threshold for acceptance in an overall significance test of this type is roughly 10, so we can easily reject our null hypothesis and as a result conclude with statistical reliability that all of the coefficients are not equal to 0 in this first-stage regression.

### LNWAGE First-Stage Equation:
`LNWAGEi = B0 + B1 EXPERi + B2 EXPER2i + Ei`
Running the above first-stage regression on lnwage gives us:

*EXPER*: The reported coefficient for EXPER is 0.048, which we can interpret as meaning that for every additional year of a wife’s labor market experience, one could expect her to earn 4.8% more.
	* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of 3.4. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is larger than this, we can safely reject our null hypothesis.

*EXPER2*: The repoted coefficient orted coefficient for EXPER2 is -0.001, which we can interpret to mean that for every year of square-rooted experience in the labor market a wife has, we could expect her wages to decrease by .1%.
	* *Statistical Significance*: We can test the individual significance of the reported coefficient using a simple t-test with the following hypotheses:
		H0: coefficient = 0
		H1: H0 false
		With these parameters, we get a reported t-value of -2.43. Since we are looking at a first-stage regression, our general acceptance t-statistic is roughly 3. As our reported t-value is smaller than this, we cannot safely reject the null hypothesis without taking more than 5% risk of committing type-1 error, therefore we cannot conclude that the instrumental variable EXPER2 is statistically significant on an individual basis (although we shall see that a test of joint significance still suggests the model is more accurate with the inclusion of EXPER2).

### Testing Overall Significance:
To test overall significance of our IVs in this first-stage regression (EXPER, EXPER2), we can conduct a Chow/ Wald F-test:

H0: EXPER = EXPER2 = 0
H1: H0 false
Running the above test in Stata using the command `test exper exper2` gives us a reported F-value of *9.32*. Our general threshold for acceptance in an overall significance test of this type is roughly 10, so we cannot easily reject our null that the coefficients are equal to 0, however as this value is on the boundary of acceptability, it is still reasonable to assume that the EXPER and EXPER2 IVs may be helpful in removing endogeneity from our structural equation.

# c)
In order to test whether our IV regression gives us improved results over OLS, we can use a Hausman Chi2 test to determine whether our OLS results are consistent or not. We set up a test as follows:
H0: difference in coefficients not systematic
H1: H0 false
We can evaluate our null hypothesis using a Chow/Wald Chi2 test, giving us a value (under two degrees of freedom) of *33.73*. This is much higher than our desired threshold of 10, so we can safely reject the null and conclude that the difference in coefficients is indeed systematic, and that thus our OLS results are inconsistent. We can furthermore conclude that our IV regression is the best choice from now on.

# d)
We thus know that our original systematic equation (as reported above) is no longer the most consistent way of estimating our desired coefficients with respect to hours. We can reestimate the supply equation as follows:
`HOURSi = B0 + B1<LNWAGEi> + B2<EDUCi> + B3 AGEi + B4 KIDSL6i + B5 KIDS618i + B6 NWIFEINCi + Ei` where we can now replace `<LNWAGE>` and `<EDUC>` to arrive at our new structural equation:
`HOURSi = B0 + B1 EXPERi + B2 EXPER2i + B3 FATHEREDUCi + B4 MOTHEREDUCi + B5 HEDUCi + B6 SIBLINGS + B7 AGEi + B8 KIDSL6i + B9 KIDS618i + B10 NWIFEINCi + Ei`

Estimating this in stata, we get the following, improved estimates for our coefficients:
### AGE:
Same negative sign, larger magnitude: the recalculated coefficient for age demonstrates the need for redoing the structural supply equation given at the start. We can see that our original OLS estimate of -7.72 hours worked by a wife per additional year of age was quite far off from the new estimate of -20 hours worked by a wife per additional year of age. The individual significance of AGE appears to have not changed much.
### KIDSL6:
Same negative sign, however interestingly slightly smaller magnitude. It appears that after controlling for aspects of education and experience, we now have a slightly smaller total effect attributed to kids under 6, from -342 hours worked per year under OLS to -308 hours worked per year under our new structural equation. This suggests that some of the bias from endogeneity made kidsl6 inconsistent. Interestingly, the individual significance t-statistic decreased slightly from -3.42 to -3.21, suggesting a slight fall in predictive value. This is perhaps a slight surprise, given the assumption that the new model “is better” than the old one. Clearly, this is not the case unilaterally.
### KIDS618:
The sign is the same, however the magnitude of the coefficient has greatly decreased from -115 to -74.96. This suggests that some of the variation in hours worked that was initially being attributed to kids between 6 and 18 is actually more likely to be a result of either experience or education. It is even more interesting that the individual statistical significance is unchanged for this coefficient at around 30. 
### NWIFEINC: 
This is the only original coefficient who’s sign has changed entirely, from negative under the original OLS equation to positive under the reestimate one. While the sign has flipped for the coefficient, the individual significance t-statistic has actually decreased dramatically, from -1.16 to 0.22. Regardless, even in our original regression this coefficient was not interesting because of its individual significance, but rather its overall contribution to the validity of the model (as seen in the literature by Mroz)

# e) Test for Overidentifying restrictions
We can use a Sargan Chi2 test after our initial 2SLS IV regression to determine whether we have added too many IVs. Running the command ` estat overid `
Returns a Chi2 value (with 4 restrictions, equal to the number of IVs - the number of Endogenous vars, 6-2) of *2.667*, corresponding to a probability of committing type one error of 0.6149 under our new model. Our theoretical threshold level is a p-value less than 0.05, which this Chi2 is not, so we would preferably opt to decrease the number of IVs added, trying to aim for 1-2 per variable (rather than current average of 3). Adding additional variables only makes sense if they improve the model, so in this case we would ideally opt to limit them. A potential candidate for removal is SIBLINGS, since this IV had the lowest individual significance of all the IVs added, and removing SIBLINGS from the equation results in the smallest possible drop in overall significance as compared with removing any of the other 5 IVs.

# Appendix - Stata do-file:
```
* A
corr

* B
* First stage for EDUC:
keep if lfp == 1
ivregress 2sls hours (lnwage educ = exper exper2 mothereduc fathereduc heduc siblings) age kidsl6 kids618 nwifeinc
estat overid
est store ivreg_estimates

* First stage bootleg:
reg educ mothereduc fathereduc heduc siblings
test mothereduc fathereduc heduc siblings
predict educ_ehat, resid

* First stage for LNWAGE:
reg lnwage exper exper2
test exper exper2
predict lnwage_ehat, resid

* C - Run Hausman test
reg hours lnwage educ age kidsl6 kids618 nwifeinc
est store olsreg_estimates
hausman ivreg_estimates olsreg_estimates, constant sigmamore

* D - New supply equation:
reg hours exper exper2 fathereduc mothereduc heduc siblings age kidsl6 kids618 nwifeinc

```

### Stata Output:
```
. 
. * A
. corr
(obs=428)

             | taxabl~c federa~x hsibli~s hfathe~c hmothe~c siblings      lfp    hours   kidsl6  kids618      age
-------------+---------------------------------------------------------------------------------------------------
  taxableinc |   1.0000
  federaltax |   0.9674   1.0000
   hsiblings |  -0.1744  -0.1452   1.0000
 hfathereduc |   0.2092   0.1730  -0.2948   1.0000
 hmothereduc |   0.2322   0.2096  -0.2888   0.5166   1.0000
    siblings |  -0.2137  -0.1747   0.2197  -0.2126  -0.2211   1.0000
         lfp |        .        .        .        .        .        .        .
       hours |   0.1012   0.1020  -0.0137   0.1413   0.0680  -0.0949        .   1.0000
      kidsl6 |   0.0333   0.0251  -0.0540  -0.0180  -0.0011   0.0166        .  -0.1682   1.0000
     kids618 |  -0.0225  -0.0333  -0.0105  -0.0120  -0.0549  -0.0270        .  -0.1761   0.0907   1.0000
         age |  -0.0169  -0.0001   0.0493  -0.0263  -0.0701  -0.0275        .   0.0549  -0.3384  -0.3976   1.0000
        educ |  -0.0644  -0.0776  -0.0138  -0.0073   0.0149   0.0491        .  -0.0649   0.1293  -0.0925  -0.0522
        wage |  -0.0300  -0.0452   0.0184  -0.0185  -0.0801  -0.0257        .  -0.0976   0.0314  -0.0792   0.0304
      wage76 |   0.0782   0.0356  -0.0074   0.0515  -0.0214   0.0289        .   0.2686  -0.0298  -0.1337   0.0040
      hhours |  -0.0631  -0.0561  -0.0452   0.0268   0.0495   0.0838        .  -0.0162  -0.0190   0.1153  -0.1215
        hage |  -0.0086  -0.0024   0.0505   0.0187  -0.0202  -0.0441        .   0.0458  -0.3530  -0.3547   0.8944
       heduc |  -0.0374  -0.0375  -0.0560  -0.0146   0.0260   0.0154        .  -0.0859   0.1049  -0.0310  -0.0693
       hwage |  -0.0390  -0.0408  -0.0390   0.0709  -0.0245  -0.0806        .  -0.1103  -0.0209  -0.0204   0.0887
      faminc |  -0.0292  -0.0157  -0.0150   0.0391  -0.0094  -0.0166        .   0.1507  -0.0720  -0.0487   0.1139
         mtr |   0.0178   0.0090   0.0419  -0.0999  -0.0220   0.0320        .  -0.1815   0.1247   0.1565  -0.1239
  mothereduc |  -0.0076  -0.0054  -0.0523  -0.0212  -0.0061   0.0469        .  -0.0185   0.0614   0.0455  -0.2249
  fathereduc |  -0.0133  -0.0378  -0.1095   0.0828   0.0237  -0.0111        .  -0.0583   0.0639  -0.0466  -0.1097
unemployment |  -0.0123  -0.0432  -0.0077  -0.0307  -0.0075  -0.0217        .  -0.0792   0.0143  -0.0175   0.0925
   largecity |  -0.0003  -0.0311   0.0105   0.0899   0.0914  -0.0921        .  -0.0230   0.0073  -0.0890   0.0995
       exper |   0.0377   0.0389   0.0959  -0.0244  -0.0712   0.0133        .   0.2992  -0.1856  -0.3874   0.4836
      lnwage |  -0.0218  -0.0337   0.0830  -0.0434  -0.0732  -0.0193        .  -0.0166  -0.0185  -0.1200   0.0549
      exper2 |   0.0264   0.0304   0.0900  -0.0292  -0.0849   0.0247        .   0.2594  -0.1706  -0.3658   0.5069
    nwifeinc |  -0.0484  -0.0286  -0.0210   0.0269   0.0060  -0.0048        .  -0.0815  -0.0275   0.0330   0.0970
_est_ivreg~s |        .        .        .        .        .        .        .        .        .        .        .
_est_olsre~s |        .        .        .        .        .        .        .        .        .        .        .

             |     educ     wage   wage76   hhours     hage    heduc    hwage   faminc      mtr mother~c father~c
-------------+---------------------------------------------------------------------------------------------------
        educ |   1.0000
        wage |   0.3420   1.0000
      wage76 |   0.2435   0.4220   1.0000
      hhours |   0.0959  -0.0322  -0.0493   1.0000
        hage |  -0.0699   0.0257   0.0004  -0.1319   1.0000
       heduc |   0.5943   0.1663   0.1302   0.1440  -0.1139   1.0000
       hwage |   0.3030   0.2159   0.1222  -0.2844   0.0724   0.3964   1.0000
      faminc |   0.3623   0.3027   0.2684   0.1436   0.0867   0.3547   0.6688   1.0000
         mtr |  -0.4134  -0.3143  -0.2897  -0.1889  -0.1027  -0.4385  -0.6910  -0.8845   1.0000
  mothereduc |   0.3870   0.0571   0.0409   0.0746  -0.2195   0.2752   0.0876   0.1154  -0.1563   1.0000
  fathereduc |   0.4154   0.1077   0.1003   0.0625  -0.0862   0.3346   0.1506   0.1690  -0.2178   0.5541   1.0000
unemployment |   0.1222   0.0323   0.0432  -0.1702   0.0738   0.0679   0.1737   0.0657  -0.0819  -0.0035   0.0669
   largecity |   0.1588   0.1194   0.1385  -0.1615   0.0662   0.2365   0.3630   0.2611  -0.2906   0.0347   0.1856
       exper |  -0.0152   0.0550   0.2130  -0.0888   0.4139  -0.0832  -0.1117  -0.0275  -0.0430  -0.1116  -0.1218
      lnwage |   0.3433   0.8286   0.5133  -0.0038   0.0390   0.1586   0.1686   0.3508  -0.3678   0.0471   0.0777
      exper2 |  -0.0353   0.0424   0.1784  -0.0762   0.4282  -0.0956  -0.1242  -0.0440  -0.0209  -0.1239  -0.1358
    nwifeinc |   0.2844   0.1420   0.0381   0.1789   0.0706   0.3479   0.6977   0.9299  -0.7981   0.0896   0.1492
_est_ivreg~s |        .        .        .        .        .        .        .        .        .        .        .
_est_olsre~s |        .        .        .        .        .        .        .        .        .        .        .

             | unempl~t largec~y    exper   lnwage   exper2 nwifeinc _est_i~s _est_o~s
-------------+------------------------------------------------------------------------
unemployment |   1.0000
   largecity |   0.1824   1.0000
       exper |   0.0308   0.0180   1.0000
      lnwage |   0.0414   0.0971   0.1693   1.0000
      exper2 |   0.0091   0.0006   0.9525   0.1261   1.0000
    nwifeinc |   0.0806   0.2457  -0.1609   0.1414  -0.1615   1.0000
_est_ivreg~s |        .        .        .        .        .        .        .
_est_olsre~s |        .        .        .        .        .        .        .        .


. 
. * B
. * First stage for EDUC:
. keep if lfp == 1
(0 observations deleted)

. ivregress 2sls hours (lnwage educ = exper exper2 mothereduc fathereduc heduc siblings) age kidsl6 kids618 nwifeinc

Instrumental variables (2SLS) regression          Number of obs   =        428
                                                  Wald chi2(6)    =      22.52
                                                  Prob > chi2     =     0.0010
                                                  R-squared       =          .
                                                  Root MSE        =     1244.5

------------------------------------------------------------------------------
       hours |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      lnwage |   1452.066   427.1818     3.40   0.001     614.8049    2289.327
        educ |  -123.3164   55.05881    -2.24   0.025    -231.2297   -15.40313
         age |  -9.242836    9.18779    -1.01   0.314    -27.25057    8.764902
      kidsl6 |  -248.8949   168.5478    -1.48   0.140    -579.2425    81.45266
     kids618 |  -39.65773   56.22634    -0.71   0.481    -149.8593    70.54387
    nwifeinc |  -.0118557   .0068771    -1.72   0.085    -.0253345    .0016232
       _cons |   1836.672   741.2379     2.48   0.013     383.8723    3289.471
------------------------------------------------------------------------------
Instrumented:  lnwage educ
Instruments:   age kidsl6 kids618 nwifeinc exper exper2 mothereduc
               fathereduc heduc siblings

. estat overid

  Tests of overidentifying restrictions:

  Sargan (score) chi2(4) =  2.66743  (p = 0.6149)
  Basmann chi2(4)        =  2.61517  (p = 0.6241)

. est store ivreg_estimates

. 
. * First stage bootleg:
. reg educ mothereduc fathereduc heduc siblings

      Source |       SS           df       MS      Number of obs   =       428
-------------+----------------------------------   F(4, 423)       =     78.25
       Model |  948.442932         4  237.110733   Prob > F        =    0.0000
    Residual |  1281.75333       423  3.03014971   R-squared       =    0.4253
-------------+----------------------------------   Adj R-squared   =    0.4198
       Total |  2230.19626       427  5.22294206   Root MSE        =    1.7407

------------------------------------------------------------------------------
        educ |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
  mothereduc |   .1101063   .0308505     3.57   0.000     .0494668    .1707457
  fathereduc |   .1047907   .0295261     3.55   0.000     .0467546    .1628268
       heduc |   .3733769   .0296499    12.59   0.000     .3150975    .4316564
    siblings |   .0344349   .0355288     0.97   0.333    -.0354001    .1042699
       _cons |   5.847372   .4059007    14.41   0.000     5.049539    6.645206
------------------------------------------------------------------------------

. test mothereduc fathereduc heduc siblings

 ( 1)  mothereduc = 0
 ( 2)  fathereduc = 0
 ( 3)  heduc = 0
 ( 4)  siblings = 0

       F(  4,   423) =   78.25
            Prob > F =    0.0000

. predict educ_ehat, resid

. 
. * First stage for LNWAGE:
. reg lnwage exper exper2

      Source |       SS           df       MS      Number of obs   =       428
-------------+----------------------------------   F(2, 425)       =      9.32
       Model |  9.38142005         2  4.69071002   Prob > F        =    0.0001
    Residual |  213.946021       425  .503402402   R-squared       =    0.0420
-------------+----------------------------------   Adj R-squared   =    0.0375
       Total |  223.327441       427  .523015084   Root MSE        =    .70951

------------------------------------------------------------------------------
      lnwage |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       exper |   .0476388   .0140012     3.40   0.001     .0201185    .0751591
      exper2 |  -.0010159   .0004177    -2.43   0.015    -.0018369   -.0001949
       _cons |   .8075371   .1000761     8.07   0.000     .6108314    1.004243
------------------------------------------------------------------------------

. test exper exper2

 ( 1)  exper = 0
 ( 2)  exper2 = 0

       F(  2,   425) =    9.32
            Prob > F =    0.0001

. predict lnwage_ehat, resid

. 
. * C - Run Hausman test
. reg hours lnwage educ age kidsl6 kids618 nwifeinc

      Source |       SS           df       MS      Number of obs   =       428
-------------+----------------------------------   F(6, 421)       =      5.04
       Model |  17228385.5         6  2871397.59   Prob > F        =    0.0001
    Residual |   240082634       421   570267.54   R-squared       =    0.0670
-------------+----------------------------------   Adj R-squared   =    0.0537
       Total |   257311020       427   602601.92   Root MSE        =    755.16

------------------------------------------------------------------------------
       hours |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      lnwage |  -17.40781   54.21544    -0.32   0.748    -123.9745    89.15887
        educ |  -14.44486   17.96793    -0.80   0.422    -49.76289    20.87317
         age |  -7.729976    5.52945    -1.40   0.163    -18.59874    3.138792
      kidsl6 |  -342.5048   100.0059    -3.42   0.001     -539.078   -145.9317
     kids618 |  -115.0205   30.82925    -3.73   0.000    -175.6189   -54.42208
    nwifeinc |  -.0042458   .0036558    -1.16   0.246    -.0114317    .0029401
       _cons |   2114.697   340.1307     6.22   0.000     1446.131    2783.263
------------------------------------------------------------------------------

. est store olsreg_estimates

. hausman ivreg_estimates olsreg_estimates, constant sigmamore

Note: the rank of the differenced variance matrix (2) does not equal the number of coefficients being tested (7); be
        sure this is what you expect, or there may be problems computing the test.  Examine the output of your
        estimators for anything unexpected and possibly consider scaling your variables so that the coefficients are on
        a similar scale.

                 ---- Coefficients ----
             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
             |  ivreg_esti~s olsreg_est~s    Difference          S.E.
-------------+----------------------------------------------------------------
      lnwage |    1452.066    -17.40781        1469.474        253.4754
        educ |   -123.3164    -14.44486       -108.8716        28.16584
         age |   -9.242836    -7.729976        -1.51286        .7114893
      kidsl6 |   -248.8949    -342.5048         93.6099        21.41296
     kids618 |   -39.65773    -115.0205        75.36278        14.61359
    nwifeinc |   -.0118557    -.0042458       -.0076098         .002012
       _cons |    1836.672     2114.697       -278.0254        294.2918
------------------------------------------------------------------------------
                       b = consistent under Ho and Ha; obtained from ivregress
          B = inconsistent under Ha, efficient under Ho; obtained from regress

    Test:  Ho:  difference in coefficients not systematic

                  chi2(2) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =       33.73
                Prob>chi2 =      0.0000
                (V_b-V_B is not positive definite)

. 
. * D - New supply equation:
. reg hours exper exper2 fathereduc mothereduc heduc siblings age kidsl6 kids618 nwifeinc

      Source |       SS           df       MS      Number of obs   =       428
-------------+----------------------------------   F(10, 417)      =      7.45
       Model |    39004484        10   3900448.4   Prob > F        =    0.0000
    Residual |   218306536       417  523516.873   R-squared       =    0.1516
-------------+----------------------------------   Adj R-squared   =    0.1312
       Total |   257311020       427   602601.92   Root MSE        =    723.54

------------------------------------------------------------------------------
       hours |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       exper |   45.05642   14.50229     3.11   0.002     16.54972    73.56313
      exper2 |  -.4642345   .4365997    -1.06   0.288    -1.322445    .3939761
  fathereduc |  -8.370795    12.3941    -0.68   0.500     -32.7335    15.99191
  mothereduc |   6.062705   13.10794     0.46   0.644    -19.70317    31.82858
       heduc |  -15.45753   13.12072    -1.18   0.239    -41.24853    10.33347
    siblings |  -34.02255   14.80333    -2.30   0.022    -63.12099   -4.924101
         age |  -20.01276   5.997165    -3.34   0.001     -31.8012   -8.224317
      kidsl6 |  -308.1709   95.92283    -3.21   0.001    -496.7235   -119.6184
     kids618 |  -74.95704   30.32069    -2.47   0.014    -134.5575   -15.35659
    nwifeinc |   .0007936   .0036727     0.22   0.829    -.0064258    .0080129
       _cons |   2117.718   343.2653     6.17   0.000     1442.972    2792.464
------------------------------------------------------------------------------
. 
```

<!-- {BearID:700CDFCF-A18D-41C1-A123-FFFF13E14681-37181-000206624BC618B5} -->
[[006 School MOC]] 
# ECON411 HW6
#school/Economics/ec411/hw #concepts/math/stats
#school
[[Stats Simultaneous Equations]]
[[Hausman Test]]

# a.
```
Eq 1: Md = B0 + B1Y + B2R + B3P + Ed
Eq 2: Ms = a0 + a1Y + Es
```
Where Md is money supply, B0-B3 are the coefficients of the first structural equation, R is the 3-month T-bill rate, P is the CPI with 1982=100, Ms is the money supply, Y is gdp in billions and Es and Ed are the error terms for money supply and money demand respectively

## Equation 1:
To estimate the first structural equation, we can run:
```
. * Equation 1, OLS:
. reg m y r p
      Source |       SS           df       MS      Number of obs   =        37
-------------+----------------------------------   F(3, 33)        =   1080.85
       Model |   120927214         3  40309071.4   Prob > F        =    0.0000
    Residual |  1230702.09        33  37294.0026   R-squared       =    0.9899
-------------+----------------------------------   Adj R-squared   =    0.9890
       Total |   122157916        36  3393275.45   Root MSE        =    193.12
------------------------------------------------------------------------------
           m |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           y |   .6495857   .0621198    10.46   0.000     .5232021    .7759693
           r |  -10.06647   14.79329    -0.68   0.501    -40.16365     20.0307
           p |   6.518964   2.679411     2.43   0.021     1.067661    11.97027
       _cons |  -1960.716   210.6037    -9.31   0.000    -2389.192   -1532.239
```
### Interpretation:
We can interpret the result from the first structural equation naively to tell a seemingly strong story, until we begin to address the issue of endogeneity.

Our overall significance appears high, with an F(3, 33) statistic of 1080, corresponding to a P-value of 0. High R squared and adjusted R squared values of almost 0.99 further purport that the model does a great job of explaining the variation in money demand. We shall soon see that this is not entirely the case.

Our coefficient estimates above can be interpreted as:
	* *Y:* This coefficient can be interpreted as the marginal propensity to consume, stating that for every additional billion dollars of income, money demand will go up by ~$0.65 billion. The individual t-statistic appears excellent at 10, suggesting that with a t-test of the form:
		H0: coefficient = 0
		H1: H0 false
	We can reject the null that the coefficient is equal to 0, since our observed t-value is so large.

	* *R:* This coefficient of -10.067 can be interpreted as the reactivity of money demand  to changes in the three-month T-bill. This coefficient is stating that for each additional percentage point of interest rates, money demand drops by $10 billion. We can test the individual significance of the reported coefficient using the t-test:
		H0: coefficient = 0
		H1: H0 false
	Since our reported t-statistic is only -0.68, we cannot reject our null hypothesis that the coefficient is equal to 0, thus this result is not statistically significant.

	* *P:* This coefficient of 6.5 can be interpreted to mean that for every additional percentage point increase in the CPI chained to 1982, we can expect money demand to increase by $6.5 billion. This roughly aligns with our theoretical expectations, since we would certainly to expect the nominal demand for money to rise if prices rose. We can test the significance of this coefficient using the t-test:
		H0: coefficient = 0
		H1: H0 false
	Our reported t-observed statistic of 2.43 allows us to safely reject the null hypothesis without the risk of committing type 1 error. The resulting p-value of .021 confirms this.

	* *Constant:* I do not think the constant has any theoretical or practical applicability in this case, since an income of 0 does not make sense.
 
## Equation 2:
```
. * Equation 2, OLS:
. reg m y
      Source |       SS           df       MS      Number of obs   =        37
-------------+----------------------------------   F(1, 35)        =   2910.35
       Model |   120706295         1   120706295   Prob > F        =    0.0000
    Residual |  1451621.36        35  41474.8961   R-squared       =    0.9881
-------------+----------------------------------   Adj R-squared   =    0.9878
       Total |   122157916        36  3393275.45   Root MSE        =    203.65
------------------------------------------------------------------------------
           m |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           y |   .7942336   .0147223    53.95   0.000     .7643457    .8241214
       _cons |  -2218.278   103.7868   -21.37   0.000    -2428.977    -2007.58
```
### Interpretation:
Overall significance for the estimated model is very high, with an F-statistic of 2910, which, at F(1, 35) degrees of freedom, corresponds to a P-value of 0. Furthermore, R squared and adjusted R squared are both nearly 0.99, suggesting the model almost entirely explains our dependent variable.

Equation two allows us to estimate just one coefficient, the marginal propensity to consume, or how much the demand for money increases from a proportional increase in income. The coefficient of 0.794 suggests that for every additional Billion dollars of income, we would expect the demand for money to increase by $.794 billion. We can test the individual significance of the model by using the following t-test:
	H0: Coefficient = 0
	H1: H0 false
The reported t-statistic of 53.95 seemingly suggests that this coefficient is overwhelmingly significant, however we will soon see that there are some deeper issues at play making these t-statistics almost useless.


# b. 
The issue with the above estimated simultaneous equations is the endogeneity or simultaneity of M and Y. This creates significant issues for us if we use OLS, as we will neither get consistent nor unbiased estimates from OLS if endogeneity is present among our variables. This is the case because our error terms are likely correlated with our results, since we are not adequately capturing the two-way relationship between money demand and income (M and Y)

# c. 
The key features of a reduced form equation are that each endogenous variable (M and Y) is regressed against all exogenous variables (R and P). Furthermore, the idea is to create Mhat and Yhat values which we can then use in our structural equations to predict our money supply and demand, hopefully (somewhat) solving the issue of simultaneity.

### Deriving RFy 

### Deriving RFm 
Thus we have correctly derived our two reduced form equations as functions of our exogenous variables R and P.

# d. 

### RFy:
```
. reg y r p
      Source |       SS           df       MS      Number of obs   =        37
-------------+----------------------------------   F(2, 34)        =    319.59
       Model |   181687689         2  90843844.5   Prob > F        =    0.0000
    Residual |  9664501.06        34  284250.031   R-squared       =    0.9495
-------------+----------------------------------   Adj R-squared   =    0.9465
       Total |   191352190        36  5315338.61   Root MSE        =    533.15
------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           r |  -105.9705    36.5745    -2.90   0.007    -180.2988   -31.64218
           p |    41.4663   2.036437    20.36   0.000     37.32776    45.60484
       _cons |   2422.776   406.7154     5.96   0.000     1596.231    3249.321
. predict yhat
. predict ehat, resid
```
In this reduced form equation on y, we regress our endogenous y against our two exogenous variables, R and P.

Overall, our results appear to be statistically significant. High R-squared and adjusted R squared values of .9495 and .9465 respectively show that our model explains the majority of the variation in Y. A high overall significance F statistic of 319 supports this further.

The reported coefficients cannot simply be interpreted the same way as our prior OLS results. Rather, since we had to derive these reduced form equations, the coefficients correspond to:

	* *R:* Coefficient of -105.97 is equal to a ratio our earlier estimated coefficients (B2/a1-b1). It does not have any special significance in this case, beside being used to predict our yhat, however we could interpret it to mean that for every additional percentage point of the 3-month treasury rate, we would expect income to fall by $105 Billion. We can test the individual significance of the coefficient using a simple t-test:
		H0: coefficient = 0
		H1: H0 false
	This setup gives us a reported t-observed statistic of -2.9, above our critical value of 1.96, allowing us to reject the null and conclude that this coefficient is significant.

	* *P:* The coefficient of 41.46 for P is equivalent to a ratio of our earlier coefficients, (B3/a1-b1), and can be interpreted to mean that for every additional percentage point rise of the CPI above the benchmark price in 1982, we can expect income to increase by $41 billion. We can test significance using the simple t-test:
		H0: coefficient = 0
		H1: H0 false
	Giving us a t-observed of 20.36, allowing us to reject the null that the coefficient is statistically indistinguishable from 0.

### RFm:
To estimate the following reduced form equation, we run the following in state:
```
. reg m r p
      Source |       SS           df       MS      Number of obs   =        37
-------------+----------------------------------   F(2, 34)        =    374.18
       Model |   116849167         2  58424583.3   Prob > F        =    0.0000
    Residual |  5308749.72        34  156139.698   R-squared       =    0.9565
-------------+----------------------------------   Adj R-squared   =    0.9540
       Total |   122157916        36  3393275.45   Root MSE        =    395.15
------------------------------------------------------------------------------
           m |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           r |  -78.90339   27.10721    -2.91   0.006    -133.9919   -23.81491
           p |   33.45488   1.509307    22.17   0.000      30.3876    36.52216
       _cons |  -386.9148   301.4373    -1.28   0.208    -999.5091    225.6795
```
Thankfully, our results in this reduced form equation are comparable to those in the other one, and the signs match our theoretical expectations. 

Overall, the significance of the model appears high, even seeming slightly better than  the one above. Our R-squared and adjusted R-squared values show a slight improvement to .9565 and .9540 respectively, while our F-statistic also rose to 374 as compared with 319 before.

We can interpret these results similarly to above, and our individual t-statistics are again low enough that all p-values are less than 0.05 (using the same null hypotheses that the coefficients are 0, H1 that H0 is false)

# e. 
To test for whether Y is endogenous, we can use the Hausman test.  To do this, we first save the residuals from our reduced form equation for y above, hereafter referred to as ehat. We now rerun our original structural equation for Ms, adding ehat as a last variable:
` reg m y ehat`

We can now set up our Hausman test using the hypotheses:
	H0: coefficient for ehat = 0 (Y is exogenous)
	H1: H0 false
Running the above noted regression yields a t-value of -2.42. Generally, our t-critical value for this individual significance is 3, so we cannot reject the null that ehat is equal to 0, in other words confirming that Y is likely to be endogenous. 

# f.
To check the identification criteria of our two structural equations in order to be able to estimate them, we must calculate their IC:
*Exclusion criterion: (K - k) >= m - 1*
### Equation 1
K - number of exogenous variables in the system = 2 (R, P)
k - number of exogenous vars in this structural equation = 2
m - number of endogenous variables in a structural equation
(K - k) >= m - 1
(2 - 2) >= 2 - 1
0 >= 1
Thus as the exclusion criterion is not met, we can conclude that equation 1 is under identified and we cannot use it for 2SLS
### Equation 2
K = 2
k = 0
m = 2
(2 - 0) >= 2 - 1
2 >= 1
Thus as the exclusion criterion is met, we can conclude that equation 1 is in fact overidentified, meaning we can use this equation with 2SLS.

We can only conduct 2SLS with equations that are at least adequately identified, since otherwise we do not have enough variables to predict with and end up with perfect multicollinearity.


# g.
We can estimate only equation 2 via 2SLS, as explained above. To do this in stata we write:
```
. ivregress 2sls m (y = r p)
Instrumental variables (2SLS) regression          Number of obs   =         37
                                                  Wald chi2(1)    =    2955.07
                                                  Prob > chi2     =     0.0000
                                                  R-squared       =     0.9880
                                                  Root MSE        =     198.84
------------------------------------------------------------------------------
           m |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           y |   .8019278    .014752    54.36   0.000     .7730144    .8308412
       _cons |   -2269.62   103.7225   -21.88   0.000    -2472.912   -2066.328
Instrumented:  y
Instruments:   r p
``` 
These results line up quite closely with what we had before, however we have now determined that our results are unbiased and consistent. 

Our theoretical expectations line up closely with what we would expect, we see a predicted MPC of 0.802, very close to our earlier prediction of .794. 

For individual significance, our coefficient now reports a z-statistic as opposed to a t-statistic, giving us a slightly more robust statistic. Using the hypotheses:
	H0: coefficient for Y = 0
	H1: H0 false
Our reported z statistic of 54.36 allows us to easily reject the null that the coefficient is equal to 0. 

Overall significance for the model appears to be almost the same as before, with a reported Wald chi2 statistic of 2955 (H0: all coefficients 0, H1: H0 false) comparing closely to our prior overall Wald F statistic of 2910.

Our R squared appears roughly unchanged at .988.

All told, the 2SLS model appears to be mildly better than OLS at predicting money supply.

[file:8AD482CD-D308-4F73-918F-F2B5FF6004E6-37181-0003D79708680007/ECON411_F20__Wunnava_Syllabi.pdf]

<!-- {BearID:09D7904B-1D77-4588-95B4-B5C746AAA5CE-37181-0003927E115D111F} -->
[[ECON411 HW7]]
# ECON411 HW7 Notes
#school 
Er1
Reestimate with more robust standard errors! Will give smaller T-vals



## PDL REG:
 
### Shortcut: 
## Granger: 

F test tests joint significance of coefficients!
A: UR -> for RR, assume PDI(-1 to -4) = 0
So RR: 

For B, RR is:
 
1.  
 
3.  View -> coefficient diagnostics -> Wald test
Test that coeffs 6, 7, 8, 9 = 0! 
If F-Val here is significant, then we can reject null!
 
## Sims Causality! 
It is not possible for the future to cause the present!
So we add lead terms! Xt+1 etc! Cannot be significant, so if they are must be influenced by present! There will be a relationship between Y and leading X Vals

SIMS UR: 
To restrict, we assume PDI(1 to 4) = 0!!
WHICH IS OUR UR FROM GRANGER!!!!! 
[[ECON411 HW8 Notes]]

<!-- {BearID:B7B8FDDA-11EE-41A1-BA9E-A278016623F2-87251-0005177462BF0D16} -->
[[ECON411 Applied Econometrics]]
# ECON411 HW7
#school
# a) 
Using the ad-hoc unrestricted finite distributed lag model we can estimate the following equation:
`PCE C PDI PDI-1 PDI-2 PDI-3 PDI-4 PDI-5 PDI-6 PDI-7 PDI-8`
Eviews returns the following result: 

On a surface level, this ad hoc model appears to be extremely effective - the overall significance F test supports the model:
H0: C = PDI = PDI-1 = PDI-2 = PDI-3 = PDI-4 = PDI-5 = PDI-6 = PDI-7 = PDI-8 = 0
H1: H0 false
Returns a very large F statistic of 1175, allowing us to reject the null that the coefficients are insignificant.
The R-squared and Adjusted R-squared values are also both extremely high at .9934 and .9926 respectively, indicating that the model ostensibly predicts the variation in the data with a high degree of certainty.


# b)
Running the equation above with a variety of 4, 5, 6, 7 and 8 lags returns the following values:
	* *5:*
		* adj-R: .989327
		* SC: 13.966
		* AIC: 13.762
	* *6:*
		* adj-R: .9914
		* SC: 13.7950
		* AIC: 13.5602
 	* *7:*
	 	* adj-R: .9923
	 	* SC: 13.7188
	 	* AIC: 13.4528
	* *8:*
		* adj-R: .9926
		* SC: 13.7213
		* AIC: 13.4235

Above we report the adjusted R2, Schwarz Criterion and AIC for lag models from 5  to 8. We choose not report other values as adjusted R2 falls below .98 and AIC goes above 14, so we know that only the latter ones are relevant.

Our 7-lag and 8-lag models appear to be the two most accurate. Adjusted R-squared values are best for the 8-lag model, but this is expected since adding explanatory variables always improves R-squared. As our Schwartz criteria and Akaike criteria are lowest with the 8-lag ad hoc model, we shall select this one as our preferred lag structure. 


# c)
We can reestimate the 8-lag model using a modified ARDL test, specifically using the Koyck model, to get the following results: 
This model returns us several coefficients, but the most interesting one here is the coefficient for PCE(-1): 0.864367. This represents our Lambda value, and thus tells us more clearly how the lag effect decays over time - in this case, trending towards 0 in multiples of .8644

Our F-statistic has shot up to 10926, using the overall significance test:
H0: C = PDI = PCE(-1) = 0
H1: H0 false

And our Adjusted R-squared remains impressively high at .9961 

# d)
* *Mean Lag:* [Lambda/(1-Lambda)] = [0.864367 / (1-0.864367] = 6.3728369939
	* We can interpret this value as the average lag effect of independent variable appropriations on capital expenditures. The value of ~6.373 means that the average lag effect of expenditures is $6.373 million.
* *Median Lag:* = -[log(2)/log(lambda)] = -4.755471262
	* We can interpret this value to mean that 50% of the effect happens in -$4.755 million
* *Long Run Multiplier:* B0 * (1 / (1 - Lambda)) = -50.9728 * (1 / (1 - 0.864367)) = -375.8141455251
	* The long run multiplier effect of expenditures can be interpreted to be -$375 million
# e)
## 	a)
We can estimate the requested second degree polynomial Almon lag model as follows: 


## 	b)
Using the same method as above, we can reestimate the model with third degree polynomials thus: 

The coefficients appear to be quite similar when comparing the lag effect over the 8 periods. The sum of lags is roughly similar with both models at .933 with the second-degree model and .936 with the third degree model.

Comparing significances, adjusted R-squared values are .9925 and .9929 for the second and third degree models respectively, not enough to allow us to decide. SC values for the respective models are 13.487 and 13.474, slightly in favor of the third-degree model, while AIC values are 13.368 and 13.325 respectively. All three of these measures of significance suggest the third-degree model is a better predictor of our data. The one statistic that does not align with this conclusion is the overall F-statistic, but we can understand this would be lower given a specification with more variables in the third degree model, so we will not use it in making our comparison of the models. We can say that using the hypotheses:
H0: C = PDL1 = PDL2 = PDL3 = 0
H1: H0 false

And 
H0: C = PDL1 = PDL2 = PDL3  = PDL4 = 0
H1: H0 false

Both f-tests allow us to reject the null hypotheses that the coefficients are not significant overall.

# f) 
The above three granger causality tests can be used to determine the direction of causality among our variables. Under the null hypotheses listed, we can see that interestingly the 7 lag model we decided to use at the start of this worksheet is the only one with a statistically significant degree of unidirectional causality; the null hypotheses that PDI does not granger cause PCE is soundly rejected with an F-statistic of 9.876, while the reverse null that PCE does not granger cause PDI cannot be rejected, meaning we can conclude that there is a unidirectional causality whereby PCE granger causes PDI.

(Professor, I understand now that you wanted us to use the bootleg, however I completed these tests on my own before we had out Thursday class, and unfortunately really do not have the time now to go back and redo these via bootleg. I understand this is not fully what you are looking for, and I apologize for not being able to do this properly in a timely manner. I hope that with my explanation, the above can be sufficient. I have already spent nearly 15 hours working on this assignment as of this question due to a number of technical difficulties with eviews, and unfortunately must move on to at least do some of my other required assignments for the week)

# g)
For the Sims causality test, we will be creating two regressions, one using future values of PDI, and use that to show that there is causality (since we know the future cannot influence the present) 
Running our unrestricted regression, which included the future lags:
 
We can now use a Wald test testing that:
H0: PDI(1) = PDI(2) = PDI(3) = PDI(4) = 0
H1: H0 false

Running the above test we get the output:
  
Giving us an F-value of 2.073. This does not allow us to easily reject the null that the future values are insignificant, meaning that we cannot reject our null hypothesis that there is no unidirectional causality among the variables in question.

<!-- {BearID:2A85DF10-8B1F-4C34-B6AC-51B652E17904-87251-0005263D5846FF19} -->
# ECON411 HW8 Notes
[[ECON411 HW7 Notes]] [[ECON411 HW8]]
#school
* Referenced Concepts:
	* [[Granger Causality Test]] [[Time Series Data]]
[file:2FEEEA9D-D3BD-4E9D-BD6F-09301D56323C-338-0000046007D0436F/ECON411_F20_HW8.pdf]

Assume stationary data!

## 1. 
## 2. 
Want data to be stationary! Looking for lack of a time-based trend
Lots of “memory” in the data - prior points influence later points, so not stationary! 
Can try computing correlation between Yt and Yt-1! High = non-stationary?
	Can also be looking at Yt and Yt-2, will still be high but smaller.
If there’s a lot of “memory”, would expect to see a smooth relationship as you move to older lags!
 
“Univariate series analysis” 
Looking for correlations among “levels”
 

*Get first difference space:*
`d(var)`

Group statistics

mean: 4899.221
SD: 1781.112


### Restricting time sample: 

### Descriptive Statistics 


## 3.  
Spurious regression!

## 4.  
First difference space!
Lags by 1, decreases sample by 1
 
What is Et? 0, positive or negative. What are the effects of this?
 
As t value increases, variance will explode! “Exploding variance”
This means that we can use first difference to avoid issues! 

`deltaYt-1 = Et` and var deltaYt-1 is now just sigma-squared! 
We assume we have a unit root! For the test! Implies that the coefficient for Yt-1 = 1!!!!
We must test for this!! 
If we have a unit root, would expect the coefficient for Yt-1 = 0! 
(Must be less than 0 because rho bounded by 0 and 1)

*Will go through formal testing in Thursday!* 

## 5. 
## 6. 
## 7. 
## 8.

<!-- {BearID:4392F296-FB2F-483E-8438-382E2B55DD07-87251-000573803A367C23} -->
# ECON411 HW8
#school #school/Economics 
[[ECON411 HW8 Notes]]

<!-- {BearID:6D2104E9-6166-48E6-8989-C256AE8AAF7F-338-000004120DB001EF} -->
# ECON411 HW9
#school

[[Hausman Test]]

# Part 1
*Gasoline Demand Function:*
`lnY_it = B_1 + B_2 lnX_2it + B3 lnX_3it + B4 lnX_4it + u_it`
Where :
	* Y = gasoline consumption per car
	* X_2 = real income per capita
	* X_3 = real gasoline price
	* X_4 number of cars per capita
	* i = country code (in all 18 OECD countries)
	* t = time (Annual obs 1960-1978)

## a) Pooled OLS Model
We can estimate the above gasoline demand function using a simple OLS pooling model. This means all countries are considered to be homogeneous, with no unique or distinct individual characteristics. While obviously flawed in some ways, this model is good because of it’s simplicity and lack of excess explanatory or dummy variables, which decrease degrees of freedom in the other two models below.

We can estimate the above model in Stata using the following command:
`reg lny lnx2 lnx3 lnx4`
Giving us the following output:
```
*. reg lny lnx2 lnx3 lnx4
      Source |       SS           df       MS      Number of obs   =       342
-------------+----------------------------------   F(3, 338)       =    664.00
       Model |  87.8386024         3  29.2795341   Prob > F        =    0.0000
    Residual |  14.9043581       338  .044095734   R-squared       =    0.8549
-------------+----------------------------------   Adj R-squared   =    0.8536
       Total |  102.742961       341  .301299005   Root MSE        =    .20999
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .8899616   .0358058    24.86   0.000     .8195313    .9603919
        lnx3 |  -.8917979   .0303147   -29.42   0.000    -.9514272   -.8321685
        lnx4 |  -.7633727   .0186083   -41.02   0.000    -.7999754   -.7267701
       _cons |   2.391326   .1169343    20.45   0.000     2.161315    2.621336
------------------------------------------------------------------------------
```
 
## b) Fixed Effects Model
Another way we can estimate the above gasoline demand function is using the fixed effects approach. There are two main ways we can do this; first, we can incorporate cross section dummies (optionally including all n dummies but suppressing the constant, or using n-1 dummies and having a constant) and we can further add time section dummies to capture discrete time effects. For the purposes of this question, we will use the first method, incorporating only cross-section dummies. These dummies will help capture the heterogeneity between countries. 

While we can have stata fully estimate and test our FE model for us, we will use the bootleg method for this example to demonstrate how the model works.

We start by generating dummies for each country using the command:
`tab ctry_code, gen(ctry_code_)`
Since we have 18 OECD countries in our sample, we know that this command will generate 18 unique dummies, each set to 1 for observations of a specific country, and 0 otherwise. The coefficients for these dummies will be used to capture the variation in our log of gasoline consumption per car.

We can now run the FE model via bootleg using the following command:
`reg lny lnx2 lnx3 lnx4 ctry_code_1-ctry_code_18, noconstant`
Note the important addition of noconstant at the end, since we are including all 18 country dummies, and therefore no longer need (or can utilize in any way) a benchmark constant that is not associated with any countries. The reported regression results for this model are:
```
      Source |       SS           df       MS      Number of obs   =       342
-------------+----------------------------------   F(21, 321)      =  35819.69
       Model |  6412.53833        21  305.358968   Prob > F        =    0.0000
    Residual |  2.73649024       321  .008524892   R-squared       =    0.9996
-------------+----------------------------------   Adj R-squared   =    0.9995
       Total |  6415.27482       342  18.7581135   Root MSE        =    .09233
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .6622498    .073386     9.02   0.000     .5178715    .8066282
        lnx3 |  -.3217025   .0440992    -7.29   0.000    -.4084626   -.2349424
        lnx4 |  -.6404829   .0296788   -21.58   0.000    -.6988726   -.5820933
 ctry_code_1 |   2.285856   .2283235    10.01   0.000     1.836657    2.735056
 ctry_code_2 |   2.165552   .2128985    10.17   0.000     1.746699    2.584405
 ctry_code_3 |   3.041841    .218635    13.91   0.000     2.611702    3.471979
 ctry_code_4 |   2.389457   .2080863    11.48   0.000     1.980072    2.798842
 ctry_code_5 |   2.204772   .2164698    10.19   0.000     1.778893    2.630651
 ctry_code_6 |   2.149869   .2178844     9.87   0.000     1.721207    2.578531
 ctry_code_7 |    2.33711   .2148847    10.88   0.000      1.91435    2.759871
 ctry_code_8 |   2.592326   .2436864    10.64   0.000     2.112902     3.07175
 ctry_code_9 |   2.232549   .2395407     9.32   0.000     1.761281    2.703817
ctry_code_10 |   2.375928   .2118356    11.22   0.000     1.959167     2.79269
ctry_code_11 |   2.234792   .2141724    10.43   0.000     1.813433    2.656151
ctry_code_12 |   2.216701    .203042    10.92   0.000      1.81724    2.616162
ctry_code_13 |   1.681778   .1624636    10.35   0.000      1.36215    2.001406
ctry_code_14 |   3.026343   .3945113     7.67   0.000     2.250189    3.802498
ctry_code_15 |   2.402503   .2290928    10.49   0.000     1.951791    2.853216
ctry_code_16 |   2.509989   .2356565    10.65   0.000     2.046363    2.973615
ctry_code_17 |   2.345448    .227284    10.32   0.000     1.898294    2.792603
ctry_code_18 |   3.055251   .2195996    13.91   0.000     2.623215    3.487288
------------------------------------------------------------------------------
```

## c) Random Effects Model
Another approach we can use to estimate the above gasoline demand function is the random effects model.  This model can be useful for capturing the unobserved heterogeneity between variables, so long as it is uncorrelated with any observed variables (see corr(u_i, X) = 0 (assumed)). We can estimate the RE model for gasoline demand using the stata command:
`xtreg lny lnx2 lnx3 lnx4, re`
Giving us the following output:
```
Random-effects GLS regression                   Number of obs     =        342
Group variable: ctry_code                       Number of groups  =         18
R-sq:                                           Obs per group:
     within  = 0.8363                                         min =         19
     between = 0.7099                                         avg =       19.0
     overall = 0.7309                                         max =         19
                                                Wald chi2(3)      =    1642.20
corr(u_i, X)   = 0 (assumed)                    Prob > chi2       =     0.0000
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .5549858   .0591282     9.39   0.000     .4390967    .6708749
        lnx3 |  -.4203893   .0399781   -10.52   0.000     -.498745   -.3420336
        lnx4 |  -.6068402    .025515   -23.78   0.000    -.6568487   -.5568316
       _cons |   1.996699    .184326    10.83   0.000     1.635427    2.357971
-------------+----------------------------------------------------------------
     sigma_u |  .19554468
     sigma_e |  .09233034
         rho |  .81769856   (fraction of variance due to u_i)
------------------------------------------------------------------------------
```


## d) Theoretical Expectations
	* *lnx2:* the coefficient for lnx2 represents the real income per capita. We can reasonable theorize that we would expect gasoline consumption to increase as real income per capita increases, since people with higher income might be more likely to drive further distances, irrespective of the cost of fuel. We thus expect a positive coefficient.
	* *lnx3:* The real gasoline price captured in our variable lnx3 can reasonably be expected to have a negative impact on gasoline consumption. We expect this since higher costs for gasoline would force people to substitute to other transportation methods or fuel methods generally. We thus expect a negative coefficient.
	* *lnx4:* This variable captures the number of cars per capita. A complicating factor for this variable is the fact that our dependent variable, lny, is in fact already measured as gasoline consumption *per car.* This means that that value already controls for the number of cars in circulation. Thus, this variable in fact controls for the fact that people who have more cars are likely to do less driving per each car, therefore gasoline consumption would decrease as there are more cars per person, so this coefficient can be expected to be negative.
	* *Country dummies:* These variables are added in our FE model above. They represent the unique constant level of gasoline consumption for each country. Theoretically, we would expect these to all be positive, although they are not hugely useful on an individual basis, and rather serve to improve our estimates for the more important variables identified above and remove some of the excess variation that might result from country heterogeneity.

## e) Model Interpretation
#### Pooled OLS:
This model assumes no heterogeneity among countries, in effect claiming that all differences between country estimates and actual values are either due to explanatory variables or part of the random error term.

Our reported regression results above allow us to draw seemingly statistically significant conclusions about our variables.
* *lnx2:* We can interpret this coefficient as reported above to mean that for every one percent change in real income per capita we would expect to see a 0.89% change in gasoline consumption per car.
* *lnx3:* We can interpret this coefficient to mean that for every one percent increase in the real gasoline price, we would expect to see a 0.89% decrease in gasoline consumption per car.
* *lnx4:* We can interpret this coefficient to mean that for every additional 1% of cars per capita, gasoline consumption per car falls by .76%.

#### Fixed Effects Model:
The results reported from our FE regression generally have the same signs as from our pooled OLS regression, with a positive coefficient for lnx2 and negative coefficients for lnx3 and lnx4. Interestingly and mostly in line with our expectations, the exact values are somewhat different. 
* *lnx2:* Our lnx2 coefficient is now .66, which we can interpret to mean that for every 1% increase in real income per capita, we would expect a .66% percent increase in gas consumption per car, noticeably less than our earlier estimate of .89%. 
* *lnx3:* Our lnx3 coefficient is now -.32, which we can interpret to mean that for every 1% increase in the real gasoline price, we would expect gasoline consumption to drop by .32% per car. This is much lower than our earlier estimated value of -0.89%. 
* *lnx4:* Finally, our predicted coefficient for lnx4 is now -0.64, which we can interpret to mean that for every 1% increase in the number of cars per capita, we would expect gasoline consumption to drop by 0.64% per car.

The big difference between our old and new results that led to these differences is the inclusion of a number of dummies representing each individual country, to capture country-specific effects. We can notice from the reported coefficients that the values are meaningfully different, although with significant similarities between certain countries (as we would expect). We also notice that our earlier coefficient from the pooled OLS model of 2.39 is the average of the newly reported country dummies. From the results, it appears that most countries are still fairly close to where they were with the homogeneous constant, however several countries in particular seem to benefit from the inclusion of the fixed effect dummies in particular as they are statistically different from our initial coefficient. 

One of the only concerning elements of this FE model is the potentially overinflated significance values. The overall significance F-statistic is huge at almost 36,000 and R-squared & adj-R-squared are both over .999. This appears unlikely in reality, so will need further examination when we select which model to use.

#### Random Effects Model
Our RE model again gives us the same signs as our two prior models, however the coefficients themselves are again noticeably different. 

* *lnx2:* The coefficient for lnx2 is now .55, which we can interpret to mean that for every 1% increase in the real income per capita in a country, we would expect gasoline consumption to grow by 0.55% per car. This is less than both other models, which estimated .89% and .66% for OLS and FE respectively.
* *lnx3:* The coefficient for lnx3 of -0.42 can be interpreted to mean that for every 1% increase in the price of gasoline, we would expect gasoline consumption in a given country to fall by 0.42% per car. This is roughly in the middle of the values predicted by the earlier two models of -0.32% and -0.89% for FE and OLS respectively.
* *lnx4:* Our reported coefficient for lnx4 is -0.61, which we can interpret to mean that for every 1% increase in the number of cars per capita, we would expect to see a 0.61% drop in gasoline consumption per car. 

## f) Model Choice
First, we start by testing whether we would prefer our Straight pooling OLS or FE model. To do this, we can first run a special f-test with the following hypotheses:
`H0: ctry_code_1=ctry_code_2=ctry_code_3=ctry_code_4=ctry_code_5=ctry_code_6=ctry_code_7=ctry_code_8= ctry_code_9=ctry_code_10=ctry_code_11=ctry_code_12=ctry_code_13=ctry_code_14=ctry_code_15=ctry_code_16=ctry_code_17=ctry_code_18`
`H1: H0 false`
Running the above test in stata, we get a reported F-statistic of 83.96, allowing us to easily reject the null that there is no difference among country dummies. We can thus conclude from this test that our country dummies are significant overall. 

Now that we have concluded that our FE dummies are statistically significant, we must decide between the RE and FE models. We can do this using a Hausman test. For this test our hypotheses will be:
`H0: difference in coefficients not systematic`
`H1: H0 false`
Under these hypotheses, we carry out the test by setting up in a similar way to the messy f-test. The test reports a Chi2 statistic of 302.8, which allows us to easily reject our null that the difference in the coefficients is not systematic (p=0.0000), meaning we choose our FE model over our RE model.

Although we haven’t compared the RE model to our straight pooling, we know that our FE model is preferable to straight pooling, and that our FE is preferable to RE, so we can select FE as the best model to use for this data.

# Part 2
Now that we have selected to use the FE model as the best way of modeling the data, we can proceed to reestimate our original equation and correct for the presence of any heteroskedasticity and autocorrelation. We do this by running the following FE regression using robust standard errors:
`xtreg lny lnx2 lnx3 lnx4, fe cluster(ctry_code)`
Giving us the following output:
```
Fixed-effects (within) regression               Number of obs     =        342
Group variable: ctry_code                       Number of groups  =         18
R-sq:                                           Obs per group:
     within  = 0.8396                                         min =         19
     between = 0.5755                                         avg =       19.0
     overall = 0.6150                                         max =         19
                                                F(3,17)           =      15.92
corr(u_i, Xb)  = -0.2468                        Prob > F          =     0.0000
                             (Std. Err. adjusted for 18 clusters in ctry_code)
------------------------------------------------------------------------------
             |               Robust
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .6622498   .1584215     4.18   0.001     .3280097    .9964899
        lnx3 |  -.3217025   .1263773    -2.55   0.021    -.5883353   -.0550697
        lnx4 |  -.6404829   .0998961    -6.41   0.000    -.8512453   -.4297205
       _cons |    2.40267   .5976015     4.02   0.001     1.141841    3.663499
-------------+----------------------------------------------------------------
     sigma_u |  .34841289
     sigma_e |  .09233034
         rho |  .93438173   (fraction of variance due to u_i)
------------------------------------------------------------------------------
```
The newly reported coefficients all have the same signs, as with all prior cases, and as expected the coefficients are the same as with our prior model. We are now however using robust standard errors that allow us to more critically evaluate individual and overall significance for the model.

### Coefficients
* *lnx2:* the reported coefficient of 0.66 can be interpreted to mean that for every additional 1% of real income per capita in a given country, we would expect a 0.66% increase in gasoline consumption per car. 
* *lnx3:* the reported coefficient of -0.32 can be interpreted to mean that for every 1% increase in the real price of gasoline, we would expect a .32% drop in consumption of gasoline per car.
* *lnx4:* the reported coefficient of -0.64 can be interpreted to mean that for every additional 1% of cars per capita, we would expect a proportional .64% drop in gasoline consumption per car.

### Significance
Our new results appear much more sensible than our earlier results, especially when considering significances. First off, on the overall level, our reported F-statistic for the overall significance of our coefficients is now high but reasonable at 15.92, allowing us to easily reject the null that all coefficients are not significant without being an absurd value (35,000+). Likewise, overall R-squared has dropped to 0.61, which is good and much more reasonable. It is interesting how in this case we prefer the model with lower reported significance values!

On an individual level, the results are all significance at the 5% confidence level when using the following hypotheses:
H0: coefficient = 0
H1: H0 false

Under these hypotheses we get reported t-statistics of 4.18 for lnx2, -2.55 for lnx3 and -6.41 for lnx4. All of these are higher than our required t-critical value of 1.96, allowing us to reject all null hypotheses and conclude that all coefficients are statistically significant at the 5% level.

Thus we appear to have fixed our spurious regression and any issues of autocorrelation or heteroskedasticity by using cluster/ robust standard errors.

<!-- {BearID:E578F96B-D0BC-4366-B1B3-EE0D35B881A7-16983-000103B0CC769841} -->
# ECON411 MIDTERM
#school
Michael Calvey

I have neither given nor received unauthorized aid on this exam. 
*Start Time:* 3:15
*End Time:* 4:50

# 1. Derive OLS by hand
Since we only have observations of Y, our dependent variable, the best we can do is take the mean of values and use that as our predictor.

Thus we have:
```
Bhat0 = (1 + 3 + 5 + 7 + 9)/5 = 5
```

Seeing as this is our mean value and we have no independent variables, This will provide us our most accurate estimate and can thus be viewed as an intercept (since without a slope, this value will be used to predict all Y-hat). Part of this outcome is due to our centroid condition holding, since our classical assumptions hold.

### Testing significance
To test the significance of this estimate, we must first compute the standard error of b-hat. 
*SE(bhat) = 3.16*
Using this and our coefficient estimate from above, we can conduct a t-test of individual significance:
H0: coefficient = 0
H1: H0 false

T-observed = (bhat-H0)/SE(bhat) = (5-0)/3.16 = 5/3.16 = 1.58
Our T-critical at 95% confidence is 1.96, so we cannot reject our null hypothesis that the coefficient is equal to 0 since our t-observed is smaller than our t-critical value.

Furthermore, we can also calculate an R^2 value to test the overall significance of the model. We know R^2 is equivalent to ExSS/TSS, with ExSS = BhatX’Y - nYbar^2. = 65.
(Did not have time to calculate TSS, but for R^2 I would have simply divided the two values to get the answer.)

# 2. 
We can conduct a Chow/Wald special F-test in order to test the joint null hypothesis that the regression model goes through the origin and that the slope of the regression = 1. 
Our null hypotheses will thus be:

H0: (bhat0 + 1) = (bhat1) = 1
H1: H0 false

We can then use the formula for the Chow/Wald F-test, using the correct number of degrees of freedom in the numerator and denominator to arrive at our F-statistic. We can then look up an F-critical value in an F-table (again using the appropriate number of degrees of freedom and two restrictions).

We can then say that if our F-observed value has a larger magnitude than our F-critical value, we can reject the null hypothesis that bhat0 + 1  = bhat1 = 1.

# 3. 
## a) Interpret B0
We can interpret the reported B0 coefficient to mean that every year, countries are expected to see a .56% drop in adjusted GDP per capita, all else equal. The reported t-ratio for this coefficient is -1.797, which is slightly below our 5% t-critical boundary of 1.96, as reflected in the p statistic that states that using the estimated coefficient implies a 7.3% chance of committing type 1 error.
## b) Overall Model Significance
We can use the F test to test the overall significance of the model with the hypotheses:
H0: model is not significant overall
H1: H0 false
The reported F-statistic of 13.55 gives us a p-value of 0.000, suggesting a minuscule change of committing type one error if we accept this model as valid overall. This means that the overall significance of the model is very high. 

From this F-statistic, we can also calculate an R^2 value using the formula (since F-statistics are really just a ratio of R^2 values taking into account degrees of freedom). The resulting R^2 value is .05592, suggesting that roughly 5.6% of the variation in the dependent variable is as a result of our model. This is not a very high R^2, giving us some reason to be suspicious of the model overall.

## c) Do bureaucracies fare better than democracies?
We can test this hypothesis using the following null and alternatives:
H0: bureaucracies are not better than democracies -> B4 >= B5 -> B4 - B5 >= 0
H1: bureaucracies are better than democracies -> B4 < B5 -> B4 - B5 < 0


(As I cannot remember the formula for this kind of test, I will instead calculate the 95% confidence intervals, and then show that if they do not overlap and our bureaucracies have a higher coefficient than democracies (they do), we can reasonably conclude that bureaucracies fare statistically better than democracies)

SE(B4) = B4 / t-observed(B4) = 1.277/4.838 = 0.264
SE(B5) = B5 / t-observed(B5) = 1.965/4.443 = 0.442

B4 95% confidence interval = B4 +- Z * SE(B4)/sqrt(920) = [1.260 	1.294]
B5 95% confidence interval = B4 +- Z * SE(B5)/sqrt(920) = [1.936 	1.995]

Thus since we have that the lower bound of B5’s confidence interval does not intersect the upper bound of B4s confidence interval, and the coefficient of B5 is bigger than the coefficient of B4, we can conclude that bureaucracies do perform statistically better than democracies.

# 4. 
## a) When would we use OLS?
The concern embedded in the model of eq1. Is that the variable hours is endogenous and will vary with wages and number of kids. This issue of endogeneity can be difficult to fix and result in the loss of significant degrees of freedom (and some useful statistical tools like F-statistics), so if possible it is preferable to use OLS, since this gives the most efficient estimates (BLUE), but only if the coefficients are unbiased.

Therefore, we can conclude that we would prefer OLS over a variant like 2SLS (implemented below) ONLY if we know that there is no endogeneity between our variables. Several ways to test for this would be to try implementing a 2SLS model, then comparing the output of that via a Hausman test to see whether hours is in fact endogenous. Another way would be to look for high degrees of multicollinearity between variables. If we deduce that there is in fact no endogeneity, we can use OLS and expect to get unbiased estimates.

## b) How else can we estimate equation 1?
Perhaps the best alternative way to approach estimating equation 1 would be to use IVs or Instrumental Variables to remove endogeneity from our model. The difficulty with this is that each additional variable added to the model increases degrees of freedom and thus decreases accuracy, so we must be very judicious about adding additional IVs (one per endogenous term is OK), and then test our resulting model using the Sargan Chi2 to ensure we are not adding too many IVs. 

To actually estimate the equation, we would need to first estimate a reduced-form equation for hours, then as part of a 2SLS regression use resulting predicted values for hours in our original structural equation lnwage. 

We would then use a test like the aforementioned Sargan `estat overid` to test for over identification. 

### Bonus:
Hours first-stage equation:
`HOURSi = a0 + a1LNWAGEi + a2KIDSi + Ei`

<!-- {BearID:974C6C97-AA75-4777-B96A-178ADB5B98EC-37181-00027768B3EEE851} -->
[[006 School MOC]] [[063 Economics TOC]]
# ECON431 - Economics of the EU
#school/class 

[[ECON431 Policy Memo]]

[[ECON431 Presentation2]]

[[ECON431 Final Paper]]:
	[[ECON431 Final Paper Data - Methodology]]
	[[ECON431 Final Paper Contribution - Preliminary Results]]

[[ECON431 HW9 Notes]][[ECON431 Economics of the EU]]
# ECON431 Final Paper Contribution & Preliminary Results
#school
Part of [[ECON431 Final Paper]]

Previous: [[ECON431 Final Paper Data & Methodology]]

## Results
The core result of this paper will be a distillation of the disparate sides in the ongoing conflict surrounding gas export rights to the EU, and the complex economy that exists to support Europe’s gas needs. The initial conclusion I intend to draw is that there are enough forces at play simultaneously that it in fact becomes impossible to make high-level, one-sided statements about the situation with any real accuracy, and that the viewpoint that the US is trying to disseminate cannot possibly be the sole truth in the situation. From there, I will expand on this by adding that in fact the situation is not as Russia and Germany portray either, and in reality each nation is behaving exactly as we would expect in accordance with its national interest.

## Contribution
My contribution to the existing literature will seek to broadly contextualize the stances of all involved stakeholders and to clarify some of the nuances in the ongoing conflict. Specifically, I want to focus my assessment on the idea that there is a lot more common ground among all parties than perhaps they want to make evident, and that this common ground is a good starting point for conciliatory politics. If my analysis lends itself to suggesting practical ways to proceed with the situation, I will also offer (reasoned) advice for any high-ranking politicians or legislators who may happen to read my text, or perhaps other academics wishing to start from a text that broadly discusses all stakeholders in the conflict. I think forcing decision makers to be more clear about the true motives underlying their political maneuverings is a critical first step.

<!-- {BearID:14E12211-4A07-4006-A401-03FFBB886F77-16983-00008A5867ED1ED7} -->
# ECON431 Final Paper Data & Methodology
<!-- #school/Economics/EC431 -->
[[ECON431 Final Paper]] [[005 Active MOC]]

# Money, sovereignty and natural gas: how Nordstream 2 divided the EU
I intend to conduct a deep examination of the numerous factors contributing to all of the disagreement surrounding Nordstream 2. I think the best way to approach this is to break the paper up by the following viewpoints, in this order:
## EU as a whole 
In this first section, I want to outline the context around EU gas consumption and international dependence. I want to highlight the fact that the EU is hugely dependent on gas imports from Russia, the Middle East and recently (since the shale boom) the US. 
Gross natural gas consumption mildly on the rise since 2008
 
However natural gas imports appear to have trended up more strongly 
Context on energy consumption by source for EU countries
 
This final chart shows how extremely reliant the EU is on imports to meet most of its energy needs. An increasing proportion of natural gas is imported, with roughly 85% being imported currently.

## Germany
As the last graph in the EU section shows, Germany is clearly the party with the most at stake, given their large absolute level of gas consumption and growing reliance on natural gas. 
Germany is the biggest stakeholder! They stand to lose the most when gas supplies are shut off or at risk. 
Interesting note that Germany not only has by far the highest level of imports, but also the highest level of exports of natural gas - making them a seemingly perfect candidate to become a new transit country


## The Ukraine - difficult partner 
This chart, and its underlying data, clearly show how important the Ukraine is to the EU’s energy security overall. 

In my discussion of Ukraine as a transit country, I am going to reference more of the sources I mentioned in my annotated bibliography, especially the Sauvageot (2020) article on the role of transit countries on EU energy dependence considerations. It appears that in most of their deliberations, EU leaders did not put enough emphasis on the risks embedded in transit countries, focusing instead on source countries. In practice, this means they may have ascribed too much emphasis on EU-Russian relations, and not enough on transit countries themselves

The ostensible problem with relying on the Ukraine as a core transit country is that it is also a major consumer of Gazprom’s natural gas. In practice, this has meant that when there are disputes in pricing between Gazprom and the Ukraine (When Ukrainian officials were unwilling to pay the market rate for gas) and Gazprom inevitably cut off supplies, Ukraine is only left with two choices: go without gas, or siphon transit gas from EU nations. In the crises in 2009, 2014 etc, they consistently opted to siphon EU gas. This obviously creates huge energy security risks for EU countries like Germany, which rely heavily on gas imports as shown above. 

The big question that I now begin to approach is /where/ this risk really comes from; Ukraine, Russia, or is it an inevitable outcome of the geopolitical and energy landscape of the region?

## Russia - maligned actor or honest partner?
This brings us to Russia. Although Russia is ostensibly at the center of this issue, my argument is that in fact Russia itself is really of periphery concern to the question at hand. Rather, a combination of overly-aggressive EU policy over the last two decades emphasizing the alleged value of LNG as a cleaner energy source paired with economic concerns dominate the current narrative. 

It is important to remember that natural gas exports are vital to the Russian economy, and that the European market is Russia’s main customer. This clearly means that the interdependence is not one-sided, although there is clearly significant political leverage that comes with being the main gas supplier for the EU.

## The US - why are they even involved?
Here I am going to talk about how the most vocal opponent to Nordstream 2 appears to be the US. For this, I’m going to reference the numerous US sanctions imposed on European companies and individuals connected with Nordstream 2. 
[EU countries protest US sanctions in warning to Washington – POLITICO](https://www.politico.eu/article/eu-countries-protest-us-sanctions-say-german-officials/)

After this, I will discuss what the US stands to gain from halting Nordstream 2 - namely a much more receptive market for US shale gas. The US government has acceded that it wants the US to become Europe’s main LNG exporter, although it continues to claim that Nordstream is “negative for EU energy security”
[US expands sanctions against Putin’s pipeline - Atlantic Council](https://www.atlanticcouncil.org/blogs/ukrainealert/us-expands-sanctions-against-putins-pipeline/)

# My point
Expand on this in [[
Once I lay out the context of the involved stakeholders, I will endeavor to make the point that despite all the ongoing rhetoric, the controversies surrounding Nordstream 2 stem from primarily economic concerns. The stakeholders above in my opinion roughly shape into two broad categories:
* Pro-Ukraine: Ukraine, US, some of Europe
* Pro-Germany: Germany, Russia
Thus really this is a fight for which group of countries stands to benefit most from the EU’s high level of gas imports (or more correctly the EU’s falling LNG output as compared with its LNG demand)

# New Data Sources
- [Energy statistics - an overview - Statistics Explained](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Energy_statistics_-_an_overview)
	- This page has a huge amount of useful data on EU energy statistics. Lots of details on individual country consumption, imports and exports included.
- [Natural gas supply statistics - Statistics Explained](https://ec.europa.eu/eurostat/statistics-explained/index.php/Natural_gas_supply_statistics)
	- A more specific set of data and resources surrounding natural gas in Europe. More relevant to my project than the energy statistics source above, however both provide useful context.

<!-- {BearID:C522C9E5-ED23-432F-A177-2BE55AED3882-2136-000047075805542E} -->
[[ECON431 Economics of the EU]]
# ECON431 Final Paper
#school
*PRESENTING TUES, NOV10!!!*
[[005 Active MOC]]
### Next Deadlines:
[[Paper Research Question]] - OCT 14
[[ECON431 Final Paper Data & Methodology]] - OCT 28!!! *FRIDAY!!!*
		- How am I gonna answer the question!!??
		- Show change over time!
[[ECON431 Final Paper Contribution & Preliminary Results]]
Contribution & Preliminary results - NOV 7

Go/econpapers
Go/econlit
Go/Ryan

Nordstream - looking at time line of issue and projecting the costs of political interference?
[U.S. Broadens Sanctions to Thwart Completion of Russian Gas Pipeline - WSJ](https://www.wsj.com/articles/u-s-broadens-sanctions-to-thwart-completion-of-russian-gas-pipeline-11603193032?st=t8mxzuprom7cg8i&reflink=article_email_share)

https://www.wsj.com/articles/coal-finds-a-surprising-2020-bright-spot-in-europe-11602763826?st=z5nrkcollp7nq2g&reflink=article_copyURL_share

[Germany Says Further U.S. Sanctions Over Nord Stream 2 Would Interfere With EU Energy Security](https://www.rferl.org/a/germany-says-further-u-s-sanctions-over-nord-stream-2-would-interfere-with-eu-energy-security/30670340.html)

<!-- {BearID:87398705-A5B2-4CD7-A191-68A9DE8BEAC7-19669-0001570DE5CAF190} -->
# ECON431 HW9 Notes
#school
[[Deprecated]] 

<!-- {BearID:71FE89D5-D3FC-4075-944E-177FF5AB7444-16983-00009A64DDC8E64C} -->
# ECON431 Policy Memo
<!-- #school/Economics/EC431 -->
<!-- #Important #ongoing -->
[[005 Active MOC]]

<!-- {BearID:CD37BADD-CD37-4DF3-AA97-99E26287A8C7-37181-000391470AADFA98} -->
# ECON431 Presentation2
<!-- #school/Economics/EC431 -->

[[005 Active MOC]]
[[Econ of EU Presentation 1]]

# The dangers of economic populism
Newsroom = 25 minutes, have rest of 75
1. Populism and the economics of globalization
2. Economic drivers of populism
3. Is populism necessarily bad economics?
	1. If your surname starts with A - F,  make an argument for why populism is good economics. All other names, argue for why populism rather results in negative economic outcomes.

### Need list of names for breakout rooms!
Also plan polls!


*People:*
Ashe
Bosco
Byrne
Cohen
Coslick
Domingo
Dowling

Flanagan<——————— Halfway

Heinritz 
Hudson
Lewis 
Rowan
Shapiro
Shaw 
Tarantino


/25 minutes newsroom/

/Presentation 1 Part 1 - 5.5 mins/
/Pair #1 - 2 minutes/
/Breakout - 6 minutes/
/Big group -  3 minutes/

Disco 1:
Everyone thinks something is wrong
Left vs right - quite different
Both come from disenfranchisement
Evolution of rationale behind support for National front - Adrienne
	_ starts as protest/ rejection
	_ recently platforms and ideologies have garnered support




/Presentation 1  Part 2 - 5.5 mins/
/Pair #2 - 2 minutes/
/Breakout - 6 minutes/
/Big group - 3 minutes/

/Presentation 2 - summary, 1 slide and head the slide with the paper name - .75 minutes/
		* Start by acknowledging existence of paper 2!! Say it could inform our decisions on whether we think populism is necessarily good or bad
		* 
/Presentation 3 - 3.5 minutes/
/Pair #3 - 2 minutes/
/Breakout - 6 minutes/
/Big group - 4 minutes/


2:05-2:21 section 1
2:21-2:38 section 2
2:38-2:55 Mike

<!-- {BearID:FC5FDC8B-199E-432E-BDB9-64B37C22AFF4-37181-0003A072874AC72A} -->
[[050 Concepts MOC]] | [[052 Physics TOC]] | [[Quantum Mechanics TOC]]
# EPR Paradox
#concepts #timeline/1900s/1935

The EPR paradox was formulated in the famous 1935 paper by Einstein, Podolsy and Rosen, and it shows that there are inconsistencies in the classical explanation of quantum mechanics. The paradox primarily refers to what we now know as [[Entanglement]], whereby information could seemingly be transmitted instantly, faster than the speed of light. 

Einstein called this phenomenon "spooky action at a distance," quite a funny name. I should try to read the original paper (in english).

The purpose of the paper was to expose the incompleteness of the [[Copenhagen Interpretation of Quantum Mechanics]].# Econ Major Plan
#school
[[ECON431 Economics of the EU]]
[[ECON411 Applied Econometrics]]
[[ECON211 Regression]]

11 courses minimum

+ Econ 150 macro
+ Econ 155 micro

+ Econ 210 stats
+ Econ 211 regression

+ Econ 250 macro theory
- Econ 255 micro theory
- Econ 280 game theory

- Elective 1
- Elective 2
- Elective 3
- Elective 4

<!-- {BearID:06E3D0F8-15F5-41E1-84B4-E618FC817D78-299-0000117E219BDC7A} -->
# Econ of EU Presentation 1
<!-- #school/Economics/EC431 -->

My paper: Institutional integration and economic growth in Europe

1. My paper: institutional integration and economic growth in eu
	1. Crisis question
2. Tommy’s paper: measuring the effect of eu regional policy
	1. 75% question
		1. Should we consider flipping people’s roles from their discussion?
3. Isaac: impact of regional support on growth and convergence
	1. Question on drawbacks


Meeting: 3pm Monday


Need background data page
Summarize data
Improve findings/ results, clarify,

Isaac- Somehow tying into Tommy’s conclusions
Step1 step2 step3 slide - title
Maybe don’t say this was the least convincing?


# My slides:

Against 75 %
- Ryan: Whole country benefits and grows, whole thing will do better eventually
- Max: data shows that extra funds don’t really help convergence - would be more productive to deploy in important areas
- Gordon: inequality between regions still exists, makes it difficult to unite as a union if people are so strongly unequal
- Jess: Unfair, who decides - those who pay!
- Emma: Always improvements to make


People:
Ashe
Bosco
Byrne
Cohen
Coslick
Domingo
Dowling

Flanagan<——————— Halfway

Heinritz 
Hudson
Lewis 
Rowan
Shapiro
Shaw 
Tarantino

<!-- {BearID:A48B3F9A-7366-4735-B5D4-53B7712BFE95-1228-0000B2CD19C0ECA4} -->
[[011 Mental Models MOC]] | [[054 Neuroscience MOC]]
# Ego Depletion
#concepts/psychology [[010 Mind MOC]]
# Einstein Filtering Method
#concepts 

This is a powerful method for learning concepts espoused by [[Albert Einstein]], [[Charlie Munger]] and others. The core premise is to focus on simplifying things as much as possible without losing key information. In effect, this means sifting the most important information from non-essential information. Context of course matters for this.

This process of whittling down information, knowledge and wisdom is critical to all of life, this is just a distilled theory that focuses in on it. I really ought to think about this much more. 

Some initial thoughts on approaches:
- Focus on understanding basic, timeless, general principles. This is why I'm working so much on my [[011 Mental Models MOC]]. I should try to think about new concepts from the perspective of mental models. I need to think about a good way to keep them in my active memory and use as many as are applicable/ useful to a given situation.
- Take time to think about what the goal is ([[Goal Orientation]]), and consider the 2-3 most essential variables.
- Remove non-essential clutter
- Work backwards to get to the solution. Continuously break things down into components if complexity is a problem, ie go back to step 1. This is an application of [[Inversion]].

---
aliases: ["Voltage"]
---

[[052 Physics TOC]] | [[Energy]] | [[Electric Power]]
# Electric Charge - Voltage
#concepts 
Fundamentally, this is the difference in charge between two points, or potential electricity. 

**Measured in volts**

There are also electron volts, which refer specifically to the amount of energy for a single atom or molecule.

The voltage of an [[Electrostatic Force]] shock is between 40,000 and 100,000! Current is low though, because you don't pick up many [[Electrons]].


## Transformers
Transformers use the fact that wires with an oscillating [[Electric Current]] exert a magnetic force and can create a different charged current in another wire.[[052 Physics TOC]] [[Electricity]] [[Energy]]
# Electric Current
#concepts 

[[Electric Resistance]]
Electric current is when charged particles such as [[Electrons]] move. High currents create [[Magnetism]].

The Amp or Ampere is the unit of measurement of current, representing $6\times10^{18}$ electrons per second.

Light bulbs typically use about one amp, wires at home typically have about 15.

Electrons must be replaced by others for current to occur, otherwise charge builds up and flow stops.[[052 Physics TOC]] | [[Energy]] | [[Electricity]]
# Electric Power
#concepts 

`Power (watts) = volts x amps`
Volts - [[Electric Charge]]
Amps - [[Electric Current]]

(A 1-v battery delivering 1A will have power of 1watt)

Electric power delivered by electroins depends on the [[Energy]] of the [[Electrons]] and the [[Electric Current]], or number per second that arrive.[[052 Physics TOC]] [[Electricity]]
# Electric Resistance
#concepts 

The friction of the flow of [[Electrons]] in an [[Electric Current]].

Heat created is proportional only to [[Electric Current]], not to [[Electric Charge]]. Thus wires carrying the same [[Electric Power]] can be made more efficient by increasing the [[Electric Charge]] 

Materials with high resistance are insulators, those with low resistance are conductors. Semiconductors can be made to turn from conductors to insulators and back.---
aliases: ["Electric"]
---
[[052 Physics TOC]]
# Electricity
#concepts 
Electricity refers to the mass flow of [[Electrons]].

See [[Electric Power]] 
[[Electric Current]]
[[Electric Charge]]
[[Electric Resistance]]

[[Magnetism]][[052 Physics TOC]]
# Electromagnetism
#concepts 

With light waves, we have [[Electric Charge]] moving as transverse [[Waves]], creating electromagnetic radiation. Really a combination of self-reinforcing [[Electricity|Electric]] and [[Magnetism|Magnetic]] waves.

EM waves are created when charges move up and down a conductor.

See [[Light]][[Standard Model TOC]] [[052 Physics TOC]]
# Electrons
#concepts #🌱
[[Energy]]
Small subatomic particle, exists both as a particle and as a wave. Small negative charge. Most ordinary chemical reactions occur due to interactions by electrons. This makes [[Isotopes]] of different [[Elements]] react fairly similarly, since they have the same number of electrons.

Electron energy is measured in *electron Volts*, eV. 1eV is roughly the energy of an atom or molecule.
Saying a wire is at "1 volt" means every electron in the metal is at 1 volt

Electrons exert the [[Electrostatic Force]]
[[052 Physics TOC]] | [[Forces]] | [[Gravity, force and space]]
# Electrictrostatic Force
#concepts 

The electrostatic force is the energy contained in the charge of [[Protons]] and [[Electrons]]

It is $10^{42}$ times stronger than the force of [[Gravity]]! Dasbig. REALLY strong force on a relative basis!

Likes repel, opposites attract. 

Most objects have neutral charge bc [[Electrons]]

**Electric Current:** How much charge crosses a boundary over a certain period of time. Charge flows akin to water down a river - NEED A CHARGE GRADIENT.
**Voltage:** Like the amount of energy per charge.

Once electrons flow into an object, they begin to repel new electrons.

* **Batteries** - Use chemical reactions to build up charge on cathode/ anode. The particles want to flow but are held in place. Then a circuit connects the terminals and charge flows.

Resistance: when electron flow is impeded, as they hit ions on their path

Power = current * voltage, measured in (watts?)
voltage = current * resistance

Loss to heat during transmisison: power = current * current * resistance
- This is why making current low is a good way to lose less heat! More voltage, less heat for high-voltage power lines.
[[011 Mental Models MOC]] | [[054 Neuroscience MOC]]
# Embodied Cognition
#concepts/psychology #🌲 

The idea that cognition is inextricably linked with the body, and vice versa. We think through the experienced perspective of our senses and perception, which are physiological.

I wanna learn more about this.
[[Systems Theory TOC]]
# Emergence
#concepts/mental_models 
Emergence is one of my favorite concepts. I think it is incredibly powerful, and gives rise to a number of incredible phenomena and [[Institutions]] that I believe are more powerful and efficient than anything that can be created intentionally, at least for now. 

It may be impossible to truly understand emergence at the highest levels of complexity. I would almost liken ultracomplex emergence to [[Chaos Theory]] in that it can be unpredictable, at least to us with our limited aptitude for cognition and computing power. 

## Examples
- The biggest example, as argued by [[Friedrich Hayek]], is that [[Markets]] are in fact an emergent result of [[Biologically Innate Human Tendencies]]. This is a fundamentally powerful idea in explaining the value of markets, as well as why controlling them is so difficult. Since no individual created them, but they emerged gradually and spontaneously, they are impossible to fully understand or completely control.
- [[Cognitive Neuroscience]]: Our consciousness as well as our ability to think and reason are, in my view, prime examples of powerful emergence at work. Our brains are made up of relatively simple (on an individual basis) components that enable vastly powerful outcomes when put together. This is true emergence, where the whole is greater than the sum of the parts. 
- [[Neural Networks]] are an application of the ideas of emergence in human cognition to computer-based problem solving. They also demonstrate how simple components, in this case the [[Linear Algebra TOC|Linear]] [[Perceptrons]], can combine to create a powerful non-linear model capable of not only the [[XOR Problem]], but by extension also much more advanced problems and questions# Emerging Markets ETF Post-Mortem
<!-- #finance/xcap/Post-Mortem -->

<!-- {BearID:BF448DA4-E968-45CA-A9D0-6CB1C6E9CF70-406-0000023762F2EBA9} -->
[[011 Mental Models MOC]] | [[054 Biology MOC]]
# Energy Saving Brain
#concepts/biology #concepts/mental_models 

This is a core biological concept that underlies how people think and where [[Consciousness]] has its roots. Specifically, this theory states that the brain (and body) will always take the path of least resistance and try to use minimal energy when functioning. This is a key example of [[Biologically Innate Human Tendencies]] that results in [[Heuristics TOC]] that lead to [[Cognitive Biases TOC]].

This is super-closely linked with the idea of [[Cognitive Ease]], and can be seen as a core cause thereof.[[052 Physics TOC]]
# Energy
#concepts
A page for all my thoughts on energy generation, new ideas, techs, companies etc.


[[PHYS106 Physics for Educated Citizens]]

`power = energy / time`

`1 W = 1 watt = 1 Joule per second`

`1Wh = 1 Cal`

`1eV = 1.6x10^(-19) Joules, energy for a single atom`

See also: [[Electrons]], [[Protons]], [[Electric Current]]

*Kinetic Energy Equation* (v is m/s, m in kg)
`E = 1/2 mv^2`
	* Energy scales linearly with mass
	* Energy scales with square of velocity

Energy is the ability to move things or something that can be converted to heat

### First law of Thermodynamics
Conservation of energy! Energy is neither created nor destroyed, but altered from state to state.
## Measuring Energy
* One calorie of energy raises 1cm^3 of water temp by 1c
* 4200 Joules in a Calorie
* 1kWh = energy delivered when you get 1000 watts for 1 hr
	* 3.6 million Joules/ megajoules = 860 Calories
	* 1Wh (watt-hour) is roughly 1 Calorie (1000 calories)
* 1 watt = 1 Joule per second
* 1hp = 0.18 Calorie per second ~ 1 kW kilowatt
* Typical small house consumes 1kW
* Small car consumes 20kW
* Small town electricity 1MW
* 747 plane 45MW
* Typical large power plant produces 1GW
* USA 400GW

[[Electric Charge]]

| Item                    | Volts      |
| ----------------------- | ---------- |
| Electron                | 1v         |
| TNT energy per molecule | 1v         |
| Flashlight battery      | 1.5v       |
| US house voltage        | 110v       |
| EU house voltage        | 220v       |
| CRT TV                  | 50,000v    |
| Alpha Particle          | 1,000,000v |



## Forms of Energy
* Chemical
* Potential
* Kinetic
* Heat
* Gravitational
* Nuclear

## Energy Sources
Energy density and power (speed/ ease of reaction to release) matter for fuels. Chocolate chip cookies have more joules of energy than TNT per gram.
* 1 gram of battery has 500 Joules costs $4.00/kWh
* 1 gram of TNT has 2700 Joules (although it has lower energy than CCC, more power)
* Chocolate chip cookie has 21,000 per gram
* Coal has 27,000 Joules per gram. Costs 0.4c/kWh
* Gasoline has 42,000 Joules per gram. Costs 7c/kWh
* Methane has 54,000 Joules per gram. Costs 0.9c/kWh
* Hydrogen has 110,000 Joules per gram and has 2.6 times the energy of gasoline per gram, but is much less dense per cubic cm at 0.071 times the density 
	* Light and dense!	
	* Compared to gasoline, liquid hydrogen has roughly 3x more energy per gram but 3x less energy per liter. 1 Kilo of hydrogen is roughly 1 gallon of gas.
	* Can compress gas, but then tank gets heavy.
	* Liquid works well but must be kept super cold.
* Uranium-235 has 30 million times more energy than TNT per gram




#### Companies
* Nuscale - 6.5c / kWh
	* [Featured Topic Cost Competitive | NUCLEUS Spring 2020 | NuScale Power](https://www.nuscalepower.com/newsletter/nucleus-spring-2020/featured-topic-cost-competitive)
* Geothermal?!? - 3.5c / kWh!!!, could reach 2c/ kWh
	* Can get cheaper with time since only have to drill once! Only other cost is retooling conversion equipment, once every 30+ years
* Electric grid instability
* Intermittent generation =/  baseload power (renewables may not be continuous) 
* Fusion - might struggle to compete by the time it becomes viable commercially at a large scale, ex the 2050s. Renewables will likely be cheaper
* Net-zero carbon fuels - Prometheus [Prometheus launches at Y Combinator with plan to bring zero net carbon fuel to a pump near you - Prometheus Fuels](https://www.prometheusfuels.com/news/prometheus-launches-at-y-combinator-demo-day-with-plan-to-bring-zero-net-carbon-fuel-to-a-pump-near-you/)
---
aliases: ["Entangled"]
---
[[050 Concepts MOC]] | [[Quantum Mechanics TOC]]
# Entanglement
Entangled states can’t be described by $\ket{\psi}_1$ or $\ket{\psi}_2$ individually, must look at whole $\ket{\psi}$ to describe! [[CNOT Gates]] entangles [[Qubits]] - the gate flips the target qubit IFF the control qubit is $\ket{1}$, but there are lots of other ways to do it.

Entanglement is one of the key principles that separates [[Quantum Mechanics TOC]] from [[Classical Mechanics]]. The term was coined by Erwin Schrödinger in a response letter to the original 1935 Einstein-Podolsky-Rosen paper that demonstrated the [[EPR Paradox]]. 

## Definition
* A state $\ket{\psi}$ is ***product*** if $\ket{\psi}=\ket{\psi}_A\otimes\ket{\psi}_B$
* A state $\ket{\psi}$ is ***entangled*** if there *doesn’t* exist: $\ket{\psi}_1,\ket{\psi}_2$ s.t. $\ket{\psi}=\ket{\psi}_1\otimes\ket{\psi}_2$ 

	See [[Tensor Products]]


[[006 School MOC]]
# Equity Project Notes		
#school
Add Honor code

NOTES FROM PAPA
- Cost+ pricing formula- expensing costs immediately rather than amortizing, resulting in a lower fixed asset base than they would have otherwise.
- replacement cost of warehouse/ manufacturing space per sqm, considering fully-fitted out logistics and high-end state of the art manufacturing facilities.

HYPOS:
- Deep engineering expertise and a track record of overcoming extremely difficult technical challenges, highly likely software fix for 737 will ensue this year.
- Strategically viable supplier for the US government, protecting them from permanent damage from litigation
- Did they underinvest? capex ~ at depreciation, 2-3BnAnnual R&D similar to world-class companies

- Bring up pessimistic case- 10bn costs, absorbed and back in business by 2020. 
- 10 bn makes up 5%, share price decreased more, so overreaction. Given solid long term prospects due to passenger traffic growth being twice the level of GDP, this is a company that will be a beneficiary longterm, so this should be something to look at as a buying opportunity.

- For peer analysis, look at PEG ratio and FCF, as well as Ebitda multi and PE ratios.
- Liquidation/ book value: not relevant because of buybacks & assets are undervalued significantly, NEED SOURCES
- Need titles for all charts
- Get costs for litigation from Robert Luke?



CITATIONS
- reports - [Boeing Company - Investors - Overview](http://investors.boeing.com/investors/overview/default.aspx)
- net debt / ebitda
	- 3x or less = ok
- net debt = debt - 
- Peer valuations
	- Compare with Volkswagen and samsung scandals
	- Compare with other aerospace



## Approach:
- Valuation conclusion
- Introduce Boeing
- Break down their revenues
- Break down recent partnerships and their outcomes
- Discuss the sector at large
	- Talk about growing flight demand
	- Competition from Airbus, China, Russia, Japan
- Risk
- Quantitative valuation methods
	- DDM
	- DCF
	- SML analysis
	- Liquidation/ Book value
	- Peer based multiples
- Reconciliation




Look at:
Gross debt 14.8
cash 7.6
ND = 6.2  
operating cf = 16.3


- CF / debt balance sheet liquidity
	- Low debt to cash generation
	- 5.1bn in unused borrowing lines


- CF - 15.3Bn Op CF
- capex requirements- 1.7Bn PPE
- 13.6 FCF / 220bn mkt value
- 6% fcf yield 


- Forward PE 
	- Must look at forecast earnings to determine valuation
	- PE / growth rate = PEG
	- PEG < 1 really cheap
	- Peg ~1 or 2 is avg
	- Peg > 2 expensive
	- Use schwab and morningstars future earnings per share  
	- USE INDEPENDENT ANALYST EARNINGS FORECAST


Key valuation metrics for Boeing:
FCF yield
~6% is pretty good for a great world quality business
- Appreciate with inflation
- Sales growth etc
- Earnings will likely rise every year


After decades of core operations and PPE that has been consistently maintained with extensive CAPEX, most of these have been fully depreciated. The replacement cost of reacquiring the plants, equipment and engineering knowhow would be many times more than 12Bn dollars most likely, so if you revalued the balance sheet by the fair value  


Most likely their Operating cash flow will be able to cover any liabilities arising from the crashes 
- strong cash flow
- Little debt
- Lots of legacy portions
	- defence
	- most of commercial avia
	- GS
- 12.6bn dollars of PPE does not cover their 8mn sqm, working out at ~1500 / sqm for some of the highest tech manufacturing space in the world, making it likely undervalued  

It seems quite fairly valued, but not optimal for a deep long Boeing buy. P/E of 20 is high for industrials, even for a great one like Boeing.


## Peer valuation
Compare Boeing’s PE and PEG ratio with similar 




One of the traditional measures is ROE or ROC but not very useful now, since the current balance sheet significantly underestimates the current fair value of the PPE, patents, engineering knowhow etc. $1500 / sqm
so disregard D/E or ROE as not relevant


Management team with a proven focus on returns to shareholders. A great company like boeing with such a good history will likely work out their issues quickly, so they will be back to redistributing their FCF to shareholders soon. As such FCF yield is a better measure of profitability.

<!-- {BearID:35E1299E-548D-4193-A3B1-6DE12FB5F1AD-21279-000097686FA20F71} -->
# Equity Project Plan
#school
[[Intro to Finance Class]]

## Layout
-

<!-- {BearID:25967007-EE61-4275-969D-3508615FA8B9-21279-00004F86B79C829F} -->
[[001 Meta MOC]]
# Evergreen Notes
#concepts #🌱 #concepts/definition [[061 Philosophy TOC]]
# Existence

What does it mean to exist?[[011 Mental Models MOC|Mental Models]] | [[051 Math TOC]] | [[070 Finance MOC]] | [[052 Physics TOC]]
# Exponential Growth
#concepts #🌱 #concepts/mental_models 

Exponential growth is a type of chain reaction where the outputs can catalyze a certain reaction to repeat.

[[Nuclear Fusion]] & [[Nuclear Fission]]

Explains how the earth recovers quickly after [[Mass Extinction Events]]

Epidemics!

Compound Interest

The spread of culture!

Avalanches[[Quantum Computing TOC|Quantum Computing]] | [[Quantum Algorithms]]
# Factoring Algorithm
#concepts 

Factoring is a mathematical problem that desires to find all the factors for a given number. It is traditionally very computationally expensive, enabling [[Cryptography]].

In fact, factoring reduces to period finding.

## Period Finding
[[Period Finding Alrogithm]]
$\LARGE U_f\ket{x}\ket{y}=\ket{x}\ket{y+f(x)modw}$: a function with: 
* Period r
* N-dimensional standard basis state x

![[Pasted image 20210422103531.png]]
[[080 Personal Finance MOC]]
# Fair Price Residential LLC
#my/finances 
My real estate LLC.

## Bank
Quail Creek Bank - loan officer - Jeff Massad
AN: 1128472
access ID: fairprice
Address: 12201 N May Ave, Oklahoma City, OK 73120


### Kevin details:
R#: 103003636
A#: 4030202470
AN: Windhover LLC
BancFirst

## House 1
1160 SE 19th Terrace
Purchase price: $125,000
Monthly rent: $692
Annual rent: $8,304
Tax + insurance: $825
Maintenance: $1500
Annual exp NOI: $

[[011 Mental Models MOC]] | [[054 Neuroscience MOC]] | [[Heuristics and Biases]]
# Familiarity
#concepts/psychology 

Similar to the [[Availability Heuristic]]

# Fault Lines Plan
<!-- #school/intro-to-finance -->
[[Intro to Finance Class]]
#### DUE March 21 in class

## Details:
* 3-4 pages, 11 point, double spaced, standard 1” margins

## Ideas:
[[Paper Ideas]]

# Title
*The export-led model of growth in the post-crisis era: examining resurgent Bangladesh in light of Fault Lines ch2*

# Intro 
	- Since world war 2, the model of export driven growth emerged as what Rajan calls “the primary path out of poverty in the Postwar era”
	- He claims that when countries start down this path, they pave the way for significant later struggles since there is “no natural, smooth, and painless movement away from export dependence to becoming a balanced economy.”
	- The examples of Germany and Japan are very relevant to this conclusion, but potentially only represent the bad side of development and growth. This paper will examine whether Rajan’s claims hold as strongly almost a decade after the publication of the book, and more than a decade after the financial crisis began, by comparing his examples to that of Bangladesh, a country that on first pass mirrors many of Rajan’s qualifying criteria for a late-stage developer looking to exports to help fuel growth.
# P1: Exporting to grow- Late stage developers
	- The approach taken toward growth by the late stage developers can be summarized by “he process of strengthening organizations, in their view, required massive but careful government intervention. Infant firms had to be nurtured.”
	- “it is relatively easy to describe the typical path that successful countries have followed in the search for growth”, claims Rajan.
	- Very enticing growth opportunities created through globalization, large potential for profits.
	- Governments in more highly-authoritarian countries quickly embraced this

# P2: Exporting to grow- Unintended consequences & the export trap
	- Rajan posits that following an export-heavy model to achieve growth introduces systemic issues that only manifest themselves fully well after they have begun to develop.
	- The biggest issue is an underdeveloped domestic economy, and the resulting weak domestic demand that ensures “countries like Japan [are not only] unable to help the global economy recover from a slump, but they are themselves dependent on outside stimulus to pull them out of it.”
	- 
# P3: The case study of Bangladesh
	- On first reading, Bangladesh appears to be following the same path towards becoming an export-led nation.
	- This reading is actually not entirely correct
	- Exports make up a fraction of GDP small enough for Bangladesh to qualify as a
# Conclusion
	- Bangladesh, like the examples discussed by Rajan, does in many ways mirror the development model taken by many late-stage developers that rose the rankings of wealth. 
	- However, Bangladesh does not fully follow the path described by Rajan in ways that are very promising for the future outlook of Bengali growth. Despite this, Bangladesh does experience many of the factors that lead to a continued dependence on exports for growth, including detrimental government policies, vested interests and household habits formed during the years of catch-up growth and poverty.
	- The end result is the same. Rajan’s point in the chapter that this model, while not inherently detrimental, does create a difficult situation when it is employed frequently by creating trade surpluses and dependence that cannot easily be worked around. He argues that countries that start down the path of becoming exporting superpowers have a difficult time switching to domestic-demand fueled growth once their economies reach a certain scale, presenting many of the kinds of problems we can see now in countries like Japan.
	- Not only this, but he also mentions the impacts these countries have for the global economy as a whole. Export-heavy countries are unable to stimulate demand in the case of recession, and in fact require outside stimulus to recover from major downturns, since domestic output is most strongly correlated with foreign demand.

<!-- {BearID:02B57D06-1324-443E-8DDD-EE53C4F4D892-312-00001182D509BCB7} -->
[[Machine Learning TOC|Machine Learning]]
# Features
#concepts
ML features are the input or X variables. Often arranged as a [[Linear Algebra TOC|Matrix]] for easy 
manipulation.

See also [[Labels]]

## Feature Analysis
Techniques to understand which features are the most/ least useful.

### Removal analysis
Remove one feature at a time and see how the model performes. Fits a new model and sees whether it is less accurate. Record [[AUC]] or some similar metric.

### Random Features
Try adding random features, in conjunction with the above.[[030 People MOC]]
# Ferdinand de Saussure
#people 
Thinker on [[Linguistics MOC|Linguistics]].

Referenced by [[Steven Pinker]].[[010 Mind MOC]]
# Feynman Learning Method
#concepts #🌲 

The Feynman Method is a learning method developed by [[Richard Feynman]]. The idea is to focus learning around building an ability to explain the topic to a sixth grader.


## The Technique
1. Pretend to teach a concept you want to learn to a child or rubber duck. I should come up with some actor I reuse for this!
2. Identify gaps in the explanation. Go back to the source information and understand the necessary concepts.
3. Organize and Simplify. This is basically the purpose of [[Evergreen Notes]]!!
4. Transmit. The final and optional stage, whereby the information is actually conveyed to a third party. This is important! Think of my suite and [[Quantum Mechanics TOC]].

See the related [[Einstein Filtering Method]]

[[Statistics TOC]]
# Fixed Effects Model
#concepts
[[Notes to make]]

One of the models used to estimate [[Panel Data]]. This model is useful for capturing some of the observed heterogeneity between observations. 

### Added Variables:
	* *Cross-Section Dummies:*
	* *Time-Series Dummies:*

<!-- {BearID:AA2FAC00-3619-49FB-9D6D-01A558701D98-16983-000109FC90741EE9} -->
[[052 Physics TOC]]
# Forces
[[Electrostatic Force]]
[[Gravity]][[001 Meta MOC]] | [[010 Mind MOC]]
# Frames of Reference
#🌱
#todo #concepts

I still need to figure out how and whether to delineate this from [[011 Mental Models MOC]]. There may be a distinction actually - perhaps frames of reference refers more to the application of mental models rather than the models themselves. There must be some conceivable problem solving strategy, even an informal one, that will allow me to incorporate this idea into [[My Forest]][[Heuristics and Biases]]
# Framing
#concepts/cognitive_biases 

With framing, people's decisions are unduly influenced by the way in which they're presented. This contrasts with [[Priming]], which focuses on the order in which is presented.

Some examples of this are the [[Halo Effect]], [[Pygmalion Effect]] and [[Man with Hammer]]

This bias can be attributed to a combination of things like [[Loss Aversion]], [[Recency Bias]] and the [[Affect Heuristic]].[[030 People MOC]]---
aliases: ["Hayek"]
---
[[030 People MOC]]
# Friedrich Hayek
#people 

Hayek was a very interesting economist and political philosopher. Perhaps his most notable work is [[Hayek - The Road To Serfdom]], where he describes how government economic control necessitates totalitarian rule. More deeply than this, the book talks about the danger implicit in "the character and the growth of ideas."** 

Hayek appears to have held some fascinating views on governance. He was a self-described [[Classical Liberalism|Classical Liberalist]], although he eschewed the label because of it's more modern connotations, preferring instead the self-ascribed label of "Burkean Whig", following after the ideas of [[Edmund Burke]]

## Ideas
- [[061 Philosophy TOC]] | [[063 Economics TOC]] | [[064 Politics TOC]]
- [[Classical Liberalism]]
- [[Individualism]]
- [[Collectivism]]

## Quotes
	“The political ideals of a people and its attitude toward authority are as much the effect as the cause of the political institutions under which it lives.”

	“If in the long run we are the makers of our own fate, in the short run we are the captives of the ideas we have created.”



## Outputs
[[Hayek - The Road To Serfdom]]
[[Hayek - The Constitution of Liberty]]
[[Cognitive Biases TOC]]
# Fundamental Attribution Error
#concepts/cognitive_biases 

This is people's tendency to overestimate the consistency of behaviour. The resulting bias is that we tend to chalk up our failures to randomness, our successes to skill, yet with others we consider their failures to be skill/ directly caused by their person. # General Observations - Laws
#concepts/social_knowledge

Laws are commonly accepted principles arising from general observation of social circumstance.

[[Hofstadter's Law]]
[[Moore's Law]]
[[Brooks's Law]]


[[030 People MOC]]
# Geoffrey Hinton
#people 

Really interesting guy who pioneered a lot of [[Symbolic AI]], [[Machine Learning TOC|Machine Learning]] and such fields. He's really one of the foremost thinkers.

Part of his approach that I really like is that he actively tries to seek inspiration from [[Quantum Mechanics TOC]] | [[Quantum Phases]]
# Global Phases
#concepts/cs/quantum 
See also [[Relative Phases]]

* Global phase is when you transform all items in a quantum state by the same value
* We can use global phases to transform states 1:1, in the sense that it changes the vector but not the state that is being represented.


![[Pasted image 20210323105819.png]]
# God
#concepts 
[[Religions]]
A page for discussion of the concepts of god, however they may be.
# Grade change
Dean of international programs - Carlos Vilez

 [[2021-05-14]][[Linguistics MOC]]
# Grammar
#concepts/linguistics 

The assembly of [[Words]], phrases and sentences.

Grammar can be separated into two categories:
1. Prescriptive: how people aught to construct [[Linguistics MOC|Language]]
	- Many of these are are pretty useless and a bit clumsy. Lots of them have funny history, lots from [[Latin]] or [[Ancient Greek]].
	- Lots of it has [[Geographic]] origin.
	- Thus dialects can be just as sophisticated or complex as proper language.
2. Descriptive: how people actually tend to construct language

Lots of the modern theories of grammar stem from the ideas of [[Noam Chomsky]]. Chomsky argued that we don't memorize word relations, but rather combine [[Semantics]] with knowledge of grammatical structures to build and understand new sentences.
- Chomsky claimed that children are pre-wired with a "universal grammar" that defines some extent of what languages should be like. Kids generalize rules very interestingly.
	- [[Poverty of the Input Argument]]: claims that if something hasn't been learned, it must have been pre-wired. Children's use of the structure-dependent rather than word-by-word rule of creating questions from statements support this.[[Statistics TOC]]
# Granger Causality Test
#concepts #concepts/math/stats

Implemented in [[ECON411 HW8]][[ECON411 HW8 Notes]]
[[051 Math TOC]]
# Graph Theory
#concepts #🌱 

Graph theory is a fascinating concept. Lots of systems can be abstracted as graphs, making it possible to solve problems with [[Algorithms TOC]].

## Definitions
We can define graphs in any number of ways, however a simple and clear approach is to define a graph G:

### Undirected Graph G(V, E)
Undirected graphs are made of two components, vertices and edges. 

$V = v_1, v_2... v_n$ as nodes on the graph.

$E = (v_x, v_y), (v_i, v_j)...$ as edges connecting nodes.


### Degree
The number of edges incident to a vertex/ node.

# Gravity, force and space
<!-- #_reference/physics -->

[[Newton’s Laws]]
[[Forces]]

*Acceleration due to gravity*
1 g is what we experience on earth’s surface
1g = 10m/s = 22mph/s
#🌱 
[[010 Mind MOC]] | [[011 Mental Models MOC]]
# Greater Fool Theory
#concepts #concepts/mental_models #🌱

Greater fool theory states that the price of an asset is determined by whether or not it can be sold for higher than the purchase price, rather than solely it's fundamental value. The person buying the asset later on for a higher price is deemed the *greater fool*.

This concept goes some way in explaining [[Bubbles]]

When applied as a stock-selection strategy, this becomes a [[Keynesian Beauty Contest]].[[053 Computer Science TOC]] | [[Algorithms TOC]]
# Greedy Algos
#concepts/cs/algorithms

Greedy algos are one way of solving problems, where each iteration roughly tries to do as much as possible before passing on to the next iteration. An example is the max-match segmentation problem:

## Max-Match Problem
Given a string of non-segmented words forming a phrase,
1. Start a pointer at the beginning of the string
2. Find the longest word in the dictionary that matches the string starting at pointer
3. Move the pointer over the word in the string
4. Go to 2d[[000 Life MOC]] | [[061 Philosophy TOC]]
# Greek Philosophy
#concepts #concepts/philosophy

* Arose during 6th Century BC while Greek lands were in major peril from eastern invasion. Philosophy was used to make sense of the world in a non-religious way.

## Key History
* Some argue Greek Philosophy arose out of older wisdom literature and mythological cosmogonies in the ancient Near East. There are certainly influences, but philosophy as we know it was most likely a creation of the Greeks.
	* *Pre-Socratic Philosophy*
		* Milesian School
		* Xenophanes
		* Pythagoreanism
		* Heraclitus
		* Eleatic Philosophy
		* Pluralism/ atomism
		* Sophism
	* *Classical Greek Philosophy*
		* [[Socrates]] - Considered father of modern political and moral philosophy, from Athens, 470-399BC
			* Defined separation of soul and body we now take for granted in western culture.
				* This was inherited by [[Christianity]] as it meshed well with the other teachings 
		* [[Plato]] - recorded socrates (Platonic Socrates). Was Socrates' student. Not always a reliable scribe, since he was smart and had his own metaphysical ideas.
		* [[Aristotle]]

## Key People

## Concerned Topics:
* Astronomy
* Mathematics
* Political Philosophy
* Ethics
* Metaphysics
* Ontology
* Logic
* Biology
* Rhetoric
* Aesthetics

Lots more reading and learning to do here. I think next steps will involve actually diving into some of these writings.

[[Social Contract]]

# HW 11
#school [[ECON211 Regression]]
# a)
```
-------------------------------------------------------------------------------
      name:  <unnamed>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw11.log
  log type:  text
 opened on:   7 May 2019, 23:51:25
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\HOSPITAL13.dta"
. sum D DISTANCE INCOME OLD
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
           D |        499    .7294589    .4446852          0          1
    DISTANCE |        499   -1.010644    2.675655    -3.7461     3.7648
      INCOME |        499    45.70915    3.267098   41.12455   55.16559
         OLD |        499    .8376754    .3691184          0          1
```
We will be estimating a regression on patients visiting either Cedar Sinai or UCLA Medical center hospitals, and examining the effects of distance (DISTANCE, distance to cedar sinai minus distance to UCLA), income (INCOME, income of patients zip code average in thousands), and age (OLD = 1 if > 75, else 0). Our dependent variable is a dummy, 1 if the patient visited Cedars Sinai, 0 if UCLA. Hence our initial model will be 
```
D = B0 + B1DISTANCE + B2INCOME + B3OLD + E
```
The above summary statistics show that 72% of the patients visited Cedars Sinai, while the distances were evenly spread between 3.7 miles closer or further to Cedars Sinai. Income ranged between $41,000 and $55,000. Roughly 84% of the patients were old. 
```
. corr D DISTANCE INCOME OLD
(obs=499)
             |        D DISTANCE   INCOME      OLD
-------------+------------------------------------
           D |   1.0000
    DISTANCE |  -0.4699   1.0000
      INCOME |  -0.2919   0.5030   1.0000
         OLD |   0.0011  -0.0771  -0.1265   1.0000
```
Our correlation results suggest distance is quite strongly correlated with choice, income slightly and age not at all. In terms of risk of multicollinearity, our distance and income have a coefficient of 0.5. Given our negative correlations, we can estimate that distance and income will have negative coefficients. 
```
. * Model 1: LPM
. reg D DISTANCE INCOME OLD
      Source |       SS           df       MS      Number of obs   =       499
-------------+----------------------------------   F(3, 495)       =     48.36
       Model |  22.3220692         3  7.44068974   Prob > F        =    0.0000
    Residual |  76.1548847       495  .153848252   R-squared       =    0.2267
-------------+----------------------------------   Adj R-squared   =    0.2220
       Total |  98.4769539       498  .197744887   Root MSE        =    .39223
------------------------------------------------------------------------------
           D |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    DISTANCE |  -.0719946   .0076013    -9.47   0.000    -.0869295   -.0570597
      INCOME |  -.0108073   .0062571    -1.73   0.085     -.023101    .0014864
         OLD |  -.0510464   .0480092    -1.06   0.288    -.1453733    .0432805
       _cons |   1.193452   .2971679     4.02   0.000     .6095857    1.777318
------------------------------------------------------------------------------
```
Above is our initial linear probability (LPM) model run, using the aforementioned model. Our model suggests that for each additional mile of distance closer to one of the hospitals, that hospital is 7% more likely to be chosen, which seems reasonably close to our expectations. The sign matches our expectation. Our income also matches our expected negative sign, and our coefficient estimate suggests that as someone earns $1k more, they are 1% more likely to go to UCLA. Finally our OLD coefficient of -0.05 suggests that older patients are 5% more likely to go to UCLA. Our r squared value is low but reasonable given the estimation technique, and our overall significance f value appears high, with an f statistic of 0. Hence under a cursory examination our model appears to decently explain our data.

# b)
Our R squared value is 0.2267, meaning 23% of the variation in the choice of hospitals can be explained by our model and data. Our adjusted R squared is virtually the same, meaning degrees of freedom are not significantly impacting our results. Our overall significance seems high, where using the overall F test:
H0: None of the variables are significant
HA: H0 false
we get an F value of 48, meaning we can reject our null hypothesis that none of the variables are significant with more than a 99.9% confidence. As such our F statistic is 0. Our Root MSE is quite high at .39, given our choice should only be 0 or 1. 

Most of our individual significances appear to be high. We shall directly test the significance of the distance variable. To do this, we set up a one-tailed t test at 95% significance with:
H0: variable not significant
HA: H0 false
We calculate our t observed value by doing B1 / SE(B1) = -9.47. This is much larger than our critical t value of 1.96, so we reject the null in favor of the alternative. Hence we can conclude that our distance variable is significant.
```
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |        499 -303.1658   -239.031       4    486.0621   502.9125
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
Our Akaike’s information criterion and Bayesian information criterion values are very large at 486 and 503 respectively. This suggests that there may be some issues with our model or regression technique, as we shall soon see.
```
. predict lpm_hat
(option xb assumed; fitted values)
. sum lpm_hat
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     lpm_hat |        499    .7294589    .2117154   .3343681   1.007409
```
Above we can see that our predicted choice values do not lie within the theoretical bounds of 0 and 1, suggesting issues with our linear probability model. 
```
. imtest, white
White's test for Ho: homoskedasticity
         against Ha: unrestricted heteroskedasticity

         chi2(8)      =    106.39
         Prob > chi2  =    0.0000
Cameron & Trivedi's decomposition of IM-test
---------------------------------------------------
              Source |       chi2     df      p
---------------------+-----------------------------
  Heteroskedasticity |     106.39      8    0.0000
            Skewness |     160.12      3    0.0000
            Kurtosis |       3.63      1    0.0568
---------------------+-----------------------------
               Total |     270.14     12    0.0000
---------------------------------------------------
```
Finally, we suspect that there may be an issue of heteroskedasticity. To check for this, we run a white test with 
H0: homoskedasticity
HA: heteroskedasticity
With 8 degrees of freedom, our resulting Chi2 value is 106.39, resulting in a p value of 0. Our IM test decomposition shows that the data is both strongly heteroskedastic, and strongly skewed.

## c)
```
. * Model 2: Logit
. logit D DISTANCE INCOME OLD
Iteration 0:   log likelihood = -291.31434  
Iteration 1:   log likelihood =  -236.8383  
Iteration 2:   log likelihood = -234.53106  
Iteration 3:   log likelihood = -234.51768  
Iteration 4:   log likelihood = -234.51768  
Logistic regression                             Number of obs     =        499
                                                LR chi2(3)        =     113.59
                                                Prob > chi2       =     0.0000
Log likelihood = -234.51768                     Pseudo R2         =     0.1950
------------------------------------------------------------------------------
           D |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    DISTANCE |  -.3755931   .0462444    -8.12   0.000    -.4662305   -.2849558
      INCOME |  -.0722135   .0361188    -2.00   0.046     -.143005   -.0014219
         OLD |  -.2892923   .3085399    -0.94   0.348    -.8940194    .3154347
       _cons |   4.412409   1.727957     2.55   0.011     1.025674    7.799143
------------------------------------------------------------------------------
```
Above, we reestimate our previous model using a logit regression, bounding our predicted values between 0 and 1.  As a result, we have to interpret our results slightly differently, as our marginal probabilities now vary depending on the point of evaluation. We now interpret our coefficients as the change in the log of the odds ratio. For distance for example, we would say that for each mile closer to Cedars Sinai, the log of the odds ratio of choosing Cedars Sinai is .37

# d)
Our Logit model has a pseudo R squared value of .195, meaning 19.5% of the variation in our dependent variable can be explained by the data and model provided. To test for overall significance, we will use a likelihood ratio test with:
H0: all independent variable coefficients = 0
HA: H0 false
```
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |        499 -291.3143  -234.5177       4    477.0354   493.8858
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
We can calculate our Chi2 value to be 113.59, allowing us to reject the null hypothesis at 95% confidence and conclude that the model is statistically significant. Our AIC and BIC values are slightly improved from the original model.

Individual significance looks the same as before except for OLD. If we set up a one tailed t test for individual significance with:
H0: Not significant
HA: Significant
We get can calculate a t value of -0.94, not even reaching the bar of 1 for inclusion in the model, suggesting age could be dropped. The other variables are individually significant using the same test to 95% confidence.

# e)
First, we shall derive the marginal probability of choosing one of the hospitals over the other based on income.  To do this, we first calculate our Pi, written below as p_logit. With this, we calculate our odds ratio and use that to find our resulting marginal probability for income to be -0.0134.
```
. scalar p_logit = exp(4.412409 + (-.0722135 * 45.70915))/(1 + exp(4.412409 + (
> -.0722135 * 45.70915)))
. di p_logit
.75242566
. scalar odds_ratio = p_logit / (1 - p_logit)
. di odds_ratio
3.0391908
. scalar mp_logit =  -.0722135 * (p_logit * (1 - p_logit))
. di mp_logit
-.01345202
```
Then, we run mfx to generate the remainder of the values, and check our results. Our Probability was very close to the actual value, differing by 0.02. Furthermore, our marginal probability for income was only .009 off of the actual value. We can interpret both of these values as meaning that for the average patient, the marginal probability of choosing one hospital over the other for a thousand dollars of additional income was -0.0125
```
. mfx
Marginal effects after logit
      y  = Pr(D) (predict)
         =  .77710086
------------------------------------------------------------------------------
variable |      dy/dx    Std. Err.     z    P>|z|  [    95% C.I.   ]      X
---------+--------------------------------------------------------------------
DISTANCE |  -.0650584       .0079   -8.23   0.000  -.080547  -.04957  -1.01064
  INCOME |  -.0125085      .00621   -2.01   0.044  -.024681 -.000336   45.7091
     OLD*|  -.0473948       .0477   -0.99   0.320  -.140885  .046095   .837675
------------------------------------------------------------------------------
(*) dy/dx is for discrete change of dummy variable from 0 to 1
end of do-file
```

# f)
As we explicitly calculate above, the odds ratio of the average patient choosing Cedars Sinai over USLA is 3.039.

<!-- {BearID:18967950-EE09-48FE-AFCB-F140593E7871-1155-000034F26BFCFEE7} -->
# HW4
#school 
[[ECON211 Regression]]
```
----------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw4.log
  log type:  text
 opened on:  10 Mar 2019, 17:19:02

. 
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\taiwan.dta"

. 
. tsset year
        time variable:  year, 1958 to 1972
                delta:  1 unit

. 
. 
. * Data summary
. summ q l k year

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
           q |         15    24292.53    13864.75     8911.4      54308
           l |         15    578.6133    244.3245      281.5      999.2
           k |         15    159919.3    38425.85     120753     239715
        year |         15        1965    4.472136       1958       1972
a) This data summary shows several important things about our data sample. First, it is clear that we have time-series data from the year 1958 to 1972. Our dependent variable in this case will be real gross product q, which ranges from 8911.4 to 54308, with a mean of 24292.53. Our independent variables are labor input and capital, coded as l and k, and they range from 281.5 to 999.2 and 120753 to 239715 respectively. These data appear to be suitable for use in the following regression, so we can proceed to examine the correlations. 

. corr q l k
(obs=15)

             |        q        l        k
-------------+---------------------------
           q |   1.0000
           l |   0.9618   1.0000
           k |   0.9949   0.9636   1.0000
a) This correlation result reveals a more complicated story. Here we can see that while the dependent variable correlates highly with the two independent variables (0.9618 and 0.9949), the two independent variables also have a very high correlation (0.9636). This could present future problems as one of the variables may not be accurately represented in a traditional linear model.

From this correlation result, we can hypothesise that both labour and capital will have positive coefficients.

. 
. 
. * MODEL 1: LINLIN (Linear Production function)
. reg q l k

      Source |       SS           df       MS      Number of obs   =        15
-------------+----------------------------------   F(2, 12)        =    594.76
       Model |  2.6644e+09         2  1.3322e+09   Prob > F        =    0.0000
    Residual |  26878436.6        12  2239869.72   R-squared       =    0.9900
-------------+----------------------------------   Adj R-squared   =    0.9883
       Total |  2.6912e+09        14   192231167   Root MSE        =    1496.6
------------------------------------------------------------------------------
           q |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           l |    2.44061   6.125326     0.40   0.697    -10.90533    15.78655
           k |   .3440342   .0389469     8.83   0.000     .2591763    .4288922
       _cons |  -32137.37    2993.35   -10.74   0.000    -38659.32   -25615.42
------------------------------------------------------------------------------
. 
. estat vce
Covariance matrix of coefficients of regress model

        e(V) |          l           k       _cons 
-------------+------------------------------------
           l |  37.519615                         
           k | -.22988386   .00151686             
       _cons |  15053.524  -109.56143   8960145.8 

a) The above estimated regression appears to be very successful at first glance. We see that the Adj R-squared is 0.9883, and the overall formal significance has a value of 0.0000. Both of these suggest exceptionally strong results. However, as we look at the results of the included t-tests, an interesting problem emerges. We notice that while both of our dependent variables correlated highly with our independent variable, our labour variable has a t-statistic of 0.697. This means that when we test the significance of labour in this regression, we use H0: A1=0 and HA: A1=/=0, and we cannot reject the null hypothesis that the coefficient on labour is nonzero. This suggests a contradictory result, since we know labour is highly correlated to output, and our economic theory underpinning the model remains strong. This suggests that perhaps our variables do not closely relate in linear space, so we shall run a LogLog model to test this.

. 
. 
. * MODEL 2: LOGLOG (Cobb-Douglas Production function)
. gen lnq = log(q)
. gen lnl = log(l)
. gen lnk = log(k)

. summ lnq lnl lnk

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
         lnq |         15    9.949177    .5662919   9.095087   10.90243
         lnl |         15    6.271022    .4469553   5.640132   6.906955
         lnk |         15    11.95752    .2266224    11.7015   12.38721

. corr lnq lnl lnk
(obs=15)

             |      lnq      lnl      lnk
-------------+---------------------------
         lnq |   1.0000
         lnl |   0.9798   1.0000
         lnk |   0.9768   0.9486   1.0000
b) These data seem similar to the prior ones, however some aspects are fundamentally different. All the observations are now on a log scale, so the ranges are much smaller. The correlation bewteen the independent variables is slightly lower than before, but still far above our general concern point of 0.8. We still have 15 observations.

. reg lnq lnl lnk

      Source |       SS           df       MS      Number of obs   =        15
-------------+----------------------------------   F(2, 12)        =    335.83
       Model |  4.41080622         2  2.20540311   Prob > F        =    0.0000
    Residual |  .078804495        12  .006567041   R-squared       =    0.9824
-------------+----------------------------------   Adj R-squared   =    0.9795
       Total |  4.48961072        14   .32068648   Root MSE        =    .08104
------------------------------------------------------------------------------
         lnq |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         lnl |   .6731036   .1531438     4.40   0.001     .3394319    1.006775
         lnk |   1.181609   .3020373     3.91   0.002      .523526    1.839692
       _cons |  -8.400986   2.717716    -3.09   0.009    -14.32238    -2.47959
------------------------------------------------------------------------------
. estat vce
Covariance matrix of coefficients of regress model

        e(V) |        lnl         lnk       _cons 
-------------+------------------------------------
         lnl |  .02345303                         
         lnk | -.04387859   .09122655             
       _cons |  .37760491  -.81568002   7.3859826 
. 
. * Model comparisons:
. * Model 1 adjusted R^2: 0.9883
. * Model 2 adjusted R^2: 0.9795 
c) While we can try to compare the above adj-R^2 values, they are not representative of the relative validities of the two models since they are explaining entirely different relationships, hence the comparison would be invalid. We see that both have very high results of .9883 and .9795 for Models 1 and 2 respectively, however our LogLog model is much more valid in actuality, when looking at the economic intuition underlying our choice of double log space.

* 
. * Model 1 labour elasticity: [Bk] * x_bar/y_bar
. di 2.44061*(578.6133/24292.53)
.05813184
. * Model 1 Capital elasticity:
. di .3440342*(159919.3/24292.53)
2.2647994
 
. * Model 2 labour elasticity (same as lnl coef.): 0.6731037 
. * Model 2 capital elasticity (same as lnk coef.): 1.181609

d) Our elasticity results show why the switch to the double log functional form was essential. We can see the labour elasticity go from a very unrealistic 0.05813184 to 0.6731037, while the capital elasticity roughly halfs from 2.2647994 to 1.181609. Both of these changes are meaningful, as we can see a much tighter relationship between our output and our dependent variables.

. * Special F-Test
* UR: lnQ=LnB0+B1lnL+B2lnK+E
* RR: ln(Q/L)=lnB0+B2ln(K/L)+E
* H0: B1+B2=1
* HA: B1+B2=/=1

. gen output_labour_ratio = log(q/l)
. gen capital_labour_ratio = log(k/l)
. 
. * Restricted Regression:
. reg output_labour_ratio capital_labour_ratio

      Source |       SS           df       MS      Number of obs   =        15
-------------+----------------------------------   F(1, 13)        =      4.32
       Model |  .085343795         1  .085343795   Prob > F        =    0.0581
    Residual |  .257000303        13  .019769254   R-squared       =    0.2493
-------------+----------------------------------   Adj R-squared   =    0.1915
       Total |  .342344098        14   .02445315   Root MSE        =     .1406
--------------------------------------------------------------------------------------
 output_labour_ratio |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
---------------------+----------------------------------------------------------------
capital_labour_ratio |  -.3215597   .1547643    -2.08   0.058    -.6559077    .0127882
               _cons |   5.506705    .880816     6.25   0.000     3.603818    7.409592
--------------------------------------------------------------------------------------
 
. * restrictions
. * h = 1
. * n = 15
. * k^UR = 3
. 
. gen F = ((.257000303-.078804495)/1)/((.078804495)/12)

. di F
27.13487
. * F-critical = 9.33

* Hence, we can conclude that we can reject the null hypothesis, meaning there are not constant returns to scale.
```

<!-- {BearID:C4ECCB62-0038-4B74-9472-1D5F1A4218E3-312-00003E3F412693C4} -->
[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]] | [[Quantum Gates]]
# Hadamard Gates
#concepts 

Hadamard gates act on the standard basis states by putting them in [[Superposition]].

$\Large\ket{0}\rightarrow\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$

$\Large\ket{1}\rightarrow\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})$

We typically write these two states as $\large\ket{+}$ and $\large\ket{-}$.

Note that a very interesting side effect of this defined behaviour is [[Quantum Interference]], where states can cancel out if we get the $\ket{-}$.

Used in [[Deutsch's Algorithm]]# Halloweekend Plans
[[Deprecated]]
 
<!-- {BearID:C781EE5A-8B82-4BDA-9CE3-84CF3166D59C-37181-0003936CB497BD88} -->
[[053 Computer Science TOC]]
# Halting Problem
#concepts/cs

The halting problem is an [[Decidability TOC#Decidable properties]]  [[Turing Machines TOC]] problem asking:
```
Given TM M and string x, does M halt on x?
```

We can say whether TM M does /not/ halt on x, but we cannot definitely say whether it does.

The halting problem is computationally equivalent to The Membership Problem, or `HP = MP`

## Proof
	* Suppose every bit string encodes some [[Turing Machines]]
	* Let M_x be the TM encoded by bit string x
	* Create a table of all TMs and all bit strings (really a table of all RE sets)
	* Table entry (i, j) indicates whether TM M_i halts on string j
		* Fill in table with Y or N respectively
*NOW to PROVE:*
* Suppose K is a total TM that can fill in the table
	* Given M#x, K runs TM M on string x
	* K halts and accepts if M on x halts
	* K halts and rejects if M on x loops
	* So: `L(K) = {M#x | M halts on x}`
	* Thus, the language accepted by K is pairs M#x where M halts on x
	* Now consider the entries on the diagonal of the table. These entries encode what the behavior of M#x is for input x
		* We now do the opposite (assuming we are given TM K that can actually fill the table out in the first place) So we create and define a new TM D that given any string x:
			* Runs K on M_x#x
			* If K rejects M_x#x then D accepts x
			* If K accepts M_x#x then D enters an infinite loop
			* D is a new TM not previously in the table, meaning we did not actually enumerate all possible TMs since D is not in our original table of all RE sets
			* Thus there is a contradiction! This comes from the fact that we assumed there was a total TM K that could fill in the table, but no such TM exists. 
			* Thus, *halting is undecidable/ not RE.*

See an example table of all RE sets:
[[Todo]] #todo Transcribe into LaTeX from bear image![[300 Inputs MOC]] | [[054 Neuroscience MOC]] | [[AI]] | [[Neural Networks]]
# Hassabis - Neuroscience Inspired AI
#inputs/articles 

[[Demis Hassabis]]

Very interesting summary article talking about the history, present and potential future of [[AI]] and [[054 Neuroscience MOC]] research. I think there's a lot of merit to looking at biology for inspiration and confirmation.

This article presented TONS of further things to read for me. Gonna have to gradually go through and digest it.

**Main Idea: Neuroscience can serve to provide *inspiration* for new areas of research in *validation* that progress is going in the right direction**

## Further reading
- Marr, D., and Poggio, T. (1976). From understanding computation to understanding neural circuitry. A.I. Memo 357, 1–22.
	- Describes three levels of analysis needed to understand a complex biological system
		1. The goals of the system
		2. The process and computation that achieves this goal
		3. The mechanism by which this process is implemented
-  Rumelhart, D.E., McClelland, J.L., and Group, P.R. (1986). Parallel Distributed Processing: Explorations in the Microstructures of Cognition, Volume 1 (MIT Press).
	-  Origins of the PDP/ parallel distributed processing movement. Essentially a team of psychologists and neurosci's who thought about brain functionality. Focus on [[Visual Cortex]]. Led to the development of [[Neural Networks]]
-  [[Statistics TOC]]
# Hausman Test
#concepts #concepts/math/stats 

The Hausman test can be used to detect whether a variable suffers from endogeneity.

For an example see [[ECON411 HW6]] [[ECON411 HW9]]

# Steps to conduct the Hausman
1. Start with a structural equation containing at least one potentially endogenous variable.
2. Create a reduced form equation for each endog, regressing it against all exogenous variables
3. Save the residuals from this RF equation
4. Rerun the structural eq, adding residuals
5. Want individual t~3, f~10
6. Profit

<!-- {BearID:297514A4-1EEB-48AF-AF26-CAC564BC111A-37181-00044D775191B6CD} -->
[[300 Inputs MOC]]
# [[Friedrich Hayek]] - The Road To Serfdom
#inputs/books

#### Topics
[[Classical Liberalism]]
[[063 Economics TOC]]
[[064 Politics TOC]]
[[061 Philosophy TOC]]

This book is an ode to the simultaneous power and danger of ideas. This is such an interesting contrast to [[Socrates]], who rejected the importance of ideas alone, and instead extolled the virtue of actions via knowledge. Ideas spread, moreso now than ever before. This book is gonna be an interesting read. 

Now that I've read a bit more, I certainly have more thoughts. I'm a lot more convinced by the overall dogma of [[Classical Liberalism]] that Hayek works so dilligently to advocate for. It's interesting how much his core argument of centralized control equalling tyranny resembles my suspicions on the topic. I could just be finding evidence to confirm my thoughts, but I really think that this is what makes me so uncomfortable by for example the [[Middlebury Socialist]]. 

## Interesting Arguments
At one point, Hayek argues that [[Socialism]] is really a flavor of [[Collectivism]], where collectivism describes some of the methods by which certain individuals hope we can arrive at the socialist ideal.

## Quotes
	“If in the long run we are the makers of our own fate, in the short run we are the captives of the ideas we have created.”
	
	“The political ideals of a people and its attitude toward authority are as much the effect as the cause of the political institutions under which it lives.”

	"While history runs its course, it is not history to us. It leads us into an unknown land, and but rarely can we get a glimpse of what lies ahead."
	
	"By moving from one country to another, one may sometimes twice watch similar phases of intellectual development."
	
	"The problem is not why the Germans as such are vicious, which congenitally they are probably no more than other peoples, but to determine the circumstances which during the last seventy years have made possible the progressive growth and the ultimate victory of a particular set of ideas, and why in the end this victory has brought the most vicious elements among them to the top."
	
	"According to the views now dominant, the question is no longer how we can make the best use of the spontaneous forces found in a free society. We have in effect undertaken to dispense with the forces which produced unforeseen results and to replace the impersonal and anonymous mechanism of the market by collective and “conscious” direction of all social forces to deliberately chosen goals."
	
	"Democracy and socialism have nothing in common but one word: equality. But notice the difference: while democracy seeks equality in liberty, socialism seeks equality in restraint and servitude."
	
	"Many a university teacher during the 1930s has seen English and American students return from the Continent uncertain whether they were communists or Nazis and certain only that they hated Western liberal civilization."
	
[[011 Mental Models MOC]] | [[Heuristics and Biases]]
# Heuristics MOC
#concepts/mental_models 

Heuristics are mental shortcuts we rely on to simplify the decision making process, since all decisions can't be made fully [[Consciousness|Consciously]]. The downside to using these quick tricks is that we end up making common, repeatable mistakes that can be broadly termed [[Cognitive Biases TOC]].

I don't have a very exhaustive list of these, and they're obviously very closely linked to the aforementioned [[Cognitive Biases TOC]] page. I put more thought into the biases, which are the resultant disconnects between expectation and reality, rather than the heuristics that cause them.

## List of Heuristics
- [[Affect Heuristic]]
- [[Availability Heuristic]][[010 Mind MOC]]
## Heuristics and Biases
#concepts/mental_models #concepts/cognitive_biases #🌲 

[[Heuristics TOC]] lead to **[[Cognitive Biases TOC]].**

These heuristics and biases are common, recurrent mistakes human beings make in their decisions. I started thinking about all of this seriously from the book *Nudge* by Tversky & Thaler, and have been diving into building an understanding of how and why this is from books like *Thinking Fast and Slow* by Kahneman.

Think of [[Heuristics TOC]] as mental shortcuts. The key thing to remember here is that they were all at some point evolved through [[Evolution]], suggesting that there should be scenarios where all of the heuristics serve a purpose that creates a [[Competitive Advantage]]

See also [[011 Mental Models MOC]] for a more complete list, and not just negative pitfalls.
- [[Framing]]
- [[Priming]]
- [[Anchoring]]

[[The Map is not the Territory]]
[[Affect Heuristic]]
[[Availability Heuristic]]
[[Law of Small Numbers]]
[[Regression to the Mean]]
[[Story-based Understanding]]
[[Jumping to Conclusions]][[071 XCAP MOC]]
# Hinton Closing Letter
#projects/old/xcap 

Dear Wiatt,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |            |
| -------------------- | ---------- |
| Amount Invested      | $ 500.00   |
| Shares Purchased     | 489.54     |
| Cash-out Share Price | $2.16      |
| Cash-out Value       | $ 1,057.41 |
| Return on Investment | 111.5%     |

- Amount Invested: $500.00
- Shares Purchased: 489.54
- Cash out share price: $2.16
- Cash out value: $1,057.41
- Return on Investment: 111.5%

Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL
[[054 Neuroscience MOC]]
# Hippocampus
#🌱 

Part of the brain involved in memory storage and retrieval. # Historic mkt caps
[[071 XCAP MOC]] [[TCNNF]] [[NMIH]]
Trulieve - *5.74 - 13.36*
Curr price: 15.73
Mkt cap: 1.72
630mm- 1,460mm


Tencent - 40-72.9
curr: 68.33
mkt: 641b
375bn - 683bn

<!-- {BearID:66800C0C-DA2A-4AAC-AF28-B85F04662B40-4760-0002929FB88E8C75} -->
[[Quotes]] | [[General Observations - Laws]]
# Hofstadter's Law
#inputs/quotes 
Coined by Douglas Hofstadter

**Hofstadter's Law:** It always takes longer than you expect, even when you take into account Hofstadter's Law.
[[011 Mental Models MOC]] | [[054 Biology MOC]]
# Human Instincts
#concepts #concepts/mental_models #🌱 

This page needs some love!! I want to link this idea to my [[Decision Machines]].

Human instincts are the resulting tugs on our behavior that stem from [[Biologically Innate Human Tendencies]]. They are generally attributable combination of [[Social Pressures]] and [[Evolution]].

- [[Narrative Instinct]]
- [[Curiosity Instinct]]
- [[Language Instinct]]
- [[Commitment and Consistency Instinct]] - humans have a preference to keep prior commitments and stay consistent with our prior selves when possible.
- [[Survival Instinct]]
- [[Inherent Laziness Instinct]]

I want to discern these from the slightly more social-focused:
- [[Trust]]
- [[Envy and jealousy]] - never satisfied
- [[Denial]]
- [[Loss Aversion]]
- [[Sensitivity to Fairness]][[061 Philosophy TOC]]
# Humanism
#concepts 

Humanism is a philosophical stance that emphasizes human freedom and progress. THe term has been used to describe a fairly wide array of beliefs, but generally human beings are seen as the starting point of moral and philosophical inquiry. 

In modern times, humanism tends to be associated with [[Secularism]][[Waves]]
# Huygens Principle
#concepts 
This explains why all waves bend to the slow side, as with atmospheric or oceanic sound distortion. Or think about sound propagation during regular weather vs an inversion.[[006 School MOC]]
# INTD 1242 Carbonomics
#school/class

Carbonomics - Applied economics, finance, chem
	* Carbon must get consideration in asset allocation

# Intro to Climate Science
* Energy Balance <- main idea!!
	* Temp of a system rises or falls if energy consumption + production not in balance
	* When an object warms up (anything above absolute 0) EM radiation increases as 4th power of the temp. 
		* Only way earth releases energy!
		* Stable temp for earth SHOULD be -18c
		* Actual average temp is about +15c
		* Why the discrepancy?
			* Carbon/ water in atmosphere! Earth’s atmosphere is largely transparent to incoming sunlight, but opaque to outgoing longwave radiation
* Anthropogenic forcings
	* Well-mixed gasses
		* CO2
		* CH4 (methane)
		* N20 (nitrous oxide)
		* HFCs
	* Ozone
	* Surface albedo
	* Aerosols
* Can put about 400Gt more carbon into the atmosphere before things go off
	* There’s about 2000Gt of fossil fuel carbon
* Carbon transfer processes
	* Biochemical
	* Physical
	* Consider spatial/ temporal transfers, and what affects their rate of change
	* What does higher CO2 do?:
		* temp influences
		* Photosynthesis rates
	* What does higher temp do?
	* What does changing moisture level do?
	* If we want high carbon storage, need certain type of tree, age matters
	* Fires! Change ability to sequester
* Paris agreement - Voluntary effort
* REDD+

## Middlebury History
* Trustees denied divest on financial reasons
* bought land + forest
	* about 30,000 credits, $10 for others, $3.5 for us
	* Managed by Bluesource
* Market price - set domestically
	* 1 credit = 1 metric ton
* Requires 40 year commitment
* Biogenic vs geologic carbon
* Energy 2028: Midd’s way of shutting up SGA
* Cogeneration - using burned fuel for heating but also energy - all excess heat used for heating 
* Woodchips are now the primary source for heating on campus.
	* When this isn’t enough, oil/ gas burners are used - as of 2016, natural gas
* UBI in the form of carbon tokens?
* 

## Carbon Accounting
* Enables orgs to measure, calculate or estimate; report on and act on GHG emissions
	* Must measure to take any real action
* Renewable energy markets <=> carbon markets
* Speaker - Arnaud Brohé, Agendi
* Broadly two approaches:
	* Territory-based approach - no double counting, most rigorous, usually implied/ mandatory
	* Carbon footprint approach - eg for company, risks double counting, often voluntary
* Greenhouse Gas Protocol - shows best practices
* US emissions have been dropping over last decade, mostly as a result of coal phasing out (gas in), larger renewable footprint in certain states (TX, CA)
* Usually track CO2 “equivalent”, combining everything


1. Scope 1:
	1. DIrect, personal emission, eg gas for boiler for heating, driving car
2. Scope 2:
	1. Indirect emissions, but from the sourcing of electricity, heat or steam
3. Scope 3:\
	1.   
* Hydropower CAN have huge carbon footprint (via methane emissions) if there is a forest in the basin that gets flooded
	* Not with “run-of-the-river”, but dam building does have an effect.
* Consider “lifecycle emission” - same as with electric cars, carbon may be emitted during production
* SBTi Net Zero is becoming a new standard
* To calculate GHG emissions look at all inputs and activities and multiply through by emission factors (eg for one liter of gasoline x CO2 is emitted)
* Use GWPs to calculate comparable heating effect
* CDP/ TCFD standards
	* Implementing TCFD makes all the big climate risks visible
		* Internal carbon fee/ shadow accounting
	* CDP is a survey sent out to CFOs of big companies, with about 100 
* Global supply chain makes carbon accounting effective even in developing nations
	* Investing in climate projects in developing countries is a necessity
		* To create offset tokens for rich country companies!\
* NEED global carbon market!
* Moonshot idea: “green belt” across sub saharan africa
	* Could sell carbon credits to generate revenue at the country level!!!!!!
* REC - renewable energy cert (Usual US standard, but markets fragmented across states
	* Stat arb??????
* Intellectual resources key. for development of new energy tech, nuclear dying out (esp with the progress of wind and solar)
	* Storage solutions are more important!

## Soil Organic Carbon recapture - Steve Apfelbaum
* Adaptive Multi-paddock (AMP) farming
* Measured declining soil carbon from conventionally grazed land
* Microbes do a lot of the job
* Land needs time to recover from grazing
* Overgrazing is more a result of long periods of grazing, so 1000 cows on an acre for a day won’t kill any plants but 10 cows on 10 acres for a year will.
* Soil MUCH more effective as a carbon sink - storing below ground lasts for longer


## Carbon Pricing and Emissions Trading Systems - Michael Azlen
* $1-2bn in carbon assets traded daily
* 1.5 & 2c are the crucial thresholds because they trigger positive feedback loops
* Charging people or entities for carbon emissions makes them think twice about emissions!
	* Carbon tax + dividend
		* Sets price but not quantity
	* Cap & trade market
		* Sets quantity and allows mkt to set price
		* Auctions generate revenue for investment in green tech
		* Each year cap is reduced, so weakest companies stop each year/ co with lowest abatement cost stops producing
		* Creates direct path to monetization, encouraging investment and
* Several different carbon markets:
	* Compliance Carbon Market
		* Mandatory participation
		* Liquid
		* Highly regulated
		* $300bn last year
	* Voluntary Carbon Market
		* Not mandatory
		* Illiquid (no futures/ listed products)
		* Small market
		* $300mn last year
		* No regulation
	* International Carbon Market
		* Developed under Kyoto protocol
		* Countries can trade carbon among themselves to meet NDCs
		* Currently suspended 
* Carbon asset returns not correlated between markets due to lack of fungibility
* But performed amazingly well in recent years!
* Marginal Abatement Cost Curve - most industries can be switched to 0 carbon emissions, but at a certain cost - price must be over $100/ ton to make it economic
* Carbon Leakage - Companies in Europe are subject to a carbon price, so they move all the dirty factories out of EU and produce goods elsewhere where there is no carbon price.
	* Carbon Border Adjustment Mechanism - Any goods coming into EU market that doesn’t come from a country subject to carbon price will be subject to carbon import duty.
* CARB ETF?!?!?!
* TWO KEY TESTS:
	* Additionality - does this product reduce carbon emissions more than they would have otherwise reduced
	* Permanence - Will the offsets last 50+ years?
* 

## Regen Network - Greg Landau (chief regenerative officer)
[Regen Network](https://discord.gg/AraRwtCX)
- Operates a decentralized platform for a thriving planet
- Ecological claims made on platform
	- Generating consensus that a certain claim is valid
- Process to generate a credit
	- Need to estimate carbon content without sampling everything
	- Created carbon Plus credit
	- Correlates satellite data with “ground truth data” to create extrapolated estimates
	- Cost of verifying credits: ~$1-5mm
		- Can’t do it on a small scale!
	- Uses Cosmos based on Proof-of-stake
		- As opposed to Proof of Authority
- Financial reporting vs disclosure reporting
## Prezzie plan
1. Endowment Transparency
	1. first big obvious area for improvement
		1. was tough to gauge current progress
	2. Currently much weaker  than nescac comps
	3. You cant change what you dont track
	4. What is needed?
		1. more clarity on what constitutes ESG
		2. actual breakdowns of investment allocations
2. Forest Investment Project
	1. small project with a lot of upsides
	2. Repeating success with a new development
		1. explain project, run thru basic economics
		2. financial returns ~
			1. IRR just under 4%
			2. ROI near 8%
			3. Conservative estimates ignoring development of carbon markets
		3. Covers Midds own need for new carbon credits
			1. but any excess can be sold
		4. Publicity for incoming students





## Doug Wilson - Practical Regenerative Agriculture


## Project Outline
[[Carbonomics Project]]
Consumption of energy:
	* Thermal (Heating/ cooling)
	* Electricity
* Pull a Biden and get more EVs for campus vehicles? 

# Abbreviations
BTU - British Thermal Unit
IPCC - intergovernmental panel on climate change
GHG - Greenhouse Gas
ETS - emissions trading schemes
COP - conference of parties (of which Kyoto protocol was the result of COP3)
GWP - Global Warming Potential

# Movies
### Kiss the ground
* Ha ha woody harrelson
* The role of tilling in destroying soil - past empires pose an interesting question. Is that how they were all destroyed?
* Chronic stress, overinsecticiding, overpesticiding
* Chemical agriculture
* Microclimate changes from tilling
* UN estimates that topsoil will be gone in 60 years
* Permaculture, perennials

#### Disco:
* Farmer blame?
	* Always consider upsides of change - there are upsides, no need to make decisions because of blame
* Political difficulty - hinders implementation
* Poor practices subsidized
* Timeframe - short run/ long run
* Ignores plant based diet


## MOVIE THOUGHTS
* Lots of ranchers don’t own their own land
* Agricultural subsidies distorting things
* Energy independence -> dominance
	* Has negative undertones
	* Don Young AK
* Unconvinced with arguments that energy security was bad
* Giving land back to the people
	* Tough argument to refute!
	* Vague enough that it doesn’t tell you what’s happening
* Foreign mining on US soil
* 


## Reporting Scopes
* *Scope 1:*
	* Immediate footprint of reporting entity
* *Scope 2:* (indirect)
	* Electricity bought from provider
* *Scope 3:* (indirect)
	* Upstream and downstream activities (ie supply chain)
	* Commuting

<!-- {BearID:D12F6FD9-F062-4285-9D4E-44202B342767-574-000012A00135F0BB} -->
[[095 Journals MOC]]
# India Adventure Memories and Notes
#outputs/journal

## Goa
Goa was a really interesting, if slightly disappointing experience. We were expecting a huge party place with lots of opportunities to dance, but the reality of South Goa was quite different. TBH it was not terrible, just much quieter than I expected.  

The Portuguese influence in Goa is fascinating. There are old European houses, beautiful old churches and some other amazing architecture. It was quite a trip to see it. We spent the first day chilling in the hotel which was very pretty but a little artificial, but the next day we took the 1h30 to drive to Old Goa and Panjim. It was crazy seeing little European streets in the midst of the Indian hubbub. It’s tough to forget you are in India, but sometimes for a moment it almost happens. Exploring the little streets together was so much fun, and we got to take some really cool photos while we walked around. 

We stopped at what people told us was the nicest restaurant in Panjim, and they weren’t mistaken. It was called The Fisherman’s Wharf, and apparently the place has several restaurants around India. We had some amazing fish prepared in an Indian way, and it was lovely. The drive back was a little boring, but it was cool to see the Goan countryside.

Overall a relaxing experience, even though it was different to what we imagined. The ITC Grand was very nice, the staff were all helpful and we enjoyed the stay a lot. 

## Jodhpur
After the wet Goan heat, we woke up at 4am to take two consecutive flights to fly to Jodhpur. Even as the plane was on its final descent we could see this was going to be a very different experience. The century-old havelis stood out even from the plane, but the 48-degree hazy desert heat was not obvious until we got off the plane. The moment we stepped through the planes opened doorway, we were greeted by it like a sledgehammer to the face. The heat doesn’t just touch your skin, it soaks through to your bones. Even the two minute sprint to the airport terminal was tough to our unaccustomed bodies. 

In the terminal, while I was getting the bags, Mahi got up to some of her usual business and started talking to the airport police chief who was standing at the door to the tarmac. As she approached him, he turned around grouchily to try to dismiss her, but within a minute the pair were laughing and he was amicably explaining the ins and outs of the airports daily routine. Mahi never fails to surprise me with her charms, and I love to see her do this kind of thing.

Jodhpur made a huge impression on both of us. The architecture immediately struck me as being centered around pragmatism. It is beautiful without being showy, but most importantly it all makes perfect sense for the blistering climate and weather in Rajasthan. 85% of the buildings are constructed from natural, unpainted sandstone, and most of the windows are actually grates which allow air to pass through. Together, these two features make the buildings feel cool and breezy even when the temperature outside is nearing 50 C. 

Jodhpur has been the seat of the local Muharajah for centuries, and it really shows. As you drive down the beautiful avenues or walk down the busy bazaar streets, things feel regal in a way that many other cities do not. A great palace of sandstone sits atop the tallest hill near the city, where the Maharajah still lives to this day. On the other side of the city sits the old fort, sitting on top of a huge cliff face, overlooking the entirety of the city. The fort is truly amazing, and creates some stunning views while making for a really cool attraction to visit and get a glimpse into the old royal way of life. Just behind the fort lies the blue city, one of the oldest parts of the city where the old Brahmin population traditionally paint their houses blue. This makes for an iconic trip, and is a must see for anyone visiting.

The last highlight of this part of our visit was the old bazaar I briefly mentioned earlier. We bought traditional clothes for our evening excursions and went to the finest fabric exporter in the city. He put on a real show for us, and showed us many divine pashminas made from the finest cashmere one can find. Although many sellers will claim this, he actually does work for many western designer labels (as confirmed by forbes and many others). We didn’t part before buying a lifetime supply of shawls and throws. 

Finally, we took an evening trip out to the desert for what may be the most memorable part of our whole trip. First we got to see the ancient trader town of Ossian, where traders have passed through on their way along the silk route for more than a millenium. The oldest temple there was over 1000 years old, and it was incredible to see how well the sandstone detailing survived this long. As the sun started to set, we rode a camel over the sand dunes encroaching on the little town all the way to our little dinner, where we sat by a fire and watched some traditional musicians and a dancers perform for us. It was truly a unique experience. 

## Delhi
After the hot calmness of Jodhpur, Delhi was a real shock. It was busy, noisy and polluted. The first day we took a walking tour through the old city, and were constantly surrounded by unfriendly glares. Now, both of us are quite used to receiving lots of attention on our travels, but it was never like this before. People seemed angry at us for the first time, just for our presence. Despite this, the history of the city still has an effect on you, as numerous kingdoms and empires have made it their base of operations.

We took a day trip out to Agra, and saw the Taj Mahal and Agra fort. Both were very impressive, but what used to be a sleepy town some 120Km from Delhi now houses 3m+, so the experience was much like Delhi in being crowded and messy. Still a unique experience, and well worth the time to at least visit once. 

## Shimla

<!-- {BearID:2530C24A-2913-40F4-A768-D27121645591-3600-000004D364B9F85F} -->
[[061 Philosophy TOC]]
# Individualism
#concepts #concepts/philosophy # Information Theory
#concepts

#### Head
Note started for [[CS333 Week 1]] of [[CS333 Quantum Computing]]


## Key Terms

* *Classical Information*
	* Binary format information stored as ones or zeros. Need `2^n <= m` to hold to represent m states in n bits.
	* Information content
	*log(m) = n* n bits can store m discrete messages
	* If we denote the information content of a message by /I/ and the prob of its occurrence `p` then:
	*I = -log(p)* meaning the less probable a message, the higher its information content
	* So with the earthquake in Missouri example if the probability of it not happening is 0.995 information content is: *-log(0.995)=0.0072*
	* But if we take the inverse, probability of it happening at 0.005, we get: `7.6439` 


* *Shannon Entropy* - measures information content or amount of uncertainty in a signal

*H(X) = -SUM(p * log(p))* 

	* Let X be a random var with prob distro p, and it can assume values [x1, x2… xn] with associated probabilities [p1, p2… pn] such that [0 < p < 1] and SUM(p) = 1
	* Signal with 1 possible outcome has an entropy of 0
	* Signal with 2 possible outcomes has an entropy of 1
	* Signal with 3 possible outcomes has an entropy of 1.585
	* The more uncertain we are as to what comes next, the higher the Shannon Entropy
	1. Decrease uncertainty -> increase information
	2. Increase uncertainty -> increase entropy
	

	* /Average bit rate/ to encode X:
	*R_x = SUM(l * p)* 
		* Shannon Entropy is the lower bound of this value! *H(X) <= R_x*

	* /Relative entropy/ of two vars X and Y (characterized by p and q prob distros) is:
	*H(X||Y) = SUM(p * log(p/q) = -H(X) - SUM(p * log(q))*

	* /Conditional entropy/
	*H(X|Y) = -SUM(p(x|y) * log(p(x*y)))
		* Satisfies *H(X|Y) < H(X)*

<!-- {BearID:D98547E6-05B3-4D9E-85DA-2C192259512F-2534-0000B3F33D85FBC4} -->
[[051 Math TOC]] | [[Linear Algebra TOC]]
# Inner Products
#concepts/cs/ml 

The inner product is an operation on [[Matrices]] that multiplies each set of components piecewise. Can think of this as revealing the overlap between two [[Quantum States]].

See also [[Outer Products]]

Note that the dot product is a special case of inner product which is positive definite.

## Definition
The inner product of two [[Quantum States]] $\ket{\phi}$ and $\ket{\psi}$ is defined as: 
$$
\ket{\psi}=\alpha\ket{0}+\beta\ket{1}
$$
$$
\ket{\phi}=\gamma\ket{0}+\delta\ket{1}
$$

$$
\braket{\psi|\phi} = \gamma*\alpha+\beta*\delta
$$
Where $*$ is the [[Complex Conjugate]]

Or in the general case:

$$
\braket{V,W}=\sum_i{v_i*w_i}
$$

## Uses
* Easy to calculate the length of a vector $v$: $|v|=\sqrt{v*v}$
* Angle between two vectors: $v*w=|v||w|cos\theta$

![[Pasted image 20210330102551.png]][[300 Inputs MOC]]
# Innovation
#outputs/thoughts 
The key when thinking about innovation is not to think about how to make something better, but rather to think about how to do something in a different way to avoid pain points entirely or change the experience to make it superior.
# Interesting quote from reddit
This goes well in hand with another article released this year (sorry I can't find the link to it) that said the biggest cause of procrastination is an inability to navigate or mitigate the negative emotions associated with doing a thing.

It also explains much of what we see in people presenting with ADHD.  Procrastination and a difficulty regulating emotions are two hallmark characteristics, which it increasingly seems are one in the same.

In people without executive impairment, it would make sense that mindfulness, which is the brain calling attention to itself, is much like a person consciously exercising the muscle of its executive function; analyzing and scrutinizing the signals coming from the various circuits and choosing one and muting others.

It also reminds me of a case study with a man who watched a violent movie and was then consumed with thoughts of murdering his girlfriend.  These thoughts consumed him and made him convinced he was evil or bad or wrong.

But after seeing a cognitive behavioral therapist, they made the conclusion that quote the contrary, it was because those thoughts disturbed him so much, and because he gave them so much weight and attention, that they recurred and disturbed him.

The reality is our brain is vast and full of a myriad of random thoughts and impulses, some dark, but our executive function is the switchboard that chooses what we think and what we disregard.  That is the reflection of who we are.

We have this fallacy wherein we think the deepest thoughts are the most real; that people who have private thoughts but do not act on them are *hiding' their true self; but nothing is less true.  It is who we choose to be and what we choose not to be and not to give weight to that is the best reflection of our self.

<!-- {BearID:DDF268F8-AAB4-4B34-9035-81F1E621D0C2-8340-000006C4678D89CD} -->
[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# Inteferometer
#concepts 
Made up of two [[Beam Splitter]] and two [[Mirror]] [[Quantum Gates]].

**Basic gate operations:**
$\ket{1}_P$  Vertical polarization

$\ket{H}_A$  Horizontal movement

**Basic entangled particles:** *(two elements of a single photon describe it's state)*
$\ket{\Psi_0} = \ket{+}_P\ket{H}_H$ -> $\frac{1}{\sqrt{2}}\ket{0}_P\ket{H}_A + \frac{1}{\sqrt{2}}\ket{1}_P\ket{V}_A$

This is what is used in the [[Quantum Bomb Game]].
![[Pasted image 20210329221007.png]]

For more on this see Q1 in [[CS333 PS5]]

# Interpretations of Quantum Mechanics
#concepts
This note collates some of the interesting interpretations of [[Quantum Mechanics TOC]] I’ve come across. The varying interpretations have a huge impact on our understanding of sub-atomic matter, and some indeed carry massive ramifications for our understanding of the truth of the universe.
See [[Bell’s Theorem]] for the origin or rather impetus for these varying interpretations

## Non-local hidden variables
#todo
## Transactional interpretation
#todo
## Superdeterminism
[[Superdeterminism]]
* The idea that the whole universe is in fact fully deterministic, and both future and past can be known in the present by evaluating current states. This undermines the concept of free will, and posits that our conceptions of free will are also “pre-planned” outcomes. VERY interesting how this idea interacts with concepts of religion and god. 
## Copenhagen Interpretation
[[Copenhagen Interpretation of Quantum Mechanics]]
* Currently the prevailing interpretation, and allegedly the most commonly taught. Core takeaway is that material objects do not have a defined state innately, rather they have a “wave function” of probabilistic outcomes. Upon observation the wave function collapses into the final value.
# Interviewing MS
[[Reading List TOC]]
- Always been entrepreneurial
	- Poker club
	- Paper airplanes
	- First company at 18
- International background
	- 3 Languages
	- 3 countries lived in 
	- 2 parent countries
- Corp finance class now was inspiring
- Technical prowess
	- Shohoz
	- Cloudcity
	- Now I want to apply my quick learning and hard working approaches to learning about financial markets and modeling etc

- Read their annual report and understand how they describe their business, approach etc. Compare IB and WM, and how many ppl work in each. % revenues and profits

- Search WM from a customers perspective- as a platform


## Questions for me
- How and where do you see your career development in the future.
	- CS Econ, aptitude for tech
	- An innovator
	- Capital is crucial though- need to understand it. Working for MS is the best way to learn about that and develop an analytical grounding for my future.
	- 

## Questions for them
- What is the relationship between WM and other parts of MS, how do they work together?
	- AM/ WM Overlap??
- What jobs are available?
- How many summer interns do they hire?
- Do they work together on a plan?

<!-- {BearID:0E39E633-B80F-433C-8B18-4877E865E9EF-9275-00000915794A1B3C} -->
# Intro to Bloomberg	
[[070 Finance MOC]]

Command line based software

Can search by stock tickers, seeing research etc

[Stock name] [Equity/ Corp] [keyword: FA (financial analysis), DES (description)]

Corp allows you to look at corporate bonds
Looking for single security functionalities:
- Load the security

## Relative Value
How do we compare two companies in the same industry?
RV - relative valuation
GX - graphical experience comparison
EQRV - equity relative valuation

## Equity Screening
EQS - add an indie, then start to narrow down
FTW - Factors to watch- most exposed to a factor are in Q1

Backtesting - Looking how a certain strategy or factor would have performed in the past

## Bloomberg Intelligence
Can learn about industries etc for doing good research
Can look by macro trends or through specific industry data.

## Excel Templates
XLTP - download excel templates that link back to bloomberg api

## Bloomberg Market Concepts
BMC command- gives nice run throughs on aspects of financial markets
HIGHLY RECOMMENDED

## Supply Chain Analysis
SPLC - Can see how much of suppliers revenues come from a certain equity. This is a very interesting deep layer of data, can see all the way through suppliers etc. Can be a good way to make cool plays
Can go all the way into who supplies suppliers etc, and where a company sells to as well.

<!-- {BearID:86D4F790-C758-4538-A45F-C2FAD8E7E1F6-303-00005EC7BE645D33} -->
[[006 School MOC]]
# Intro to Finance Class
#school/class

*INTD217*
## Deadlines:
	* *Fault lines essay* due April 8 in class
	* *Midterm 1,* March 18 in the lab; CH 4, 5, 6, 7, 9, 12, 13
	* *Equity Project*, April 29 by email
	* *Final exam*, May 6; CH 14, 15, 16, 17, 19, 20, 21 & 23
#### Office Hours:
	* Tuesday: 9.45 am – 10.45 am & 4.30 pm - 5.30 pm
	* Thursday: 9.45 am - 10.45 am & 4.30 pm - 5.30 pm

## General
	+ Email Gansbeke about homework structural issues.

## Fault Lines Paper
[[Fault Lines Plan]]
#### DUE April 8 in class
	+ Read Fault Lines Chapter 2
	+ Review requirements and grading
	+ Decide central theme for paper
	+ Figure out 3-5 points that support this theme with book references
	+ Write dat intro
	+ Wreck those content ‘graphs
	+ Finish up the content with a slick conclusion to tie it all together
	- Ensure (Chicago) references throughout

## Equity Project
#### Deadlines:
	* March 19 submit company names
	* April 29 final project due
	- Decide on equity project- Boeing??

<!-- {BearID:81405A7C-B2D2-4A9E-A4FA-1685F267620D-312-00000A33757D372A} -->
[[071 XCAP MOC]]
# Investment Contract Agreement
#projects/old/xcap
#outputs/legal
This agreement is made and entered into as of February 26, 2019, by and among:
Michael Calvey, Akhil Koppisetti, Carl Langaker, Wiatt Hinton

“The fund“, “Rise” or “we” shall refer to the Rise Capital Fund. “The shareholders” shall refer to all of the shareholders of the Rise Capital Fund. “I” shall refer to each shareholder, from the moment of the signing of this contract until its dissolution. Unless stated explicitly, all fund equity shall be subject to this agreement.

1. *Investment volatility* 
I accept that investments in equity markets, bond markets and other investments the group decides on are inherently volatile. I accept that I could lose a portion or all of my initial investment. Neither Rise Capital nor any of its shareholders shall be held liable for any investment losses the fund may incur.
2. *Distributions*
By contributing a sum of money to the fund, I acknowledge that I shall be entitled to whatever proportion of the fund that money makes up at market open on the day funds are received. This entitles me to a payout equivalent to that proportion of whatever the fund is worth at the moment divestment procedures are finalized. See the following point for divestment procedure.
3. *Dissolution:*
	a) *Divestment:*
I acknowledge that divestment is a complex, time consuming procedure. From the moment a divestment request is received, Rise Capital shall reserve the right to take the remainder of the current quarter, and one additional full quarter to finalize the divestment. This is to ensure that sudden divestments do not harm the long term strategy of the fund.
4. *Management and Control*
	a) *Investment Board:*
The top 4 Shareholders of the fund by equity percentage shall be entitled to a seat on the Investment Board. 3 further Board seats shall be determined by a vote of the Shareholders. No Shareholder can hold more than one Board seat. If a Shareholder does not want their board seat, they may forfeit it. It shall also be determined by a vote of the Shareholders, in addition to the other three. The investment board shall be charged with making all operational decisions, except as explicitly stated in this agreement. The Shareholders still have the right to overrule any action by the Board, if they pass it by a vote as defined below. The board will be responsible for ensuring the following:
	 	- Maintaining up-to-date records of all investments. This must contain an investment thesis, a summary of investment financials, and an update on how the current state of the company aligns with or contradicts the original investment thesis. In addition, they will ensure oversight over each position is delegated, including the maintenance of due  diligence on potential risks.
		- Assigning needed roles to the group. This could include analysts, researchers etc. One person each meeting to deliver a brief summary of macro events and changes. 
		- Carry out any operational decisions they deem necessary to pursue the fund mission, as drafted by the Board and ratified by the Shareholders.
	b) *Chief Investment Officer:*
One of the members of the investment board shall be elected by a majority vote of the shareholders of the fund to act as the Chief Investment Officer (CIO). This person shall manage, control, and oversee the business and affairs of the Fund as Chairman of the Investment Board without any further action or approval by the Shareholders or the Board, as subject to the following conditions. The CIO may be changed via a majority vote of the Shareholders. The CIO shall not have the power or authority to override any decisions made by either the Board or the Shareholders, as well as not having the power to carry out the following:
	* Orchestrate the purchase of a new position except for with the explicit approval of the Board.
	* Change the ownership of the Fund except as outlined in this agreement.
	* Make decisions that fall outside of the high-level strategy laid out by the Board and Shareholders.
	* Carry out any actions that contradict any motions passed by a majority of either the Board or Shareholders.
	c) *Books, Records and Reports*
In the absence of a CFO, the CIO shall take on the roles defined for the CFO hereafter: the CFO shall cause the Board to maintain the books, records and other documents as required by law. The CFO shall be responsible for providing the Shareholders with a complete annual report of all business activities. A financial report at the end of every quarter shall also be drafted by the CFO and made available to Shareholders. Any of these duties can be delegated, as long as their completion and quality are assured.
5. *Voting*
	a) *Shareholder Votes:*
Shareholders will be responsible for setting the long term strategy of the fund, as well as periodically reevaluating the performance of this strategy and altering it depending on market conditions. Any shareholder vote shall require at least 50% of the total shares to vote in favor of a proposal for it to pass, meaning the quorum for a shareholder vote is 50%. Any official meeting of the shareholders shall be announced at least 2 days before the time of the meeting to ensure attendance. If in-person attendance is not possible, voting rights can also be exercised remotely where the situation permits. 
	b) *Board Votes:*
A vote by the Board requires at least 5 members for a quorum. This can happen in person or remotely. Any measure taken to a vote needs majority support to pass. If there is a conflict between the result of a Shareholder vote and a Board vote, the Shareholder result shall be upheld. 



Signed,
Date 	  ___________________________


Signature _______________________________________________________________________

<!-- {BearID:4B390E8E-C686-4538-AC48-6E4020915C31-406-000002206E0190D7} -->
# Isaac Newton
<!-- #_reference/people #_reference/math #_reference/science -->

1642 - 1727

*Invented:* 
		* Laws of motion [[Newton’s Laws]]
		* Universal law of gravitation
		* Calculus

<!-- {BearID:43835C90-7FDA-4934-B6A6-38EC0CC6F9B6-3006-0000CAB61A0D3D28} -->
[[Religions]]
# Islam
#concepts
[[RELI254 Islam in South Asia]][[RELI254 Week 2]]
[[God]]

* Qur’an - Islam holy book
* Hadith - sayings of the prophet
* Fiqh - Muslim law



# Branches of Islam
### Sunni
Sunni Shia disagreement over who takes over leadership after Mohammed’s death - Sunni said elected leadership


### Shia
Shia said leadership should flow through family
Indian Shia have elaborate mourning processions


### Sufi
Can be considered a branch of Sunni/Shia, but the focus is different - more mystical
Not very commonly embraced, frequently criticized - but very effective with regular people


# Vernacular Islam
A less institutional day-to-day form
Very individualized
Gender roles and negotiations

<!-- {BearID:04E39A9A-A0BE-458D-8F64-05EE059E318D-375-0000069F9B2F0D05} -->
[[Standard Model TOC]] | [[052 Physics TOC]]
# Isotopes
#concepts 

Isotopes are versions of regular [[Elements]] that differ in the number of [[Neutrons]]. This does not affect the charge, but does affect the mass.

Isotopes tend to be rarer than their regular elements because they are often less stable (to the point of sometimes being radioactive). However, they are not usually more chemically reactive - they have the same number of [[Electrons]].

### Example - Hydrogen
Hydrogen atoms usually have one proton and no neutrons. 1 in 6000 hydrogen atoms has an extra neutron and is called *Deuterium*. 1 in $10^{-18}$ hydrogen atoms has 2 extra neutrons and is called *Tritium*. It is radioactive.

### Example - Uranium
Over 99% of uranium on earth has 92 [[Protons]] and 146 [[Neutrons]], a total of $92+146=238$ particles, called `U-238`. 0.7% of all uranium has only 143 neutrons, 3 less, called `U-235`. This rarer isotope is more useful for [[Chain reactions, nuclear reactors and atom bombs]] (Nuclear Power)[[Raps]]
I’m a trappa

Look at me yo
I’m a trappa
Got it down 
In wrapper
When u sit down on that crapper
Ya we know, u ain’t got it
think u got it? Naaah u bought it
...
Like your girl
Oh shit, what did I unfurl?
It’s a beef, it’s a shank, it’s a prime cut steak 
If u wanna know more, shit u Better hit my mail
Like a fan, through Iran
Oh shit don’t u stop my plan
Ha u can’t 
Nah u can’t
Even make ur girl cum

<!-- {BearID:266589D2-03F3-47DE-A97F-806768453902-469-0000017A0208CDC0} -->
[[030 People MOC]]
# James Clear
#people 

Clear wrote a book that has been quite influential to me on the formation and impact of [[014 Habits MOC|Habits]].

[[Clear - Atomic Habits]][[030 People MOC]]
# James Clerk Maxwell
#people 

Realized from several [[Magnetism]] and [[Electricity]] equations that [[Electromagnetism]] must exist, and that [[Light]] could be represented as [[Waves]].

Speed of light in a vacuum is given by the wave speed! [[030 People MOC]] | [[063 Economics TOC]]
# John Maynard Keynes
#people 

## Quotes
“The ideas of economists and political philosophers, both when they are right and when they are wrong, are more powerful than commonly understood. Indeed the world is ruled by little else. Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas.” - The General Theory
[[300 Inputs MOC]]
# [[Paul Johnson]] - Socrates, A Man For Our Times
#inputs/books 

My first real introduction to the life of [[Socrates]]. The interesting thing about Johnson is how he emphasises the existence of Socrates the man, in direct opposition to the frequently quoted mythical philosophical figure. This really brings Socrates down to earth and contextualizes him significantly.

This book was a recommendation from Papa---
aliases: ['KNN']
---
[[Machine Learning TOC|Machine Learning]] | [[ML Models TOC]]
# K Nearest Neighbors
#concepts/cs/ml 

KNN models figure out how to classify a point based on what the closest labelled points are. They tend to be quite powerful. "Use peer pressure to determine classification and regression predictions."

## Training
Hold onto all training points and associated labels or regression values.

## Prediction
For any new example x, find the k=3 closest examples in training data. Have their labels vote, most common vote wins and is kept for classification, or the average is taken for regression.

The difficulty is finding closest examples! Time and space complexity can get out of control with tons of examples.

### Calculating Distance
Calculating distance requires taking a stance on how to define distance. 
* **Euclidian Distance: ** The classic distance defined in pythag theorem. Generalizes to more dimensions easily: $c = \sqrt{a^2+b^2}$. Can think of this as similar to the residual sum of squares.
* **Hamming Distance: ** Very CS view - how many bits are flipped between two binary vectors? (Note must be ints & bools)

### Feature Scaling
Feature scaling can significantly help KNN, since it makes it's distance calculations more accurate.

## Advantages
- Leverages all your data
- Can learn arbitrary shapes - fairly intuitive

## Drawbacks
While KNN is generally very robust, it does suffer from several critical flaws. 
* Since it does not assign differing weights to different features, scaling matters and can change the results, unlike for linear models. 
* Random [[Features]] will destroy KNN performance, since the model is unable to downweight poor features. 
* Need to decide what distance *means*

## Pseudocode
```py
def predict_knn(D, K, x_hat):
	s = []
	for i in range(1, N):
		s.append((distance(x_i, x_hat), i))
	s.sort()
	y_hat = 0
	for k in range(1, K):
		(dist, n) = s[k]
		
```[[300 Inputs MOC]]
# [[Daniel Kahneman]] - Thinking Fast and Slow
#inputs/books


### Other PPL
[[Paul Slovic]]
[[Amos Tversky]]
[[Richard Thaler]]
[[Charlie Munger]]
[[Jack Knetsch]]
[[Cass Sunstein]]
[[Seymour Epstein]]

Very interesting book on the human decision making process. Kahneman takes an interesting approach where he starts by focusing his attention on the two decision making systems, [[Systems 1 and 2]]. He explains how [[System 1]] is fast, intuitive and automatic, and [[System 2]] is slow, deliberate and methodical.

Kahneman also introduces the idea (to me) of [[Attention]] in the process of decision making. Attention of [[System 2]] can be mobilized by [[System 1]].

Using attention causes [[Ego Depletion]]. This idea focuses on the fact that we have a finite amount of willpower/ ability to focus during a given timeperiod. 

[[Embodied Cognition]] - Lots of interesting takeaways on the physiological connection between mind and body - pretty clear that the mind experiences and understands through the lens of the body. 
Interesting thoughts on the role of [[Familiarity]] in the decision making process. Related to [[Cognitive Ease]]
[[Affect Heuristic]]

[[Associative Machine]] - another big theme in the book. Lots of unconscious bias as a a result of this.

[[Risk]] depends on [[Measurement Basis]] acc to [[Paul Slovic]].

[[Availbility Cascade]] - interesting idea closely tied to the [[Availability Heuristic]], where the importance of an idea is judged by the fluency and emotional charge with which it comes to mind. The availability cascade itself is a chain of events whereby innocent pandering to availability heuristic by for example news can flow into policy changes - a minor news story prompts concern, prompting further coverage etc until changes happen. [[Availability Entrepreneurs]] take advantage of this process and inadvertently accelerate it. 

[[Hope]] and [[Fear]] play a critical role in determining our true decision making process. These are two key elements for my [[Decision Machines]]

[[Rare Events]] are impossible for humans to gauge accurately. 
	[[Certainty Effect]]
	[[Possibility Effect]]
	
[[Narrative Self]]
[[Planning Fallacy]] - comparing against vague alternatives (success vs failure) where there  is one path to success but uncountably many paths to failure. Can't contrast.
[[Negativity Bias]]
[[Denominator Neglect]]
[[010 Mind MOC]] | [[011 Mental Models MOC]]
# Keynesian Beauty Contest
#concepts #concepts/mental_models #🌱 

The Keynesian Beauty Contest is a concept described by [[John Maynard Keynes]] to explain price fluctuations in equity markets. Keynes built an analogy based on a fictional newspaper contest, in which entrants are asked to choose the six most attractive faces in a hundred photographs. A naive contestant would simply respond with the six faces they found most attractive. A more sophisticated contest entrant, wishing to maximize their chances of winning ([[Incentives]]), would think about majority perceptions of beauty and make a selection based on those standards. This could be taken one step further to account for the fact that each contest has their own idea of what public perceptions of beauty are. The strategy can then be extended to increasing orders as desired.

Some commentators on this strategy have attempted to explain this phenomenon with the principle of [[Nash Equilibrium]].# LSE Application
#school
[[Econ Major Plan]]

- I love computer science and economics
- I have a very global outlook, and love to apply what I learn in a global context. 
- I am driven by a deep desire to explore the opportunities presented by a broad range of this world’s contexts.
- One of my biggest skills is thinking critically to find novel solutions to existing problems,

## Course Picks:
- FM300 corporate finance
- EH101 the internationalization of economic growth
- AC102 Elements of Financial Accounting
- AC103 Elements of Management Accounting, Financial Management and Financial Institutions 
- EC221 Priciples of Econometrics
- EC313 Industrial Economics
- EC315 International Economics

## Notes with paul monod:
- Need to fight for my own classes
- Would be helpful to talk about lse as a longwr term dream
- Talk to professors and push push push
- Very quantitative econ program- focus on math
- Emphasize math background in my statement
- Dont talk about wanting to be in london!
- Talk about how the courses which ive seen round out what Ive done
- Work experience definitely
- Motivated and driven
- Spring is the best time
- Politics and international relations prof, taught at midd: james morrison asst prof dept of international relations. International political economy, econ history. j.a.morrison@lse.ac.uk. Write to him and ask some questions. Go to his office hours!!!!
- 



## In a nutshell:
- Open with two sentences that clearly explain my main purpose
- Show first that this is my primary area of curricular focus
- Show second that I also love to apply these ideas in a broader context throughout my life, both locally at school and in totally different situations.
	- Tie into Shohoz experience in Bangladesh- need to sell it as the most exciting company in a country that is exploding with growth and development.
	- Bring this all to my eventual goal; to change peoples lives for the better by using technology in previously unexpected ways

## Mike’s thoughts
I need to tie everything into a coherent package. My academic interest needs to tie in clearly with my past experiences, and I should very plainly describe why the general course is a perfect fit for me. To do this, I need to have a clear selection of 6 courses that play well with my references and overall theme.


*Describe in approximately 500 words your academic interests and your unique purpose and objectives in undertaking the General Course. Your academic background, strengths and subject interests at LSE are of particular relevance to the selector.*

My curiosity is fired up by
I am inspired by the potential of combining computer science with economics.

I am deeply inspired by the opportunities created through the combination of technology and business. I am an economics and computer science double major at Middlebury College. Throughout my whole education, I have sought to pursue 







/During my time at Middlebury, I have been very focused in pursuing quantitive courses in both my majors as well as additional statistics, calculus and discrete maths./ 

I am a double Economics and Computer Science major in my second year at Middlebury College in Vermont. I find the nexus of these two disciplines fascinating, and have spent the last six years actively pursuing both in academic and extracurricular settings. My biggest skill throughout my life has been my ability to apply my abstract technical knowledge to practical, real world problems. This always translated into a strong ability to code and develop software, but when I was younger I did not actively pursue technical knowledge. When I got to Middlebury, I decided that I needed to supplement my quantitative knowledge, so since then I have been very focused on taking classes in discrete math, statistics and theoretical computer science and economics. I really enjoy applying abstract ideas to real world applications. Last summer, I spent two months working at one of the most exciting e-commerce startups in Bangladesh focusing on this exact idea; I was tasked with solving the issue of recurring fraud on the company’s ridesharing platform, so I created a new algorithm for sorting drivers after a request for a ride was made that effectively decreased the incidence of fraud on the system. I also applied my knowledge of graph theory to help create a tool to visualize and detect fraud as it was occurring. I now feel like the academically rigorous environment at LSE will be the perfect place for me to continue to push myself on both technical and theoretical levels.


/this allowed me to skip the first year of Computer Science classes with no prior academic experience. Since then, I have been actively trying to learn as much as possible in and out of class to/

<!-- {BearID:1B5996E7-EC45-4C25-BDEB-ECEB297F9547-312-0000B6F16AD68316} -->
# LSE Classes
[[006 School MOC]]
 

1. 
- *EC202* Micro II [EC201 Microeconomic Principles I](http://www.lse.ac.uk/resources/calendar/courseGuides/EC/2018_EC201.htm) 
2. 
- *EC315* International Economics
3. 
- *AC100* Elements of accounting and finance
- *MG303* international business strategy 
4.  
- *MG208* Business Transformation and Project Management
+ *FM300* Corporate Finance, Investments and Financial Markets

<!-- {BearID:DF7BE500-4CC0-4AEA-BBB3-CA35EEE6CC00-299-00000F878B7DB2D7} -->
[[Machine Learning TOC|Machine Learning]]
# Labels
#concepts 
ML labels are what we are trying to predict, or the actual y values. We use [[Machine Learning TOC|ML]] algorithms to generate predictions $\widehat{y}$.[[Linguistics MOC]]
# Language of Thought Theory
#concepts/linguistics 

This set of theories, at it's most simple, states that [[Thought]], like language, has an innate syntax. 

In part, the system seems to advocate for a sort of *mentalese*, an innate language used for higher-order cognition. LOTH also implies that the brain has some tacit knowledge of the rules of [[Inference]], [[Syntax]] and [[Semantics]]. 

I think this system sort of mirrors my view of cognition, which is that it is based upon representation and connection, with symbols attached to ease the process.---
aliases: ["Language"]
---
[[010 Mind MOC]] | [[011 Mental Models MOC]]
# Languages MOC
#concepts/mental_models #concepts/language

Language is one of the most important differentiators of the human species, full stop. Without the ability to think in abstract terms through a language, our thinking and rationalizing skills would be greatly diminished. This is especially true when taking into account things such as the [[Narrative Instinct]].

I need to learn heaps more about the confluence of reasoning, intelligence and language. They're clearly very closely linked. I'd also like to devote some minimal amount of time to studying [[Ancient Greek]], although it's pretty useless besides providing a foundation for definitions. 

The [[Brain]] is an inextricable component of understanding languages. There's been a lot of thinking throughout history about the relationship bewteen language and [[Thought]][[011 Mental Models MOC]]
# Law of Vocal Minority
#concepts #🌱 [[052 Physics TOC]]
# Laws of Thermodynamics
#concepts 


0. Objects in contact tend to reach the same temperature
1. Energy is conserved
2. Extracting useful energy from heat requires a temperature difference
3. The entropy of the universe is constantly increasing

<!-- {BearID:D7929B67-EAA0-42D8-AD82-6C82D18D8AAE-3006-00007898F863411B} -->
[[000 Life MOC]] 
# Layout Doc
#### //
Basic explanation of what is going on in this note

#### NAV

Hierarchical navigation, if such exists (eg for subfiles)
#### REF

References to overarching theme, tags etc
[[Machine Learning TOC|Machine Learning]]
# Learning Algorithms
#concepts/cs/ml 

There are several approaches to implementing an algorithm that helps "learn" based on input data.

## Backpropagation
This probably eventually deserves it's own page, but in short, this is the process used in conventional [[Neural Networks]] or [[Multi-Layer Perceptron]] systems to make them adjust weights based on error calculations. Summarized by the classic saying relating to [[Neurons]]: "Neurons that fire together wire together." In other words, trends are sought out and learned. Can think of this as optimizing a loss function. It used to be viewed as a mirror of neuron biology, however now is known to be not a very accurate model of learning.

## Predictive Coding
This one is super cool. Here's an article exerpt on how to think abt this biologically:
"Predictive coding posits that the brain is constantly making predictions about the causes of sensory inputs. The process involves hierarchical layers of neural processing. To produce a certain output, each layer has to predict the neural activity of the layer below. If the highest layer expects to see a face, it predicts the activity of the layer below that can justify this perception. The layer below makes similar predictions about what to expect from the one beneath it, and so on. The lowest layer makes predictions about actual sensory input — say, the photons falling on the retina. In this way, predictions flow from the higher layers down to the lower layers." - https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/

This article also raises the question of attention. I think this is hugely important, and low key what I've been going at with a lot of my [[Decision Machines]] thinking. This is really what the impulse input requirement, and discomfort minimization, are getting at. I think combining a predictive machine such as described above with constant inputs and an innate desire to minimize or maximize certain resulting "feelings", cognition may emerge. 

* Would also add that attention itself primes neurons to be more malleable or callable.
* Similarly when an animal does something better than expected it gets a flood of dopamine, reinforcing such action in the future.

# The Learning Process
The learning process generally is a vastly interesting question. I think a lot of more modern applications do not give the learning portion of cognition enough credit. Yes, it's in the name ML, but we don't put the same emphasis on teaching ML models as we do say children or even in some cases other animals. Obviously current models couldn't cope, but I think this is a major area for future exploration. 


[[010 Mind MOC]]
# Learning
#concepts 
[[051 Math TOC]] | [[Linear Algebra TOC]]
# Left Stochastic Matrices
#concepts 
Columns sum to 1, no negative entries.

Used in classical [[Probabilistic Computation]][[XCAP Writing]]
# Letter to the board
#projects/old/xcap 

Dear board members,

I want to first start by thanking you all for your time, effort and energy in making Rise the success it is today. It’s now been more than half a year since you all joined me in launching Rise 2.0, and while we aren’t nearly done, the progress has been awesome. Comparing our efficiency this year with prior years is very telling — in Q1 of this year, we made a gross $886 in trades. In Q2, that number jumped to $3504. So far in Q3, we’re at 3170, and the quarter’s barely halfway through. While we shouldn’t be trading for the sake of it, I feel like now we are actually starting to engage on a deeper level with our investments and taking this whole organization to the next level.

Looking to the future, we need to keep effecting this rebirth, or Rise, if you will, of our fund. I want to keep reevaluating our processes on a quarterly basis, revisit board structure and function and keep moving onwards and upwards. I know this is no easy task and I have been known at times to have a somewhat controlling personality, but I want this change to come from everyone. I want everyones’ input now as we revisit the initial mandate we set out for the board. Moving forward, if you all will ratify them, I think we should set out a short list of board priorities that we should all be working on constantly to elevate our business to the next level.

1. To keep leading as a group and figure out ways to increase group engagement.
2. To refine our operating processes to make things more efficient while still involving everyone that wants to be involved.
3. To continuously reevaluate board structure and organizational efficiency to ensure everyone has some stake or responsibility over the fund’s success, while learning about how to organize groups and coordinate on deliverables.

In the coming weeks, we will hold several conversations at the board level on this topic, as well as individual calls with me to get feedback from everyone about how people feel they have been involved, as well as how they would like to be involved. As it stands, a majority of the work output by this group is delivered by Carl. He’s doing an excellent job, but it isn’t just his priority - it’s all of ours’. This is the main reason I would like to reevaluate all of the work and content we are outputting as a group, and figure out where we can all be doing more. I have been trying to lead by example with this — I’ve started conducting a top-to-bottom financial review of our holdings over time, so we can build a more accurate picture of our performance over time. In terms of deliverables, I set up a new spreadsheet featuring a quarter-by-quarter breakdown of the fund’s value and transactions. However, this is just the start. I would like to have several documents we keep up to date, and have different people take responsibility for the completion and maintenance of each one. Let’s all brainstorm on this topic next time we all call together and keep the conversation going in the slack, but here are my main ideas:

## Quantitative
- Daily fund value (currently maintained by Carl)
- Fund inflows/ ownership (currently maintained by Carl)
- Quarterly performance tracker (maintained by me)
- Daily tracker of the value & volatility of each of our positions, with a different person for each position/ couple of positions. This will tie in with qualitative point #1.

## Qualitative
- Review of sale/ purchase timing - what did we do? Why? How was the timing? In hindsight why were we wrong, and what indicated that we may have been wrong at the time? What should we look at for the future.
- Explanation/ review of each positions’ performance.
- Recap and summary of ideas discussed during the quarter.
- Reevaluation of our investment thesis, looking at how well the recaps above fit in with the vision.
- Review of overall organizational effectiveness. How often did we meet? What did we do in meetings? Were we able to get things done? What did we do well? And badly? What should we look at for future?
- Quarterly review of inflows, ownership and trend in AUM. This should include a quantitative aspect that breaks down increases into unrealized investment returns, realized investment returns and inflows.

I think maintaining these deliverables will not only give everyone wanting to get more involved more stuff to dig into, but it will also revamp the educational value of our group. This will give everyone a better opportunity to learn about how this type of business is run, and will allow us to continuously evaluate our performance and learn from our mistakes — two core tenets of our fund.

To conclude I thank you all for undertaking this journey with me. Let’s go make bank, and keep growing our AUM! We’ve nearly doubled our total fund assets since the start of this year. Let’s keep the explosive growth going!

Yours,

Michael Calvey
Chairman of the Board

<!-- {BearID:CA658A83-8F76-4269-B431-F39CECF3A143-26685-0000388A33CAF27E} -->
[[010 Mind MOC]]
# Levels of Reading
#concepts #🌲

There are different ways to read things, and I should use the different methods at different times as appropriate. Initial thoughts:

* **Reading to Entertain:** This is like reading at elementary school level
* **Reading to Inform:** A superficial read, skim, dive in and out, get an overall feel
* **Reading to Understand:** The usual workhorse of reading. A thorough read, diving into things and digesting them.
* **Reading to Master:** Finding numerous sources on a topic, reading and digesting all of them, and forming an opinion while evaluating contradictions.

## Blank Sheet Method
* The best way to take notes on a book is to first start with a fresh note, and write down preconceptions/ what is already known
* When a reading session is done, add to the mindmap and correct/ connect ideas.
* Review the page before continuing with the topic.

This is a really great technique. It takes advantage of [[Priming]] to maximize retention and understanding.

## In-Text Annotation
I shouldn't shy away from in-text annotation. While reading, I should write in the margins. I've already started doing this, but I should do it more. At the end of each chapter, I should write a few bullet points of key takeaways as well as any unanswered questions. When I finish a book, I should write a brief summary on the inside cover. This summary can also be done here in obsidian.

[[Reading List TOC]]
# Liberation of Jerusalem

Seminal text written by Torquato Tasso

* Idealized account of the first crusade. Follows 3 powerful women, all start muslim, fall in love with Christian nights and adopt christianity. Two of the women are warriors, the other is a sorceress.
* Heavily inspired Spencer, Shakespeare etc.
* Overlap with [[Aristotle]] -> catharsis, epic, tragedy
	* Bends the truth from time to time - openly, in accordance with Aristotelian conceptions of plot

<!-- {BearID:1269EBF5-2786-4A79-B4B5-B16DE3777179-37181-00039ADF98ACD122} -->
---
aliases: ['EM Radiation', "Electromagnetic Radiation"]
---

[[052 Physics TOC]]
# Light
#concepts 
https://sites.google.com/a/coe.edu/principles-of-structural-chemistry/relationship-between-light-and-matter/electromagnetic-spectrum

Light is really a combination of an [[Electricity|Electric]] field and a [[Magnetism|Magnetic]] field into a transverse [[Waves]]. From the wave speed to frequency * wavelength equation, we can figure out that the frequency of a light wave is roughly a million per nanosecond, fast af boi. The frequency of light is equal to the number of bits per second that can be sent.

Light is also really [[Radiation]]. All particles above absolute 0 emit Radiation (some light), although mostly not in the visible frequencies.

Light travels at different speeds in different materials, depending on their *index*, lower index = faster light. This allows for functionality like sound channels, see [[Huygens Principle]]. This means light bends depending on say temperature. This is why mirages form on hot days. Light, like all waves, bends in the direction of slower travel.

In the eyes, [[Rod Cells]] and [[Cone Cells]] percieve light. Cone cells are more important for color perception.

Different frequencies of light have different wavelengths and thus speeds, and are interpreted as different colors if in the visible spectrum. Lower wavelengths have more watts per square meter. 

## Properties
[[Diffraction]]
[[Refraction]]

## Sun Energy
Energy emitted from the sun:
![[Pasted image 20210416113609.png]]---
aliases: ["Linear Algebra", "Linear"]
---
[[051 Math TOC]] | [[Statistics TOC]]
# Linear Algebra Bridge
#concepts #TOC 

[[Linear Regression]] 

#### Thoughts
Wow, this was my first attempt at organizing a concept using bear, back while I was still using that. Wild.

## Summary
This document lays out the basics of Matrix Algebra, mostly as paraphrased from the Rudiments of Matrix Algebra reference file.
An MxN matrix would have M rows and N columns, or in other words be M tall and N wide.
/note: this is one of the firsts notes I took using my new backlinking system!/ [[Daily Log 2020\/10\/14]]
[[Todo]] #todo Transcribe into LaTeX from bear image!

## Index
[[Matrices]]
[[Inner Products]] [[Tensor Products]]
[[Outer Products]]

## Matrix Types 
*Home:* [[Linear Algebra TOC]]
* *Row Vector:* A 1xN (wide) matrix
* *Column* An Nx1 (tall) matrix
* *Square Matrix:* Any MxM matrix
* *Diagonal Matrix:* A square matrix with at least one nonzero element in the diagonal and zeros in every other position
* *Scalar Matrix:* A diagonal matrix whose diagonal elements are all equal
* *Identity or Unit Matrix:* A diagonal matrix whose elements are all 1
* *Symmetric Matrix:* A matrix that is equivalent to its transpose, `A = A’`, or one that can be mirrored along the main diagonal
* *Null Matrix:* A matrix of 0s
* *Null Vector:* A row or column vector of 0s
* *Equal Matrices:* Same order, same items


## Matrix Operations
*Home:* [[Linear Algebra TOC]]

### Addition and Subtraction
Add matrices if they are of the same order (MxN and MxN), point by point.

### Transpose
Here a 3x2 matrix A becomes a 2x3 matrix A’:
[[Todo]] #todo Transcribe into Matlab from bear image!
* *If A and B conformable to addition, then C = A + B and C’ = (A + B)’ = A’ + B’*
* *(A’)’ = A*
* *If AB is defined, then (AB)’ = B’A’ and (ABCD)’ = D’C’B’A’*
* *Transpose of identity is identity, so I’ = I, same with scalars*
* *If A is square and A’ = A, A is a symmetric matrix*
### Scalar Multiplication
To multiply a matrix by a scalar just multiply each point by the scalar.

### Matrix Multiplication
Let A be an MxN matrix and B be an NxP matrix, then define AB to be the resulting matrix C of order MxP
To obtain the first item, sum the result of the first row of A multiplied by the first column of B, and so on.
A nice heuristic to use to remember how this works is that to obtain each value of C you will have to multiply N items, or:
*The row position of A and column position of B determine the row and column of the respective element in C*

*NOTE:*
	* *A row vector post multiplied by a column vector is a scalar* 
	Consider OLS residuals `[u1, u2, u3...]` so u is a column vector 1xN and u’ is a row vector Nx1,
	we have that `u'u = [u1^2 + u2^2 + u3^2...] = ei^2` , a scalar, the sum of residuals.
	See [[Linear Regression]] for more on OLS.
	* *A column vector post multiplied by a row vector is a matrix*
	* *A matrix postmultiplied by a column vector is a column vector*
	* *A row vector post multiplied by a matrix is a row vector*
	* *Matrix multiplication is not commutative, so AB != BA*
	* *Matrix multiplication is associative, so (AB)C = A(BC) where A is MxN, B is NxP and C is PxK*
	* *Matrix multiplication is distributive with respect to addition, so A(B+C) = AB + AC and (B+C)A = BA + CA*
	* If AB and BA both exist, may not be same order
	* Even if they are square so AB and BA both exist, resulting matrices will not necessarily be equal
	
### Matrix Inversion
An inverse matrix A, denoted A^-1 (read A inverse), if it exists, is a *unique square matrix* such that:
`AA^-1 = A^-1A = I`


## Determinants
*Home:* [[Linear Algebra TOC]]

### Evaluating Determinants
Funky cross-multiplied addition/ subtraction. For a 2x2 multiply main diag and subtract multiplied other diag.
See example:

### Properties of Determinants

### Rank of a Matrix
The rank of a matrix is the order of the largest square sub matrix whose determinant is not zero.

### Minor of a Matrix
If a point in a matrix is delated the determinant of the resulting submatrix is that point’s *minor*

### Cofactor
A cofactor is the determinant of a minor signed by (-1)^(i + j), where the minor is from the i’th row and j’th column.
	* The adjoint matrix is the transpose of the cofactor matrix so (adj A) = (cof A)’

### [[Inner Products]] 
The inner product of two matrices A and B is obtained by multiplying the complex conjugate of A by B, or $A\dagger B$. See [[Complex Numbers]]

### Outer Product
The outer product of two matrices A and B is obtained by multiplying $AA\dagger$

### Tensor Product
A special quantity used in [[Quantum Mechanics TOC]]. 
Think of this as a special form of matrix multiplication that multiplies each element of A by all of B.


## Finding the Inverse of a Square Matrix
*Home:* [[Linear Algebra TOC]]

*Steps:*
	1. Find the determinant of A, if it is nonzero, proceed to 2
	2. Replace each element aij of A by its cofactor to obtain the cofactor matrix
	3. Transpose the cofactor matrix to obtain the adjoint matrix
	4. Divide each element of the adjoint matrix by |A|
[[Machine Learning TOC]] | [[ML Models TOC]]
# Linear Models
#concepts/cs/ml 
Linear models are those that can be described by the function:

$f(x)=\vec{w}.\vec{X}+b$

Where the eventual classifier problem is:

$\vec{w}.\vec{X}+b>0?$

Where:
* $f(x)$ is the predictive model
* $\vec{w}$ is the weights vector, component of [[Hyperplanes]]
* $\vec{X}$ is the points vector.
* $b$ is the bias term

Really linear models' whole purpose is trying to find w and b.

They *cannot* solve the XOR problem

## Models
[[Linear Regression]]
[[Logistic Regressions]]
[[Perceptrons]] [[Averaged Perceptrons]]
[[Multinomial Naive Bayes]]
[[Coordinate Ascent]]
[[Stochastic Gradient Descent]]
[[Support Vector Machine]]

## Implementation
```py
@dataclass
class LinearModel:
    weights: np.ndarray  # note we can't specify this is 1-dimensional
    bias: float = 0.0

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        """ Compute the signed distance from the self.weights hyperplane. """
        (N, D) = X.shape
        assert self.weights.shape == (D, 1)
        # Matrix multiplication; sprinkle transpose and assert to get the shapes you want (or remember Linear Algebra)... or both!
        output = np.dot(self.weights.transpose(), X.transpose())
        assert output.shape == (1, N)
        return (output + self.bias).reshape((N,))

    def predict(self, X: np.ndarray) -> np.ndarray:
        """ Take whether the points are above or below our hyperplane as a prediction. """
        return self.decision_function(X) > 0

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """ Take predictions and compute accuracy. """
        y_hat = self.predict(X)
        return metrics.accuracy_score(np.asarray(y), y_hat)

    def compute_auc(self, X: np.ndarray, y: np.ndarray) -> float:
        """ Distance to hyperplane is used for AUC-style metrics. """
        return metrics.roc_auc_score(y, self.decision_function(X))
```# Linear Regression Class
<!-- #school/Linear-Regression -->
[[Todo]]
### Deadlines:
	- Midterm 1: Thursday 14th @7PM
### Office Hours:
	Tuesdays & Thursdays 10:15 - 10:50 AM and 2:00 PM - 2:45 PM

## General Todo
	+ HW Due Monday 3pm at his office
	- HW5 Due 20/3 in lab
	- Read CH5
	-

<!-- {BearID:0C78A855-644F-44B9-BE47-F1198DC0BD10-312-000009A91A00CB15} -->
# Linear Regression HW6
#school
```
      name:  <HW6>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw6.log
  log type:  text
 opened on:   1 Apr 2019, 13:28:35
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\profdummy.dta" 

. * Data summary
. gen lny = log(y)
. gen inter = x*dummy
. summ y lny x dummy inter
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
           y |        222    79.09747    23.87269       41.3        153
         lny |        222     4.32541    .3025113   3.720862   5.030438
           x |        222    18.01351    10.61224          1         45
       dummy |        222    .4504505    .4986632          0          1
       inter |        222    11.01351    13.27348          0         45
```
Analysis of data summaries:
We are working with 222 observations of data that relates university professor salaries and years of experience, as well as a dummy for male. Our first order of business is to generate variables for the log of income, since we can reason experience to have a decreasing impact on income. Our income data appears to be somewhat normally distributed, since the mean is $79,000 and the standard deviations are $23,900, so the max of $153,000 is approximately 3 standard deviations from the mean. The minimum value however is higher than we might expect for a perfect, unbiased normal distribution, since it is less than two standard deviations below the mean. The years of experience has a mean of 18 years and a standard deviation of 10.6 years, suggesting that we do not have a totally random sample of experience here either, there certainly again appears to be a bias for more experience, a reasonable conclusion given we are examining incomes of university professors.

Our dummy variable encodes whether a Professor is male or not, with 1 being male. The sample mean of 0.45 suggests that there are slightly more non-male Professors than there are male ones. As part of our analysis, we will attempt to statistically test whether dummy has an impact on the slope as well as the intercept of the predicted income, so we generate an interaction variable named inter, defined as the product of years of experience and dummy. 

Before we run our models, we shall use our economic intuition to describe what signs we expect to find on the coefficients. First, we expect Professors can always earn a salary, regardless of experience. This means our constant term should be positive. Next, we can generally expect more experienced Professors to be better at their jobs, resulting in increased salaries.   
```
. corr y lny x dummy inter
(obs=222)
             |        y      lny        x    dummy    inter
-------------+---------------------------------------------
           y |   1.0000
         lny |   0.9892   1.0000
           x |   0.6628   0.6922   1.0000
       dummy |   0.8426   0.8456   0.5504   1.0000
       inter |   0.8171   0.8106   0.7010   0.9185   1.0000
```
Our correlation results above suggest first that the log of income is closely correlated with all of our explanatory variables. The lowest appears to be years of experience at 0.69. We also support our earlier decision to take the log of income, as we can see the log of income is correlated more highly with years of experience than is income itself. Dummy and inter do not change significantly with the change to logarithmic space. 

Our multicollinearity results are low everywhere except for dummy and inter, which has a correlation of 0.9185. This can be easily explained since inter is generated from the dummy, and is 0 whenever the dummy is 0.
```
. * Model 1- baseline model
. * lnyi = B0 + B1xi + ei
. reg lny x
      Source |       SS           df       MS      Number of obs   =       222
-------------+----------------------------------   F(1, 220)       =    202.36
       Model |  9.68982947         1  9.68982947   Prob > F        =    0.0000
    Residual |  10.5345598       220  .047884363   R-squared       =    0.4791
-------------+----------------------------------   Adj R-squared   =    0.4767
       Total |  20.2243892       221  .091513074   Root MSE        =    .21882
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .0197313   .0013871    14.23   0.000     .0169976    .0224649
       _cons |    3.96998   .0289824   136.98   0.000     3.912862    4.027099
------------------------------------------------------------------------------
. predict lnyhat1
(option xb assumed; fitted values)
. gen yhat1 = exp(lnyhat1)-1
. scatter yhat1 y x, name(Model_1)
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |        222 -49.07187   23.32544       2   -42.65087  -35.84552
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
a) Our first model regresses the log of income against a Professor’s years of experience. Our results state that for every additional year of experience, we expect a Professor’s income to go up by 1.97%, with a standard error of 13.8 BPs. Our positive constant suggests that Professors with no experience still earn a salary. Statistically however, this model is not very strong in some regards. While it has perfect P values for both the overall F test and the individual t-tests, our R-squared value and our adjusted R-squared value are both quite low, indicating that approximately 48% of the variation in the log of income can be explained by years of experience alone. Our root MSE is fairly low at .22.

We can see the effects of this result in the attached Graph 1, where we see a close initial grouping of predicted and actual income, followed by a rapid divergence as some Professors receive substantially larger amounts in a smaller time period. The graph suggests that we are in fact capturing the basis of the relationship with years of experience being one of the most important factors of determination, however we can also see that we are missing a lot of information. It appears that the size of our error scales with the years of experience to a point, and then decreases as earnings rarely exceed around $135k, so beyond a point they tend to not increase.
```
. * Model 2- gender dummy, intercept
. * lnyi = B0 + B1xi + a0dummyi + e2i
. reg lny x dummy
      Source |       SS           df       MS      Number of obs   =       222
-------------+----------------------------------   F(2, 219)       =    408.94
       Model |  15.9528068         2  7.97640341   Prob > F        =    0.0000
    Residual |  4.27158243       219  .019504943   R-squared       =    0.7888
-------------+----------------------------------   Adj R-squared   =    0.7869
       Total |  20.2243892       221  .091513074   Root MSE        =    .13966
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .0092749   .0010603     8.75   0.000     .0071853    .0113646
       dummy |   .4043302   .0225641    17.92   0.000     .3598596    .4488008
       _cons |   3.976205   .0185006   214.92   0.000     3.939742    4.012667
------------------------------------------------------------------------------
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |        222 -49.07187   123.5226       3   -241.0451  -230.8371
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
. predict lnyhat2
(option xb assumed; fitted values)
. gen yhat2 = exp(lnyhat2)-1
. scatter yhat2 y x, name(Model_2)
```
b) Our second model includes a gender dummy that encodes whether a Professor is male, with 1 signifying male. Interestingly, the results are immediately very different from the first Model. Now, one extra year of experience lends only a 0.93% increase in salary, grouped tighter than before with a standard error of 10.6BPs. Our dummy coefficient of 0.404 suggests that being male leads to a 49.7% increase in salary, with a standard error of around 2%. Our constant value is roughly the same as before, again suggesting that Professors with no experience can still earn a positive salary.

Statistically, Model 2 is a significant improvement over model 1. While we see the same perfect P-values for the F and t tests, our root MSE decreases from .218 to .140, and both of our R squared values increase significantly from ~.48 to ~.79. This suggests that the addition of the gender dummy is very important for an accurate explanation of expected salary. Our actual F score has also doubled from the previous model, although the difference is negligible at this point.

Our graph for Model 2 supports our statistical and observational conclusions. We can see that the two trendiness are much closer to the data points on average. The graph raises the question of whether we should examine if slope is also affected by gender, since we seem to be slightly over predicting some values at the higher end of the male curve. This shall be examined in the next model.
```
. * Model 3- gender dummy, intercept and slope
. * lnyi = B0 + B1xi + a0dummyi + a1(dummyi * xi) + e3i
. reg lny x dummy inter
      Source |       SS           df       MS      Number of obs   =       222
-------------+----------------------------------   F(3, 218)       =    286.13
       Model |  16.1283823         3  5.37612743   Prob > F        =    0.0000
    Residual |  4.09600695       218  .018789023   R-squared       =    0.7975
-------------+----------------------------------   Adj R-squared   =    0.7947
       Total |  20.2243892       221  .091513074   Root MSE        =    .13707
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .0116128   .0012914     8.99   0.000     .0090675    .0141581
       dummy |   .5399403   .0495827    10.89   0.000     .4422174    .6376632
       inter |  -.0066663   .0021808    -3.06   0.003    -.0109644   -.0023683
       _cons |   3.946425   .0206061   191.52   0.000     3.905813    3.987038
------------------------------------------------------------------------------
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |        222 -49.07187   128.1814       4   -248.3629  -234.7522
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
. predict lnyhat3
(option xb assumed; fitted values)
. gen yhat3 = exp(lnyhat3)-1
. scatter yhat3 y x, name(Model_3)
```
c) Our final model is the same as model 2, except it also uses the inter variable to determine whether gender has an influence on the slope of the predicted regression, as well as the aforementioned impact on the intercept. Our results suggest that an increase in experience of 1 year leads to an increase of 11.6% in a professor’s salary, a slight increase from before that means that female professor’s salaries increase slightly faster, as they are the baseline. Then, our small negative slope of -.6% for the inter variable suggests that male professor’s salaries increase slower than the baseline. Our dummy increases from 0.4 to 0.54, meaning new male professors earn more to start with than their female counterparts, but then increase their salaries at a slower rate.

Statistically, this model is an improvement over model 2 in several ways. First, our R bar squared value increases from .786 to .795, a small but meaningful increase. Our p-value for the F test is still 0, and all of our t tests of individual significance are still well beyond our usual required level of 95% significance. Our new inter variable has a p value of .003, which is still significant to more than 99%. Our root MSE decreases to .137.

Finally, our graph 3 result also supports our statistical and observational analysis, as before. We see that the trend line for male professors more closely fits the upper portion of data than before. We can also see the increased baseline slope and decreased male slope clearly reflected in the diagram.
```
. * Special F-Test
. * RR = model 1, UR = model 3
. * H0: a0 = a1 = 0
. * HA: a0 or a1 =/= 0
. * h = 2
. * n = 222
. * kUR = 4
. * 95% critical F value: 3.04 
. gen special_f = ((10.5345598 - 4.09600695)/2)/(4.09600695/(222-4))
. di special_f
171.33815
. test inter dummy
 ( 1)  inter = 0
 ( 2)  dummy = 0
       F(  2,   218) =  171.34
            Prob > F =    0.0000
```
d) The extract above describes a special F-test that tests whether dummy, inter or both are statistically meaningful additions to our initial model. Our null hypothesis states that both of the coefficients on these two variables will be equal to 0. Our alternative states that at least one of them is nonzero. With this and the other information noted above, we can use our known equation for the special f-statistic to calculate that the value will be 171.34 with our current degrees of freedom. Using an f table, we can find the f-critical value to be 3.04. Hence we can easily reject the null at 95% significance, and in fact as the builtin stata test shows, we can see that the resultant p value is in fact 0. This result suggests that the addition of gender to our model is a statistically important inclusion, and that we will have suffered from an omitted variable bias in the first model.

e) For selecting a preferred model, we shall examine several selection criteria. First, we must note that the P values for our F statistics are 0 for each model, suggesting that overall they are all significant in explaining the variation seen in salaries, so we must go further. Next, we compare adjusted R squared values, and see model 3 having a slight advantage, with .794, as compared to .787 for model 2 and .477 for model 1. These results indicate that the inclusion of gender in some capacity is important, but the values for models 2 and 3 are too close to make a definitive choice. Finally, we compare the results from our Akaike’s Information Criterion (AIC) test. Model 1 had -42.65087, Model 2 had -241.0451 and Model 3 had -248.3629. These results support our observations from the adjusted R squared test, as we can see that the addition of gender has a very significant impact on our result. We can also see that Model 3 ends up slightly better with the lowest value by 8 points, so we can conclude that Model 3 is the best model for predicting Professor salary. 

This means that when considering salaries, it is integrally important to also consider how gender impacts both the rate of increase in salary with experience, as well as starting salary. Clearly, both are important to understanding Professor’s pay.

<!-- {BearID:BE0ABBD4-8021-403C-A474-4BFFE59BE516-314-00003C602B846182} -->
[[ECON211 Regression]]
# Linear Regression HW7											Michael Calvey
#school
a)
```
-------------------------------------------------------------------------------
      name:  <unnamed>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw7.log
  log type:  text
 opened on:  8 Apr 2019, 19:49:15
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\hw7.dta"
. summ q l k 
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
           q |         15        2418    159.6961       2110       2650
           l |         15    2385.467    92.18914       2230       2565
           k |         15    1717.333    330.2914       1150       2450
. corr q l k 
(obs=15)
             |        q        l        k
-------------+---------------------------
           q |   1.0000
           l |   0.9827   1.0000
           k |   0.9652   0.9925   1.0000
```
For this regression, we will be modeling the impacts of labor input *l* in worker-hours and capital *k* in machine hours on output *q* in tons. Our basic economic intuition tells us that both of these explanatory variables will be instrumental in determining output, and our summary statistics above seem to suggest the same, as we can see both labor and capital are highly positively correlated with output with correlations of 0.9827 and 0.9652 respectively. The data summary reveals that we are using a sample of 15 firms, with a pretty small range of outputs and inputs. Labor seems to vary  little among the companies, ranging from 2230 to 2565, while capital is more spread out, ranging from 1150 to 2450. The corresponding standard deviations of 92 and 330 support this. Our output varies from 2110 to 2650. All of the variables appear to be distributed fairly normally, with standard deviations and means that seem to match normal distributions for the data. 

The correlation statistics bring two main observations. First, we can see that both independent variables are highly correlated with the dependent variable. This means that we have selected good variables to use for this model. Second, however, we see an even higher degree of correlation among the independent variables, with a correlation of 0.9925 between labor and capital. This is the first sign that we might have to deal with multicollinearity.

For our first regression, we shall fit a Cobb-Douglas production function of the form:
Q=B0 * L^B1 * K^B2 * e^E
To do this, we shall use log rules to rewrite the function in linear terms:
lnQ = B0 + B1lnL + B2lnK + E
```
. * Model 1 - LogLog
. reg lnq lnl lnk
      Source |       SS           df       MS      Number of obs   =        15
-------------+----------------------------------   F(2, 12)        =    186.81
       Model |  .061682789         2  .030841395   Prob > F        =    0.0000
    Residual |  .001981101        12  .000165092   R-squared       =    0.9689
-------------+----------------------------------   Adj R-squared   =    0.9637
       Total |   .06366389        14  .004547421   Root MSE        =    .01285
------------------------------------------------------------------------------
         lnq |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         lnl |   .7575309   .7073246     1.07   0.305    -.7835969    2.298659
         lnk |    .188014    .138675     1.36   0.200    -.1141328    .4901608
       _cons |   .5006224   4.480004     0.11   0.913    -9.260468    10.26171
------------------------------------------------------------------------------
```
Here are the results from the model we described above. First, our B1 coefficient of .7575309 translates to an elasticity of 113.3%, meaning a 1% change in labor results in a 1.133% change in output. Since we are working in a LogLog space, we know our results are directly elasticities, though they must be translated back into linear space if we want to interpret them in numeric terms. Second, our B2 coefficient for ink is also slightly above our .15 interpretation cutoff, so we translate it to an elasticity of 20.68%, meaning a 1% increase in capital results in a 0.21068% increase in output. 

Statistically, this model suggests several interesting conclusions. Firstly, the overall significance is very high, as we can see the P value for the f statistic is 0. Further, our R squared is 0.9689 suggesting a very high degree of overall significance, supported by an adjusted R squared value of 0.9637. In terms of individual significances, however, the story is very different. 

For our B1, we test:
H0: B1 = 0
HA: B1 =/= 0
t = 1.07
With a resulting p value of 0.305, we cannot reject our null hypothesis that B1 = 0 with any reasonable degree of statistical accuracy.

For our B2, we test:
H0: B2 = 0
HA: B2 =/= 0
t = 1.36
With a resulting p value 0.2, we also cannot reject our null hypothesis.

These two results directly contradict the overall significance results, and our standard errors are likewise very high, barely lower than the coefficients for both B1 and B2. This is a clear indication of the presence of multicollinearity.

b)
```
. estat vif
    Variable |       VIF       1/VIF  
-------------+----------------------
         lnk |     63.69    0.015701
         lnl |     63.69    0.015701
-------------+----------------------
    Mean VIF |     63.69
```
First, we note that our overall significance is very high. The p value for our f statistic is 0, our RMSE is very low at 0.01285, and both our R squared and adjusted R squared suggest a very high degree of overall significance. This suggests that, dependent on other indicators, there is potential for multicollinearity.

Second, we note that our individual significances are very low, as enumerated explicitly in the answer for a) above. Not only this, but the standard errors for both B1 and B2 are almost as large as the coefficients themselves, further suggesting a very weak individual significance. In tandem with the prior observation about the high overall significance, our suspicions of multicollinearity are thus far supported.

Furthermore, we notice that the sample is quite small, and our observed correlation between labor and capital is higher than between either labor and output or capital and output. This degree of dependent variable correlation is a strong indicator of multicollinearity. 

Finally, we notice our VIF, as noted above, is 63.69, or significantly larger than 10, our selection criteria. The final touch involves reevaluating our model to ensure we cannot drop an explanatory variable; our economic knowledge tells us that both labor and capital are intrinsically involved in determining output, so we cannot drop a variable. This means all of the conditions for multicollinearity are in place, so we can conclude that it is very likely our capital and labor variables are multi collinear.

c) If multicollinearity is present, there are several possible consequences. Firstly, we know that our estimates will most likely remain unbiased, but the variances and standard errors of our estimates will increase. Because of these outcomes, we also know that computed t scores will fall, decreasing our individual significances. Additionally, our estimates will become very prone to changes in the specification. Finally, we can expect the overall fit of our equation to be largely unaffected.

d) If we know that our given production function might be characterized by CRS, we could rewrite our function to be in terms of constant output elasticities, preserving both explanatory variables but allowing us to look at them as a ratio, instead of as absolute values. This involves the use of some logarithmic laws. 

The preceding rearrangements show in the first 3 lines our initial conversion to LogLog, and in the last line the rearrangement to use constant output elasticities. If we change our regression specification to match this new description, we get the following model:
ln(Q/L) = B0+B1[ln(K/L)] + E

Here are the results of running this new regression:
```
. * Model 2
. gen ql = log(q/l)
. gen kl = log(k/l)
. reg ql kl
      Source |       SS           df       MS      Number of obs   =        15
-------------+----------------------------------   F(1, 13)        =     71.24
       Model |  .010865275         1  .010865275   Prob > F        =    0.0000
    Residual |  .001982587        13  .000152507   R-squared       =    0.8457
-------------+----------------------------------   Adj R-squared   =    0.8338
       Total |  .012847862        14  .000917704   Root MSE        =    .01235
------------------------------------------------------------------------------
          ql |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          kl |   .1749273   .0207244     8.44   0.000      .130155    .2196996
       _cons |   .0726307   .0078425     9.26   0.000     .0556879    .0895734
------------------------------------------------------------------------------
```
It appears that the multicollinearity has been eliminated. Without increasing the sample size, we now have much higher individual significance for both our new B1 than would be implied by our old B1 and B2, as well as a higher B0 significance. The p values for both are now 0. Clearly, given our new assumption of CRS, it is possible to rewrite our original model in a different functional form to essentially make it so that we have a single independent variable, eliminating the chance of multicollinearity.

e)
i) Pooled t test:
H0: B1 = 1
HA: B1 =/ 1
lnq = B0 + B1lnk
to = [Bhat1 - H0B1] / SE(Bhat1) = [.665] / .0175 = -38
tc = 2.131
|to| > |tc| so reject H0, CRS is statistically significant

ii) Chow/ Wald Special F test
RR: ln(Q/L) = B0+B1[ln(K/L)] + E
UR: lnQ = B0 + B1lnL + B2lnK + E
h=1
n=15
k^UR = 3
eRR = .001982587
eUR = .001981101
H0: B1 + B2 = 1
HA: B1 + B2 =/= 1

We can describe our F statistic as: 
so inputting our above values we get:
Fcalculated =  [(.001982587-.001981101)/1]/[.001981101/12] = [0.000001486] / [0.00016509175] = 0.009001055474
Fcritical = 3.17655
|F calculated| < |F critical| so we cannot reject the null of CRS, therefore the F test supports our prediction of CRS.

iii) Chi Squared test
Given the formula for the Chi calculated value is the same as the F value, except without the upper degree of freedom, and since our upper degree of freedom in the last test was 1, we can assume that our Chi calculated value will be the same in this case as for our F test, as we use:
RR: ln(Q/L) = B0+B1[ln(K/L)] + E
UR: lnQ = B0 + B1lnL + B2lnK + E
n=15
k^UR = 3
eRR = .001982587
eUR = .001981101
H0: B1 + B2 = 1
HA: B1 + B2 =/= 1

so our Chi calculated =  [(.001982587-.001981101)/1]/[.001981101/12] = [0.000001486] / [0.00016509175] = 0.009001055474
Chi critical = 21.03 
Chi critical > chi calculated, so we cannot reject our null hypothesis of CRS.

<!-- {BearID:02C6EDFF-45DC-4656-B78D-7DFBBF3974C2-530-0000237EBDE60ADD} -->
# Linear Regression Lab 2
[[ECON211 Regression]]
#school 
* TSS: Total Sum of Squares
* MSS/ESS: Model Sum of Squares
* RSS: Residual Sum of Squares
 
<!-- {BearID:2CB4A789-5E50-4578-B7BC-111EE76E9AAB-303-000009933B55F6AD} -->
[[051 Math TOC]] | [[Statistics TOC]] | [[Machine Learning TOC|Machine Learning]] | [[ML Models TOC]]
# Linear Regression
#concepts/cs/ml 

Not to be confused with [[Regression]], the problem type of predicting floats in [[Machine Learning TOC|ML]]. Linear Regression is a specific approach that can be used for [[Regression]] itself.

Linear regression is the idea that you can describe trends through linear combinations of coefficients paired with their respective variables
See [[Linear Algebra TOC]] for more info on notation and execution.

See also for the ML application of LinReg
[[Linguistics MOC]] | [[Sapir-Whorf Hypothesis]]
# Linguistic Determinism
#concepts/linguistics 

Linguistic determinism states that one's language inexorably shapes their ability to perceive the world. It is the strong form of the [[Sapir-Whorf Hypothesis]], and essentially claims that faculties such as [[Categorization]], [[Memory]] and [[Perception]] are inextricably linked to language. 

I don't agree with this much. I'm much more of a ground-up thinking ([[Biological Determinism]]) type of person, so I think there are elements of learned language that serve to enhance and magnify our capacity to think. I will go learn abt it now, but perhaps the [[Linguistic Relativity]] theory will have more resonance with me.[[Linguistics MOC]]
# Linguistic Relativity
#concepts/linguistics 

This is another side of the [[Sapir-Whorf Hypothesis|Whorfian]] coin (as opposed to the strong-form [[Linguistic Determinism]], essentially arguing that the structure of language affects the speaker's worldview or cognition. The weaker form of the whorfian hypothesis has held up better to empirics. 

---
aliases: ["Language", "Linguistics"]
---
[[010 Mind MOC]] | [[011 Mental Models MOC]]
# Linguistics MOC
#concepts/mental_models #concepts/linguistics 
#🌲 

Language allows us to express an infinite number of ideas with finite mental resources.

## Key components of linguistics
- [[Grammar]]: How language is structured, building [[Syntax]]
- [[Phonology]]
- [[Semantics]]
- [[Pragmatics]]
- [[Psycholinguistics]] - how language is processed in realtime
- [[Neurolinguistics]] - how language is computed in the brain.

## Key distinctions
- Language is not the same thing as [[Written Language]]. Furthermore, alphabetized language (where individual symbols stand for vowels and consonants used in speach) emerged only once in human [[020 Timeline MOC|History]], among the [[Canaanites]] about 3700 years ago.
- Humans have no instinctive tendency, but must learn to do so through learning and schooling. 
- Language is not the same as [[Thought]]. Much thinking possible wihtout language. Furthermore, language can be used to extend thought in interesting ways and add powerful dimensionality through the ability to build [[Abstraction]] and [[Relational Thinking]].
	- People constantly seek ways to express their thoughts.
	- And language can also significantly affect thought.
		- [[Sapir-Whorf Hypothesis]]

## Understanding language
- Interaction with [[Memory]] is quite interesting. 
	- Short term memory is quite verbal
	- Long term memory tends to focus on [[Semantics]], capturing the gist.

## How does language work?
### Components
1. [[Words]] - Building on the ideas of [[Ferdinand de Saussure]]. 
2. [[Grammar|Rules]] - recipes or algorithms to assemble bits of language into more complex ones, including [[Syntax]], [[Morphology]], [[Phonology]]
3. Interfaces - connecting language to the outside world.


Language is one of the most important differentiators of the human species, full stop. Wikipedia defines language as relying on [[Social Systems]] and [[Learning]]. Furthermore, language and [[Culture]] are codependent, and evolve together.
[[Narrative Instinct]].

I need to learn heaps more about the confluence of reasoning, intelligence and language. They're clearly very closely linked. I'd also like to devote some minimal amount of time to studying [[Ancient Greek]], although it's pretty useless besides providing a foundation for definitions. 

## Language and Thought
The [[Brain]] is an inextricable component of understanding languages. There's been a lot of thinking throughout history about the relationship bewteen language and [[Thought]].

Currently, there are two main theories on the link between these two concepts:
1. [[Sapir-Whorf Hypothesis]]
2. [[Language of Thought Theory]]# Logic Gates
#concepts 

#### HEAD
This document was created after seeing my relative deficiency with remembering specific logic gates and how they work.

#### REF
[[Quantum Cryptography]][[Quantum Mechanics TOC]]
<!-- #_reference/cs [[Reference Homepage]] -->


## XOR - Exclusive OR
Think of this operation as addition mod2!!

x	y	x(xor)y
0	0	0
0	1	1
1	0	1
1	1	0

<!-- {BearID:C3EC8C4D-EC5A-47B5-945A-916442312DC9-2534-0000F2156BD6BF3B} -->
[[Statistics TOC]] | [[ML Models TOC]] | [[Machine Learning TOC|Machine Learning]] | [[Linear Models]]
# Logistic Regressions
#🌱 #concepts/cs/ml 

Like Logit?# Looking for Jobs
<!-- #Jobs -->
- Charlies VC- Almaz
	- CPO of AWS!!!
	- Gridgain- using memory as storage!
	- Hover- 3d image creation
	- Zededa
		- “We define the box”

- Look at companies
- Look at advisors
	- Turing award winner!!!
	- Amazon, google etc!!!!!

charlesryan2@me.com

<!-- {BearID:505EF025-5CC8-48A2-96DB-F1FEA0932429-406-0000B0310E09ABE2} -->
[[011 Mental Models MOC|Mental Models]]
# Loss Aversion
#concepts 

People attribute roughly twice as much value to a loss as a proportional gain. This is one of the main [[Biologically Innate Human Tendencies]].

A lot of ideas like [[Prospect Theory]] are built out around this idea. # MA Cross Strategy
<!-- #finance/algo -->

[[Quantitative Finance]]
This is one of the first strategies I planned and executed, sort of a hello world.

Here’s a comparison of different MA windows for TCNNF:
 
Seemingly good values:
44, 19
Place a long trade every time fast MA crosses above slow MA. 
￼￼￼￼￼ 


And here’s one for CURLF: 

<!-- {BearID:7A7B1A7E-603F-46D7-A33B-193AB1D34097-87251-0004D6EDB425A2E4} -->
[[Machine Learning TOC|Machine Learning]]
# ML Models Bridge
#concepts 
#TOC 

## Linear Models
![[Linear Models#Models]]

## Non-linear Models
[[Neural Networks]] [[Multi-Layer Perceptron]]
[[Decision Trees]]
[[Random Forest]]
[[K Nearest Neighbors]]

## NLP-Specific
[[Bert Models]][[000 Life MOC]][[005 Active MOC]]
(Sub MOC)
# Title
A brief summary of the point of this note

[[007 Work MOC]]
# MS Getting Started
#work/ms 

# Training
Starts July 12
Ends August 27?
Catch up with team before things start!!!! Go get beers tho


## Knopman
support@knompan.com

- SIE 
- S7
- S64
- OPTIONAL: S65, FA exam

1. Read SIE book
2. Take 1 assessment exam
3. And one additional full length exam before start date# MS Internal Communications and Alignment Report
<!-- #work/ms -->
Examining how well WM—WM  and WM—Client communications go through
Factors to consider:
- Real time chat/ communication
- Information sharing/ portal
- Scheduling
- Video call/ remote meeting

3 main relationships to focus on enabling: 
	- WM—WM
	- WM—MS
	- WM—Client

# Week 1 Observations:

## WM — WM
- Coordinating client information

## WM — MS
- Obtaining good insights/ up to date info

## WM — Client

<!-- {BearID:8640DD82-09A3-42A3-8198-3308327B1EA7-855-000000F45A9B21C7} -->
# MS Recruiting Efforts
* *Link to share w midd kids w/ interest in internships:* [Summer 2022 Internship Interest Survey - Morgan Stanley Campus](https://morganstanley.tal.net/vx/lang-en-GB/candidate/postings/6954)
*

<!-- {BearID:AB44B7A8-8F8C-4466-87D5-F7ACA4A876A2-16983-0000A90C0723DEEC} -->
# MS Recruiting
[[007 Work MOC]]
## Gavin Gattuso
Freshman
Looking at WM
Thinking IPE
Interest in personal investing


## Atiyk Ahmeed
- Starting at Midd fall 21
- polisci/ consulting/ finance
	- wants to understand how things work, have an impact
- 

## Jacob Savitz
jsavitz@middlebury.edu
- Sophomore


## Dante Mayeno
(Non Midd)
Going for CM, has WM internship

## Adam Latif
(Non Midd)

## Ruiqi Xue
Midd Junior, Econ + psych
# MS big meets
Nate Stein
Sean Maher
Pat Natale
Emmett Simmons
Mark Lauricella
Jasper Huang
Tully Horne
Anthony DiBenedetto
Ben Eng

# MS stuff
Morning call: 
+44 2076 772 721
+1 (877) 777 8895
1290051#

mydesk.morganstanley.com
[[Todo]] #todo Transcribe into Matlab from bear image!

# MSWM Capital Markets Goals
<!-- #work/ms -->

3-5 Business goals:

1-2 professional development goals:


[[Todo]] #todo Transcribe into Matlab from bear image!---
aliases: ["ML", "Machine Learning"]
---
[[051 STEM TOC]] | [[053 Computer Science TOC]]
# Machine Learning TOC
#🌲
#concepts/cs/ml #MOC 
**Class: [[CS451 Machine Learning]]**

The study and design of computational systems that automatically improve their performance through experience, or somehow otherwise run an optimization function that allows them to predict outputs based on inputs.

# Summary
Building blocks:
* [[Features]] For an explanation of how feature sets work
* [[Labels]] For an explanation of feature labels
* [[ML Models TOC]] For links to the core models
* [[Linear Algebra TOC]] | [[Statistics TOC]]
* [[Regular Expressions]]
* [[Optimizers]]
* [[Learning Algorithms]]
* [[Bagging]]
* [[Boosting]]

# Applications
- [[Natural Language Processing TOC]]

## Hyperparameters
- What model?
- Learning rate?
- Max depth?
- "k" in [[K Nearest Neighbors]]
## Parameters
- Nodes and splits
- Learned weights
- Instances in a KNN tree

```
X - N * D matrix, training data
N - # of rows (examples)
D - # of columns (features)
y - labels
f - learned model, f(X) close to y
```
ML is a set of techniques that allow mapping of input matrices (features) to output (label) matrices.

* Attempts to make predicted outputs as close to actual label as possible.
* *Accuracy vs Error* - percentage of outcomes that match or don’t match labels
	* Think of this as pairs (y, y_hat) - when they differ, accuracy falls.

# Types of ML problems
* [[Binary Classification]]: yes/ no - like sorting whether review are good or bad, buy or sell etc.
* [[Multi-Class Classification]] - same as above but returns enum (yes/no/maybe)
* [[Regression]] - When we want to assign continuous numbers to input (f(x) -> float)
* [[Ranking]] - Prioritize or sort data
* Structured Prediction - Outputs are related in time and space, say labeling parts of speech or portions of a video that are a specific action. Makes predictions across time and space.

## Implementations
[[Linear Models]] -> described by $f(x) = w\odot x+b$ ([[Hyperplanes]])
* [[Decision Trees]]
* [[Perceptrons]]
* [[Logistic Regressions]] The ol’ logit model!!
* [[Linear Regression]]
* [[Multinomial Naive Bayes]]
* [[Stochastic Gradient Descent]]
* [[Coordinate Ascent]]

Non Linear Models:
* [[K Nearest Neighbors|KNN]]
* [[Random Forest]] <- A weighted voting by [[Decision Trees]]
* [[Neural Networks]] <- This is a neural net!! Also deep learning
* [[Unsupervised Learning]]
* [[Reinforcement Learning]]
* [[Kernels]]
* 
# Determining Success
#### Simple
- **Precision:** Of the things we marked yes, how many were yes? e.g. penalize false positives [[Type 1 Errors]]
- **Accuracy:** Of all our predictions, how many are correct? e.g/ penalize false negatives and false positives. 
- **Recall:** Of all the things we should have marked yes, how many did we get right - think of this as penalizing false negatives only. [[Type 2 Errors]]

Calling sklearn's `model.score()` generally returns the accuracy. 

#### Advanced
* [[ROC]] & [[AUC]]

# Loss functions
* This is used for training the model. 
* Takes an actual/predicted pair or pairset and outputs 

``` python
def customizable_error(y_actual, y_pred, loss_fn) -> float:
# sanity check input
assert(len(y_pred) == len(y_actual))
assert(len(y_pred) > 0)
# sum mistakes
error = 0.0
for pred, actual in zip(y_pred, y_actual):
    error += loss_fn(pred, actual)
return error / len(y_pred)
```
See how custom loss fns can be passed here!
* For classification can use zero/one loss, like a label
* For regression can use:
	* L1 loss: absolute difference
	* L2 loss: squared difference (resid)
* Can think of loss as the objective function!
* Loss is on a single item, error is on the whole array


# Train Test Validate
This is an approach to splitting up data. Split into 3 groups, largest for training then slightly smaller for test and validate. Then, train using training, and fit to maximize results on validation set. Then, once happy, test with test set to see actual success. Very nice!
- Training data: for learning parameters (weights)
- Validation data: for optimizing hyperparameters
- Test: For testing accuracy of model


ML models will overfit to a dataset to get 100% accuracy, but this will not generalize well. For this reason we split data up into these three categories.

# Feature Scaling
With some models (Not trees) it's tougher to find separating hyperplanes when two axes are not in similar relative scale. This happens because euclidian distance becomes less useful if different axes have different scales.

# Thoughts
* Cost of making a mistake vs cost of extra work?
* *Explainability Question:* a model that is predictable and understandable may be better than a slightly more accurate black box model.


## Terms
* Accuracy: the total number correct over the total number tested
* Precision: the total number correct of the ones predicted true
* AUC: random performance should be 0.5, the area under the true positive/ false positives curve
* Recall: the total number found of those that could be found
---
aliases: ["Magnetic"]
---
[[052 Physics TOC]]
# Magnetism
#concepts 

Magnetism responds to moving [[Electric Charge]], but is also created by them. See also [[Electricity]]. Can be thought of as a force that occurs between two [[Electric Current]].

Also related to [[Electrons]] spin. If electrons spin in the same direction, the charges add and magnetism is created.

Force is apparent in fields.

[[James Clerk Maxwell]] Was the first person to link magnetism to [[Electricity]].

## 3 Principles
1. Moving charges respond to magnetism.
2. Moving charges create magnetic fields.
3. A change in the amount of magnetic field

[[011 Mental Models MOC|Mental Models]]
# Man with Hammer
#concepts/mental_models #inputs/quotes 

To a man with a hammer, the whole world appears to be a nail. - [[Mark Twain]][[071 XCAP MOC]]
# Manekia Closing Letter
#projects/old/xcap 

## Acct Deets
Name: Amin Manekia 
Account with Citibank New York
111 Wall Street,  New York NY
Swift Code: CITIUS33XXX
Account number: 59883936


Dear Dan,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |            |
| -------------------- | ---------- |
| Amount Invested      | $3,000.00  |
| Shares Purchased     | 2280.22    |
| Cash-out Share Price | $2.16      |
| Cash-out Value       | $ 4,925.28 |
| Return on Investment | 64.2%      |


- Amount Invested: $3,000.00
- Shares Purchased: 2280.22
- Cash out share price: $2.16
- Return on Investment: 64.2%


Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL---
	aliases: ["MOC", "MOCs"]
---
[[000 Life MOC]] [[001 Meta MOC]]
# Map of Content
#concepts #concepts/definition

## MOC Definition
MOC, The highest level note, a *galaxy* of sorts, linking all kinds of *stars* and *constellations* together. In practice these translate into [[Evergreen Notes]] and [[TOCs]] The idea is to generally try to connect more TOCs rather than concepts themselves, but that is not a hard rule.

[[030 People MOC]]
# Mark Twain
#people 

# Markets in Profile - James Dalton
[[Reading List TOC]]

Very interesting quite complete analysis of financial markets.
* There is no single key driver behind the change we’re experiencing. Rather, a series of developments - some connected and some not - over the past 30 years have created the evolution that is now underway.
*
# Markov Chains
#concepts/cs/ml 

[[Notes to make]]
Markov chains are memoryless stochastic processes that allow simulation of nth probabilistic outcomes using just the current state and a transition function

Chains can be represented in [[Linear Algebra TOC]] form, multiplying a start state column vector (1 x n)

Read about baum welch algorithm!

<!-- {BearID:F2D887E7-7164-4C8A-85DA-A56C0D521F55-550-00000023161C26F0} -->
[[060 History TOC]]
#🌱 #concepts[[051 Math TOC]]
# Matrices
#concepts 

## Ways of representing matrices
[Very interesting ](https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams)

## Important Matrices

### Identity Matrix
$$
\begin{\pmatrix}1 & 0 & 0 \\ 0 & 1 & 0\end{\pmatrix}
$$

### X Gate
[[Quantum Gates]]

### Unitary Matrix
A matrix is unitary if $(U\dagger)\dagger=U$ which is the same as:
* U is unitary if $UU\dagger=I$
* If $U\dagger U = I$ 

### Left Stochastic Matrices
[[010 Mind MOC]]
# Meditation
#concepts

# Strategies
**Forced peace:
*** Count on in and out, or just out.
* Relax up and down body, each body part in turn. Take slow breaths and LITERALLY relax dem bish.

**Head to toe:**
* Focus on calming body, starting at forehead
* Descend slowly to shoulders
* Down and up arms
* Down torso
* Down and up legs
* Up torso
* Up to forehead
* Repeat if desired


**Patch Kosha - 5 bodies metitation**
Physical Body
Energy Body (breath body)
Mental body
Intuitive wisdom
Joy/ love
# Meeting notes 14/05/19
[[XCAP Meeting Minutes]]

### Proposal Structure
- Intro
- Data
- Summary
- Providing overview of main interactions with other companies and media 


## Target asset allocation
- Want to keep 10% in cash reserves over the medium term
- Keep roughly 4:3 ratio between innovation and safety plays

- Controlling risk

- Axels Climeon pitch
	- Energy for homes and corporations from waste heat
	- Diversification play
	- Deals with lots of general industry
	- Also doing thermal energy
	- Works around the world but not USA
	- *VOTE*: Unanimous $400 buy


- Agreed to continue gradual divestment of nvidia.
- Decided to hold but be careful of Boeing.

<!-- {BearID:AA01173D-68B4-4258-8E01-FCAF0AE92718-296-0000088EFB4FBB6A} -->
# Meeting plan 26/02
<!-- #finance/xcap/meetings -->
- Initial analyst positions
- Run people through legal agreement
- Explain how contribution works

<!-- {BearID:621B99E9-D18D-4D39-94FE-4FA87BC1DA6B-406-00000385BDC03830} -->
[[061 Philosophy TOC]]
# Metaphysics
#concepts #concepts/philosophy #🌱 

As I understand it, metaphysics pertains to the more abstract, perhaps unanswerable questions of [[000 Reality MOC|Reality]]. I tended to reject the subject by name in the past, but now upon more closely examining my own beliefs I am starting to doubt that rash decision. Metaphysics interests me greatly, almost moreso now that I've read about [[Socrates]] in [[Johnson - Socrates, A Man For Our Times]], where we see Socrates to a large extent reject the more abstract side as irrelevant to human fulfilment. Perhaps it may be irrelevant, but I think one of our greatest achievements as a species has been our increasing understanding of the workings and true nature of the universe. 

This perhaps relates closely to [[My Motto]], especially the first part, "How?"

[[006 School MOC]] | [[063 Economics TOC]]
# Micro Class
#school/class


- PS2 DUE Friday 5pm at 205 Farrell House
- Exam 7:30 in munroe 314,320
# Middvantage Interview
[[005 Active MOC]]
Looking at innovation -> Fintech, early stage ventures.
[Google Sheets - create and edit spreadsheets online, for free.](https://docs.google.com/spreadsheets/d/19K0AwxsD2rCDNPphHYzRfeR-HeYppZdXuGLyRMoFmOA/edit?ts=5f8dd747#gid=0)

Alumn: Fintech guy, applied in early stage venture and larger enterprises. Also international (LatAm)

Basic idea: (30 min)
	* Interview on subject matter:
		* Have alumni talk about Fintech, paint the landscape, career-focused (entry level jobs) -> first should give context for careers in Fintech.
			* “Tell me everything there is to know about the Fintech industry, geared towards students”
		*  Talk about your career path, from campus to today.
			* What was it
			* Not a railroad track
			* Advanced degree?
		* What advice would he have for current students interested in cracking into the Fintech space.
			* Startup mode.
			* Larger enterprise mode.
		* Other interesting element:
			* Fintech in the international development space
				* What’s the relevance? Microfinance?

<!-- {BearID:EEF9BE92-AB7F-4D52-AC47-59D66A0CBF7F-37181-00039510F49D97C3} -->
[[Machine Learning TOC]] | [[Natural Language Processing TOC|NLP]] | [[Linguistics MOC]]
# Minimum Edit Distance Problem
#concepts/cs/ml/nlp #concepts/linguistics 

This is a problem often used to determine the similarity between two strings, for example for spell correction. It has lots of pretty broad applicability.

## Definition
The minimum edit distance between two strings is the minimum number of editing operations:
- Insertion
- Deletion
- Substitution
Needed to transform `A` into `B`

- Calculate edit distance "cost"
	- In Levenshtein distance, insertions and deletions cost 1, but substitutions cost 2

## Examples
- [[Computational Biology]], comparing sequences of [[DNA]]
- Named entity extraction - are IBM Inc. and IBM the same?

## Solution
- Seeking path (sequence of edits) from start to final string
	- Initial state: word we're starting with
	- Operators: insert, delete, sub
	- Goal state: word we're trying to get to 
	- Path cost: what we're trying to minimize.

State of all sequences is huge! Can't do naively.
Lots of paths arrive at same state too.

So we define:
For two strings
- X of length n
- Y of length m

We define D(i, j)
- the edit distance between X[1..i] and Y[1..j]
	- ie the first *i* chars of X and the first *j* chars of Y
- The edit distance between X and Y is thus D(n, m)

## Implementation
Typically use [[Dynamic Programming]] to solve[[Statistics TOC]] | [[Machine Learning TOC|ML]]
# Model Confidence
#concepts/cs/ml 

We need a way to order data by the confidence of [[Classifiers]] to compute stats like [[ROC]] and [[AUC]].[[071 XCAP MOC]]
# Mohling Closing Letter

## Acct Deets
Sent via text screenshot (Annoying lol)



Dear Josh,

It has been an absolute pleasure having you as a shareholder and member of XCapital. We've done some amazing things and learned a huge amount along the way - I hope the experience has been positive for you as well. These three and a half years have gone quicker than I could have ever imagined, and it's sad that I now have to write this letter to you with the intention of dissolving the fund and returning everyone's money.

Let me first start with some of the highlights of the last 3.5 years. We started off with a seed investement of $4,000 at the start of 2018, gradually taking in a total of $16,929 to finish in May 2021 with $31,657 for a total return of 87%. This doesn't quite highlight just how far we came with our process during these three years. It's almost surprising we didn't lose more money when we were starting out - the inklings of ideas were good, but had a long way to go to being actionable.

We began our humble existence by gathering just over $4k to invest in a crypto mining rig. We were gonna run this thing in one of our dorms at school, and take advantage of the free electricity to make as much money as we could essentially for free, with only an initial fixed cost. Great idea, poor timing - as we were about to make our first investment, the crypto market crashed and everything went to shit. We decided we might as well stick with the idea of investing together, and try our luck in the stock market. Ironically, if we'd just bought up bitcoin at the time, we would have ended up with a similar return (but probably many more sleepless nights). 

In our first few days, we invested in BA, PYPL, TCEHY, NVDA and AMZN. While we ended up being right in our theses on most of these, our logic back then was so primitive it is almost surprising things worked out well. To me, that acts as a warning for the future - we never know what we don't know, even now. I urge everyone to avoid overconfidence - so much of these results are down to sheer luck through timing rather than skill, and it is impossible to tell after the fact what the cause of our success was.

What was really amazing about our group was the way our decision making and sourcing processes evolved over time. We built out a comprehensive due diligence process, incorporated fundamental data more deeply and created a systematic way of looking at stocks that we eventually converted into a quantitative trading system. Creating a consistent methodology for investing is perhaps the most important takeaway I have from my experience with XCAP, and I hope you take away the same.

Our most notable successes over these years were some of our tech plays like NVDA, PYPL and AMZN, our temporal plays during the pandemic taking advantage of dislocations in healthy businesses, and last but not least healthcare. We made a number of critical errors in thinking throughout this whole process, and I expect we will make many more in the future. 

As for your share of this growing pie, it is my pleasure to report the following:

| Investment Summary   |          |
| -------------------- | -------- |
| Amount Invested      | $ 450.00 |
| Shares Purchased     | 419.91   |
| Cash-out Share Price | $ 2.16   |
| Cash-out Value       | $ 907.00 |
| Return on Investment | 101.6%   |

- Amount Invested: $450.00
- Shares Purchased: 419.91
- Cash out share price: $2.16
- Cash out value: $907.00
- Return on Investment: 101.6%


Please let me know as soon as possible bank account information so I can arrange for the transfer of necessary funds. As I will be completing this transfer from internationally, I will need a swift code and IBAN.

Kind Regards,

Michael Calvey
Portfolio Manager, XCAPITAL[[005 Active MOC]]
# Monthly Cash Flows
[[080 Personal Finance MOC]]


* Alphavantage - $100
* Polygon - $200
* Apple 1 - $5
* Google 1 - $10
* Audible - $10
* ATT - $131

*MONTHLY REQUIREMENT:*
$456!

*Got:*
$3500

- April: $456
- May: $456
- June: $456
- July: $456

= 1824

= 1676 remaining! TOTAL!!![[095 Journals MOC]]
# Monthly Plan <% tp.file.title %>
#outputs/log

[[<%tp.date.now("YYYY-MM",-1)%>]] < + > [[<%tp.date.now("YYYY-MM",+1)%>]]

# Reflecting

## Last Month Reflection

# Planning


## Coming Month Plan


## Work That Needs Doing

# Tracking

## Notes Worked On
- [ ]


## Tasks
#todo 
- [ ]


## Thoughts
- # Morgan Stanley Notes
#work/ms

- The irrelevant investor - cool resource for blog style ideas

Difference between suitability and fiduciary:

Investment brokers and investment advisers not governed by same standards, investment advisers are seen as working for the client and so must place their interests first, while investment brokers work for broker-dealers and must only determine a recommendation is suitable for a client, not necessarily their best option, as defined by FINRA.

Suitable - broker dealer must believe all decisions will benefit the client

Fiduciary - duty of loyalty and care, placing client’s interests above ones own

[[Todo]] #todo Transcribe into Matlab from bear image![[Linguistics MOC]] | [[Natural Language Processing TOC|NLP]]
# Morphology
#concepts/cs/ml/nlp #concepts/linguistics 

Words are really constructed from **morphemes**, built from:
- *Stems:* the core meaning-bearing units
- *Affixes:* Bits and pieces that adhere to stems,
	- Usually serving grammatical functions

# [[Natural Language Processing TOC|NLP]]
In NLP applications, we typically start getting into morphological study of text after conducting [[Word Tokenization]] and then [[Token Normalization]].

## Stemming
The task of reducing affixes to get words to their stem. Historically used in [[Information Retrieval]]. It's like a stem-dependent version of Lemmatization. This allows us to get down to the truer morphology of a corpus.

Most commonly used stemming [[Algorithms TOC]] is Porter's algorithm, a 'simple' set of replacement rules.
- For example all "ing" and "ed" endings removed, unless they form a stem (where there is no vowel before, for example) 
	- `(*v*)ing -> ∅`# Movie list
* American Factory
*

<!-- {BearID:8088594F-5A53-40E2-93B5-97D248EA6257-13443-000007B09A098FF9} -->
[[060 History TOC]]
# Mughal Empire
#concepts 

Muslim group that took control of India for 600 years.

Mughal rule started in 14th century in the north
Peak was around 18th century
Then downfall into the 19th century

Aurangzeb was the last powerful Mughal ruler
	Was a muslim outsider who forced conversions to [[Islam]]

<!-- {BearID:5382DB6E-900E-49FE-9DFC-284C79CFAD4E-375-000006870996CF90} -->
[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]] | [[Qubits]]
# Multi Qubit Systems
#concepts 
Systems of multiple qubits can either be defined as **product** or **entangled**:
![[Entanglement#Definition]]

And are written as:
![[Quantum States#Multi-Qubit States]]

## Measuring multi-qubit systems
Consider rewriting bases to group terms you want to measure. Can conduct [[Partial Quantum Measurement]]

![[Pasted image 20210401100451.png]]

Then need to normalize!
(takes out 1/sqrt(2))
![[Pasted image 20210401100546.png]]

Now if measure AB:
![[Pasted image 20210401100808.png]]
Other outcomes also valid. Note C qubit will not collapse. 

We group all parts that can be measured together.[[053 Computer Science TOC]] | [[Machine Learning TOC]] | [[CS451 Machine Learning]]
# Multi-Class Classification
#concepts/cs/ml 


* Trying to put an example into one of a number of classes
See [[Neural Networks]]
#🌱 #concepts[[Machine Learning TOC|Machine Learning]]
# Multinomial Naive Bayes
#concepts #🌱

A Log Lin ~ [[Linear Models]]
As seen in the count of words practicals. w, as described in [[Linear Models]], is the counts vector.

Needs review![[Waves]]
# Music
#concepts

Music is really just sound waves. Notes are different frequencies of sound, and notes that differ by an octave have double or half the frequency.[[000 Life MOC]] | [[001 Meta MOC]] | [[010 Mind MOC]]
# My Forest
#concepts 

I'd like to think of all these notes I'm making as a garden. Only things I plant will be around, and I must tend regularly to the notes for the planting to be effective. Here are a few concepts I'm thinking fit with this metaphor and help establish my note taking. For some earlier thinking see [[Thoughts on note taking]]

## Frequent Traversals
I must navigate through the garden regularly, going over various sections and reminding myself of my knowledge and mental structure.

## Planting Seeds
[[Seedling Notes]] are seeds. I plant them, then tend to them to allow them to grow and connect with other notes, potentially graduating on to being [[Evergreen Notes]].

## Frequent Tending
My notes need updating as my thinking and understanding develops. I should use the traversals as opportunities for impromptu reflection, stopping to add things here and there that I see are outdated or incomplete. This helps the [[Evergreen Notes]] grow and link together over time.

# Forest
* [[Technology]]
* [[Seedling Notes]]
* [[Random Forest]]
* [[Quantum Mechanics TOC]]
* [[Orders of Reality]]
* [[Machine Learning TOC]]
* [[Levels of Reading]]
* [[Heuristics and Biases]]
* [[Feynman Learning Method]]
* [[Decision Machines]]
* [[Consciousness]]
* [[Collectivism]]
* [[Bubbles]]
* [[AI]]
* [[054 Neuroscience MOC]]
* [[020 Timeline MOC]]
* [[011 Mental Models MOC]]
* [[010 Mind MOC]]
* [[010 Me MOC]]
* [[005 Active MOC]]
* [[003 Process MOC]]
* [[000 Reality MOC]][[000 Life MOC]] | [[001 Meta MOC|Meta]] | [[010 Mind MOC]] | [[010 Me MOC|Me]] | [[061 Philosophy TOC]]
# My Motto

## How And Why
### "Quam et Quare"

Thoughts on it:
- I do fundamentally like these two questions. I think they encompass much of what I see as most important in life. They don't *fully* do so however, as they miss the remaining key ingredient, environment, primarily as manifested in [[030 People MOC|People]][[001 Meta MOC]]
# My Tag System
#🌲 

After skimming through [[Nick Milo]]'s latest LYT workshop (which I find generally helpful, if somewhat long-winded) I've decided to again coopt some of his organizational ideas to streamline the way I think in this vault. The main issue I've had is a ballooning number of mostly useless tags, that don't really serve a purpose.

I will now try to limit myself to the minimum number of parent tags, and use nesting to achieve a deeper organization. I think there'll be some clarity and power in this. 

## #concepts
This is the same as my old concepts tag, except now I'm nesting within it various fields or areas of concepts. I can use these fields as rough groupings, and generally I think there's no reason there can't be overlap.

## #school 
All the old school-related stuff goes here. Don't wanna get rid of it, but keeping things cleaner now.

## #todo 
I'm gonna keep this one I think, although I am tempted to create a better state system. What if I did:
1. #🌱 - the old todo?
2. # 🪴 - WIP? Potted plant, so in tending? Perhaps can replace the current #🌱 
3. #🌲 - the [[Evergreen Notes]]

Bit of an effort to change now, so for now it stays as todo, #🌱 #🌲 [[NMIH]]
# NMIH Aug 2020 call
#inputs/companies 

My Q:

Hi team, first of all a big congratulations on yet another successful quarter of outperformance. 

My question for you today is about the reliance of your future results on ongoing governmental support. Obviously forbearance programs have had a significant impact on performance thus far throughout the pandemic, so could you add some additional color to how you're thinking about these plans going forward and specifically managing the risks involved in the transition away from the current aid-heavy environment? Thank you
- Expecting more policy response
- Don't think borrowers will be cut loose
	- Soft landing
	- Taking advantage of equity
- More support expected - eviction moratorium speaks to this (altho it's)
- Anchoring for additional reserves and not planning for additional reserves.
	- 
- But some churn will be expected

- mkt share?
	- Not focused on it, focusing on lenders & borrowers & profitable growth
- 97% of insured loans bought by GSEs
	- Quality of underwriting standards has remained high


Why is home appraisal so hard?!?! (at least specifically for the purpose of cancelling MI)[[070 Finance MOC]] | [[Company Research]]
# NMIH

[[XCAP 3Q20 NMIH]]
[[Residential Mortgages]]
[[ECON431 Final Paper]] - a study where I try to predict mortgage losses before they happen.

* What is the chance of people refinancing?
*

## Earnings Calls
[[NMIH Aug 2020 call]][[Human Instincts]]
# Narrative Instinct
#concepts/instincts

The human narrative instinct encourages us strongly to build causal stories explaining phenomena around us. This is a result of numerous qualities of the brain such as the [[Commitment and Consistency Instinct]], [[Inherent Laziness Instinct]] and more generally the [[Brain]] emphasizing patterns over chaos.---
aliases: ["NLP"]
---
[[053 Computer Science TOC]] | [[Machine Learning TOC]]
# Natural Language Processing
#concepts/cs/ml 

The application of ML to [[Linguistics MOC|Language]] problems.

## Applications
- [[Sentiment Analysis]]
- [[Information Retrieval]]
- [[Minimum Edit Distance Problem]]

## Tools
- [[Phrase Structure Parse]] - understanding [[Grammar]]
- Knowledge about language
- knowledge about world
- And a way to combine these

## Text Processing
[[Regular Expressions]]

## [[Word Tokenization]]
Word tokenization is the task of chunking words in running text, normalizing word formats and segmenting sentences in running text.
- [[Words]] can be defined in different ways as distinct.
- [[Sentence Segmentation]][[Machine Learning TOC]] | [[ML Models TOC]]
# Neural Networks
#concepts

[[Multi-Layer Perceptron]]
NNs are essentially groups of [[Perceptrons]] models that vote to create an output. They are more powerful than an individual Perceptron, and generalize much better.

In reality you would have layers of nodes, with a function added between layers that makes the model non-linear. 

$y_p=M_1 * h(M_2*X)$

Modern NN's are built out of blocks, using a library like tensorflow or PyTorch. They allow you to combine layers into a model. They also come with many *loss functions* and *optimizers* that define the loss and then figure out how to work with it.

## Types of Layers
There are many types of layers that can be applied to an NN:
* RNN: Recurrent NN, used to process variable input length sequences.
* CNN: Convolutional NNs, taking a small number of weights and using them more than once, making learning quicker.
* Attention: OMG SEE [[Decision Machines]], like a fancy weighterd average
* MaxPool: Max activation 
* AvgPool: Avg activation

## Unsupervised Learning
[[Unsupervised Learning]] is when you don't provide [[Labels]] for features, but rather try to extract insights from the data. NNs can be used for this, similar to [[K Nearest Neighbors|KNN]]

#todo:
- [ ] Transcribe code snippets implementing an NN here! From our lab!

## Integration with [[Reinforcement Learning]]
Very interesting line of inquiry. I should pursue/ try to implement further. I need to study RL more deeply first as well. 
This should be top of mind for next learning steps.

Look at Deep-Q Network (DQN). It allows experience replay, enabling the model to learn offline.
The replay buffer functions like a basic [[Hippocampus]], which I need to learn about.

## Elastic Weight Consolidation
Acts by slowing down learning in a subset of the network once good solutions to problems are found. Helps solve the problem of continuous learning and enables increasingly efficient use of the network. Weights become shared between tasks with similar structures. Perhaps some sort of intentional culling/ optimization system might be useful in this regard to avoid local maxima?


## Biological Metaphor
See [[Neurons]] The biological metaphor of neuron action is very interesting. We now know that simple NNs are imperfect abstractions of the way neural systems learn. Check [this](https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/) out for more.

Apparently in recent years this metaphor has been extended, and there is renewed interest in researching in this direction. This excites me, because I feel like I saw some clear drawbacks in the structure of these neural systems that could be solved by applying a more biological lens to the topic. 

See [[Hassabis - Neuroscience Inspired AI]] for tons more on the parallels here. What's really interesting is that NN system functionality draws heavily on the [[Visual Cortex]]

# SEE MORE
- Nonlinear Transduction
- Divisive Normalization
- Maximum-based pooling of inputs---
aliases: ["Neural"]
---
[[010 Mind MOC]] | [[051 STEM TOC]] | [[054 Biology MOC]]
# Neurons
#concepts/biology

## Place Cells
Fire whenever a person thinks about or associates with a certain place. I'd bet these work for concepts as well.---
aliases: ["Neutron"]
---

[[Standard Model TOC]] | [[052 Physics TOC]]
# Neutrons
#concepts 

Subatomic particle making up the [[Nucleus]] of [[Atoms]].
We posit neutrons consist of [[Quarks]]

No electric charge 

Similar in mass to protons# New vision
[[071 XCAP MOC]]
* Food tech
* Medecine
* Use demography- changing tastes from aging population

<!-- {BearID:7043D697-FD65-4A45-A5CD-EB5577F85292-3124-00000258CD5A0B57} -->
[[052 Physics TOC]]
# Newton’s Laws
[[Forces]]

Newton’s laws are examples of [[Classical Mechanics]]

## Laws of Motion
1. Objects remain at rest or move in a straight line at constant speed unless acted upon by a net external force
2. The total force on an object equals the product of the mass and acceleration
`F = ma`
3. Every force has an equal and opposite reaction


## Law of gravitation
`F = GMm/r^2`
where:
	* F: Force
	* G: Gravity
	* M: Object 1 mass
	* m: Object 2 mass
	* r: center to center distance

<!-- {BearID:911CAF4B-9E64-47AF-9571-2E7E6243BF1E-3006-0000CADD8E769630} -->
# Next steps
[[071 XCAP MOC]]

Want more contribution, more positive ideas from positive minds.

## New titles:
#### Governance:
Portfolio Manager
Director of Finance
Director of Accounting
Director of Innovation
Director of Marketing
Director of Risk
Director of Macro Strategy
Director of Internal Compliance
Director of Internal Operations

Senior Strategist
Investment Specialist
Sector Analyst
Position Analyst
Public Policy Analyst


All companies:

- BA
- NVDA

- MA
- PYPL

- CURLF

- FB
- MSFT
- AMZN

- TERP
- VRRM

- SPX







By Sector:

<!-- {BearID:661F91BB-FE93-4002-BF88-E57943E540EF-26685-00003FDB84EFA5D4} -->
[[030 People MOC]]
# Noam Chomsky
#people 

## Pioneering ideas
1. Focus on productivity - creation of novel meaning using [[Grammar]] and [[Semantics]].
2. Languages have [[Syntax]] unrelated to meaning

Chomsky pioneered the idea of the puzzle of productivity or creativity in the field of [[Linguistics MOC|Linguistics]], or people's ability to produce and understand new sentences. Most sentences are novel, and it is really hard to parse and understand them. 

[[Steven Pinker]] often references this guy.

## Common Critiques
- Universal grammar is not necessarily specific to language, could be more general to life.
- Chomsky only studied a small number of languages. No empiricism
- Chomsky never showed that more general-purpose models such as [[Neural Networks]] can't learn language in the same way, so innateness isn't necessarily required.[[053 Computer Science TOC]]
# Nondeterministic Polynomial Time
#concepts/cs

[[Complexity Classes]]
The set of problems solvable by a NTM in polynomial time, or O(|x|^k) time, where x is the input. For more on this see [Turing Machines - Time bounded computation](bear://x-callback-url/open-note?id=148A123B-B831-4683-A9B7-B902DDE5742B-5857-00012BA642B359C9&header=Time%20bounded%20computation)

These problems can be solved by “guessing” a solution and verifying whether it is correct in polynomial time.

## Examples:
	* Satisfiability of Boolean formulas, or whether a certain assignment of boolean variables leads to a statement being true.

<!-- {BearID:A851A6D6-D5FB-4AEB-9A8A-808132EF41C1-5857-00014B351D0C090A} -->
[[053 Computer Science TOC]]
# Normal Forms
#concepts/cs

All productions of a [Grammar](bear://x-callback-url/open-note?id=579EBF80-C128-4B22-A3BE-966A9C48EA0A-37181-00039465005E526C) must be the same pattern/ form.

I assume here that E is Epsilon, the empty string

Helpful for parsing algorithms, as well as proving properties of derivations.
Why might we use these: 
	* algos need certain input format. 
	* No useless symbols
For example the cky algo, which requires a CNF input

## Chomsky Normal Form (CNF):
*Uses:* better for algos and manipulation
All productions of form 
A -> BC 		or		A -> a
for A,B,C in N and a in Sigma.
[nonterminals]		[terminals]

### Converting grammar G=(N, Sigma, P, S) to CNFs:
	1. Remove E-transitions A -> E, A in N
	2. Remove unit productions A -> B, for A, B in N
	3. Remove useless symbols & productions
	4. Convert to CNF:
		1. Make righthand sides nonterminals unless production is of form A -> a
		2. Split RHS of productions A -> B1… Bk for k > 2 so they are in CNF
 
	 * Removing means revising the grammar so these aren’t needed to describe the same set of productions

#### To remove E-transitions:
If A -> BCD and C -> E then
	1. Add production A -> BD
	2. Delete production B -> E

#### To remove unit productions:
Replace each production A -> B with A -> C for each production B -> C

#### To remove useless symbols & productions:
	1. Remove nonterminals/ productions that don’t derive strings
	2. Remove nonterminals/ productions not reachable from S


## Greibach Normal Form (GNF):
*Uses:* good for parsing
All beginning symbols match rhs
All productions of the form A -> a B1 B2… Bk for k >=0,
A, B1, B2… Bk in N and a in Sigma.
Note that k=0 allows productions of form A -> a

### To convert CNF to GNF:
Loop until grammar in GNF:
1. If A -> BC where productions for B are already processed
	1. Delete the production A -> BC
	2. Add productions A -> DC for each production B -> D
2. If A -> AB and A -> C where C does not begin with A:
	1. Delete A -> AB
	2. Add new nonterminal D and productions:
		1. A -> CD
		2. D -> B | BD
		3. Keep A -> C

*ALL CFGs CAN BE CONVERTED TO CNF AND GNF*

<!-- {BearID:35505C3B-3803-442C-B7A7-7CEEF2A07BDC-37181-0003945AE9430147} -->
# Notes for Micro Final MiniEssay
[[006 School MOC]]
[The Birth of the New American Aristocracy - The Atlantic](https://www.theatlantic.com/magazine/archive/2018/06/the-birth-of-a-new-american-aristocracy/559130/)

We may have studied Shakespeare on the way to law school, but we have little sense for the tragic possibilities of life.

Obesity, diabetes, heart disease, kidney disease, and liver disease are all  [two to three times more common](https://www.urban.org/sites/default/files/publication/49116/2000178-How-are-Income-and-Wealth-Linked-to-Health-and-Longevity.pdf)  in individuals who have a family income of less than $35,000 than in those who have a family income greater than $100,000. 

We also have more friends—the kind of friends who will introduce us to new clients or line up great internships for our kids.

These special forms of wealth offer the further advantages that they are both harder to emulate and safer to brag about than high income alone. Our class walks around in the jeans and T‑shirts inherited from our supposedly humble beginnings. We prefer to signal our status by talking about our organically nourished bodies, the awe-inspiring feats of our offspring, and the ecological correctness of our neighborhoods. We have figured out how to launder our money through higher virtues.

the skin colors of the nation’s elite student bodies are more varied now, as are their genders, but their financial bones have calcified over the past 30 years.

In the United States, the premium that college graduates earn over their non-college-educated peers in young adulthood exceeds 70 percent. The return on education is 50 percent higher than what it was in 1950, and is significantly higher than the rate in every other developed country.

Behind both of these stories lies one of the founding myths of our meritocracy. One way or the other, we tell ourselves, the rising education premium is a direct function of the rising value of meritorious people in a modern economy. That is, not only do the meritorious get ahead, but the rewards we receive are in direct proportion to our merit.

why do america’s doctors make twice as much as those of other wealthy countries? Given that the United States has placed dead last five times running in the Commonwealth Fund’s ranking of health-care systems in high-income countries, it’s hard to argue that they are twice as gifted at saving lives

By now we’re thankfully done with the tech-sector fairy tales in which whip-smart cowboys innovate the heck out of a stodgy status quo. The reality is that five monster companies—you know the names—are worth about $3.5 trillion combined, and represent more than 40 percent of the market capital on the nasdaq stock exchange. Much of the rest of the technology sector consists of virtual entities waiting patiently to feed themselves to these beasts.
Let’s face it: This is Monopoly money with a smiley emoji. Our society figured out some time ago how to deal with companies that attempt to corner the market on viscous substances like oil. We don’t yet know what to do with the monopolies that arise out of networks and scale effects in the information marketplace. Until we do, the excess profits will stick to those who manage to get closest to the information honeypot. You can be sure that these people will have a great deal of merit.

It has been engineered, over decades, by powerful bankers, for their own benefit and for that of their posterity.

It isn’t a coincidence that the education premium surged during the same years that membership in trade unions collapsed. In 1954, 28 percent of all workers were members of trade unions, but by 2017 that figure was down to 11 percent.

They’re called “tax breaks,” but it’s better to think of them as handouts that spare the government the inconvenience of collecting the money in the first place. In theory, tax expenditures can be used to support any number of worthy social purposes, and a few of them, such as the earned income-tax credit, do actually go to those with a lower income.

And—such is the beauty of the system—51 percent of those handouts went to the top quintile of earners, and 39 percent to the top decile.

With localized wealth comes localized political power, and not just of the kind that shows up in voting booths. Which brings us back to the depopulation paradox. Given the social and cultural capital that flows through wealthy neighborhoods, is it any wonder that we can defend our turf in the zoning wars? We have lots of ways to make that sound public-spirited. It’s all about saving the local environment, preserving the historic character of the neighborhood, and avoiding overcrowding. In reality, it’s about hoarding power and opportunity inside the walls of our own castles. This is what aristocracies do.

But that is not to let the 9.9 percent off the hook. We may not be the ones funding the race-baiting, but we are the ones hoarding the opportunities of daily life. We are the staff that runs the machine that funnels resources from the 90 percent to the 0.1 percent.

Where were the 90 percent during these acts of plunder? An appreciable number of them could be found at Ku Klux Klan rallies. And as far as the most vocal (though not necessarily the largest) part of the 90 percent was concerned, America’s biggest problems were all due to the mooching hordes of immigrants. You know, the immigrants whose grandchildren have come to believe that America’s biggest problems now are all due to the mooching hordes of immigrants.

<!-- {BearID:93286C5C-2B71-4C0C-8FB2-D6BE5962513E-1155-00000FDAE309FD9E} -->
[[200 Outputs MOC]]
# Notes from papa
## Presentation
Wealth creation by entrepreneurs in tech related sector, high bar for digital
Cartoons/ human elts
- FA dealing with clients while trainer gets in the way
KPI - leveraging management
Identify objective measurement criteria
FAs and CSAs must think positively 
- Good quality people

## Canna proj
Start with skepticism due to loss making an
Add info on public opinion before and after legalization
Look at regulation and see where opportunities lie. 
Hard to know how regulation would make things worse- so there is only upside
Big risk is lots of money raised with low barriers of entry 
Slim to 4 cases:
- Regulstory bear, things stay same with state operation, canadian cos cant access US
- Base case: competition drives down prices, low barriers etc. uber/ ridesharing hype allowing big capital raises. Low barrier of entry. Consolidation
### 2 regulatory risks:
- Lawsuits - health, smoking
- Greater competition

Metrics:
- Price to sales
- Gross margins
- Market cap
- Net cash/ runway
- Net loss/ profit
 
Interested in brands 


Need more qual for:
- Curaleaf
- Trulieve
- Acreage
- Cresco
- Village farms
- Green thumb
- iAnthus
- Harvest health

<!-- {BearID:9753D9D7-2380-4B1A-8B21-CFD7A2DCB269-3311-000003DF55642E68} -->
[[000 Life MOC]]
# Notes to make

[[AUC]]
[[Learning Curves]]
*Topics I’m interested in researching:*
* AI/ machines thinking
	* Creativity/ imagination paired with cognition
* Quantum Computing 
	* Church of the larger Hilbert space
* Education
* Encryption/ security
* Machine Learning
* Philosophy
* Neuroscience
* Evolutionary psychology 
	* Sociobiology
* Prospect Theory
* Protein folding
* Aging
	* [Reversal of epigenetic aging and immunosenescent trends in humans - Fahy - 2019 - Aging Cell - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1111/acel.13028)
	* Aging clocks
	* Therapeutic plasma exchange
* Energy [[Energy]]
			

*Topics I know, but need to research more:*
		* Chow/ Wald special F test
		* [[Fixed Effects Model]]
		* [[Random Effects Model]]
		* Straight pooling panel model
		* Disraeli
		* Kernel modelling
		* Economic bubbles! Research history and examples! Formation, detection, commonality [[Bubbles]]
			* Consider the psychology
		* Exponential function! Learn about it and its occurrence irl, as well as real interpretation[[052 Physics TOC]] | [[Standard Model TOC]]
# Nuclear Fission
#concepts 
See also [[Nuclear Fusion]]
Fission refers to a type of [[Radioactivity]] where an atomic nucleus is suddenly split into two or more nuclei and other [[Radioactivity|Radioactive]] matter. 

Traditional nuclear reactions in nuclear bombs work fission U-235 molecules, [[Isotopes]] of U-238. Nuclear fission with U-235 releases 2 neutrons, so any surrounding nuclei also fission, creating a chain reaction. This requires enough matter, typically about 10kg of U-235.

### Spontaneous Fission
Similar to [[Radioactive]] decay. Occurs rarely in nature, but some artificial [[Isotopes]] are unstable and decay naturally.

### Induced Fission
Occurs when the right type of nucleus is hit by a [[Neutrons|Neutron]]. This kind of fission forms the basis for nuclear reactors and nuclear bombs.


## Nuclear Bombs
Nuclear bombs operate by fissioning either `U-235`, an [[Isotopes]] of Uranium (U-238) or `Pu-239`. U-235 is much harder to get than plutonium, as it occurs naturally in only 0.7% of uranium. Pu-239 is commonly produced in energy-producing nuclear reactors. The plutonium fission process yields an extra neutron compared with uranium fission. 

There is a critical mass of uranium or plutonium needed to make a bomb. This is about 15kg for U-235 and 5kg for Pu-239.


## Slow Neutrons
Fission powerplants cannot explode with the force of a nuclear bomb. They rely on *slow neutrons* which move more slowly than atoms do once they reach several thousand degrees, stopping any reaction from running away to the point where it is truly a nuke. *slow neutrons* are much more likely to hit stuff, making the critical mass needed for a chain reaction easier to achieve.


## Nuclear Waste Problem
Since the half life of Pu-239 is roughly 24,000 years, there are fears about what to do with currently produced nuclear waste.[[052 Physics TOC]] | [[Standard Model TOC]]
# Nuclear Fusion
#concepts 
See also [[Nuclear Fission]]

Nuclear fusion is the process by which two or more [[Atoms|Atomic]] nuclei come together, producing a new, heavier nucleus and possibly releasing various forms of [[Radiation]] in the process. Fundamentally the same as fission, however the resulting element weighs more than the inputs, unlike fission where this is reversed.

Two nuclei will fuse if they touch. To do so, they must overcome the fact that nuclei are positively charged, meaning they must be given sufficient energy to overcome that force. Heating things to a plasma of millions of degrees does the trick.


### Solar Fusion
The sun gets its energy from fusing 4 hydrogen nuclei to make helium, releasing 5 $\gamma$ (gamma) rays, 2 $v$ (neutrinos) and 2 $e^+$ (positrons)

## Fusion for power
Very exciting concept
Possible to create fusion, but making it a sustainable reaction + use less energy to maintain than you get out has been a challenge.# Nvidia Investment Thesis
<!-- #finance/xcap/Investments -->
Nvidia has two major areas of focus, consumer gaming and datacenter. We believe that while the consumer gaming segment is mature and not likely to see exceptional further growth, the datacenter revenues are only starting to be realized. The constant growth in the need to process huge amounts of data is only going to grow larger over the coming years, and Nvidia is well poised to take advantage of this. Graphics cards are especially good at repetitive tasks, so they also lend themselves well to applications in self driving cars and other ML/ AI driven applications. We believe the upcoming growth in self driving cars in particular is an obvious avenue of success for Nvidia.

The main risks to Nvidia come from

<!-- {BearID:52833262-E855-418A-AEFB-F81BBC9C7033-406-0000024466B8D3B7} -->
# Oklahoma Grad Trip Plan
#my/plans

[[2021-05-06]] - Plans started!

## Invited

1. Molly
21. Nicole
22. Emily
23. Sophia
28. Ana
29. Zoe
2. Me 
25. Emi
	1. Filip
26. Tayo
	1. Henry
27. Owen
28. Sebas
29. Tayo
30. Raf
31. Peter
32. Thomas
33. Nicole
34. Serg
35. Kev
36. Mar
	1. Nige
37. Omie

## Confirmed
1. Me
2. Molly
3. Ana
4. Zoe
5. Emi
6. Dre
7. Sebas
8. Peter
9. Kev
10. TT
11. Rafa
12. Emily
13. Nicole

## Maybe
1. Tayo
2. Owen



## Capacity
**TOTAL: 28 ppl!**

### Main Casa
| BR      | # ppl |
| ------- | ----- |
| 1       | 2     |
| 2       | 6     |
| 3       | 2     |
| 4       | 2     |
| Couch 1 | 1     |
| Couch 2 | 1     |
|         |       |
| Total   | 14    |


### Side Casa
| BR      | # ppl |
| ------- | ----- |
| 1       | 2     |
| 2       | 3     |
| 3       | 2     |
| 4       | 2     |
| Couch 1 | 1     |
| Couch 2 | 1     |
|         |       |
| Total   | 14    |[[001 Meta MOC]] | [[010 Mind MOC]] | [[Information Theory]] 
# Ontology
#🌱
Ontologies are a fascinating concept. The idea of a sort of lexical map that defines relationships between objects or concepts is really neat. Sort of like the exploration of the nature of being, from different perspectives.

I've certainly created an ontology of my own here in obsidian. Quite exciting. I need to keep refining and tweaking it until it gets really freaking powerful.

This definitely links with the concept of [[Decision Machines]] and potentially [[Symbolic AI]]. It would be interesting to contrast the idea of a knowledge database sys[[Machine Learning TOC|Machine Learning]]
# Optimizers
#concepts/cs/ml #🌱 
[[070 Finance MOC]]
# Options Strategies
#concepts/finance


## Quantitative and Derivative Strategies
[[Todo]] #todo Transcribe into LaTeX from bear image!

## Hedging Strategies
[[Todo]] #todo Transcribe into Matlab from bear image!

## Alternative exits
[[Todo]] #todo Transcribe into Matlab from bear image![[000 Reality MOC]] | [[000 Life MOC]] | [[011 Mental Models MOC]]
# Orders of Reality
#🌲 

1. [[000 Reality MOC|Reality]]
2. [[000 Life MOC|Life]]
3. [[Consciousness]]

Awareness of orders of reality is [[001 Meta MOC|Meta]][[051 Math TOC]] | [[Linear Algebra TOC]] | [[Quantum Algebra]]
# Outer Products
#concepts #🌱

The outer product of two states is an operation that outputs a matrix given two states. The outer product of two states is a matrix obtained by multiplying the column vector of the first state by the complex conjugated row vector of the second state.

## Definition
$\Large\ket{\psi}\bra{\phi}=\begin{pmatrix}\alpha \\ \beta\end{pmatrix}\begin{pmatrix}\gamma* & \delta*\end{pmatrix}=\begin{pmatrix}\alpha\gamma* & \alpha\delta* \\ \beta\gamma* & \beta\delta*\end{pmatrix}$

Note that with this notation, any matrix can be written as a linear combination of outer products between bit string states. For a 2x2 matrix:
$$
\large A=\begin{pmatrix}A_{00} & A_{01} \\ A_{10} & A_{11}\end{pmatrix}=A_{00}\ket{0}\bra{0}+A_{01}\ket{0}\bra{1}+A_{10}\ket{1}\bra{0}+A_{11}\ket{1}\bra{1}
$$

This is super helpful since it means that acting on [[Quantum States]] with [[Matrices]] becomes an exercise in computing overlaps between states. For example:
$$
\Large A\ket{\phi}=A_{00}\ket{0}\braket{0|\phi}+A_{01}\ket{0}\braket{1|\phi}+A_{10}\ket{1}\braket{0|\phi}+A_{11}\ket{1}\braket{1|\phi}
$$
$$
\Large=(A_{00}\alpha+A_{01}\beta)\ket{0}+(A_{10}\alpha+A_{11}\beta)\ket{1}=\begin{pmatrix}A_{00}\alpha & A_{01}\beta \\ A_{10}\alpha & A_{11}\beta\end{pmatrix}
$$

Where:
$$

$$
See also [[Inner Products]]
[[2021-04-13]][[Cognitive Biases TOC]]
# Overconfidence Bias
#concepts/cognitive_biases 

This is the cognitive bias whereby people on average tend to be aggressively overconfident in their abilities - it frequently is found in examples such as driving, teaching or spelling. [[Cognitive Biases TOC]]
# Overoptimism Bias
#concepts/cognitive_biases 

This bias is closely related to [[Overconfidence Bias]], yet I would classify it as perhaps an overarching theme of overconfidence. It stems from an ignorance of [[Base Rates]], and the belief that negative outcomes are less likely for us than they are on average, as well as the reverse. [[PHYS106 Physics for Educated Citizens]] #school
# Annotated Bibliography
Project 28 - Carbon capture and storage

https://iopscience.iop.org/article/10.1088/1748-9326/8/2/024024
Quantifying consensus on climate change - Cook et al.

Janowiak, M.; Connelly, W.J.;Dante-Wood,  K.; Domke, G.M.; Giardina, C.; Kayler, Z.; Marcinkowski, K.; Ontl, T.; Rodriguez-Franco, C.; Swanston, C.; Woodall, C.W.; Buford, M. 2017. Considering Forest and Grassland Carbon in Land Management. Gen. Tech. Rep. WO-95. Washington, D.C.: United States Department of Agriculture, Forest Service. 68 p.
* This is a useful paper about where the world's carbon is currently stored. The interesting thing is that in a given year, only a tiny proportion of the total carbon store is circulated.

Chandler, David. “MIT Engineers Develop a New Way to Remove Carbon Dioxide from Air | MIT News | Massachusetts Institute of Technology.” _MIT News | Massachusetts Institute of Technology_, MIT News, 24 Oct. 2019, https://news.mit.edu/2019/mit-engineers-develop-new-way-remove-carbon-dioxide-air-1025.
* A nice recent article about some promising carbon capture tech developments. I'd like to cite a couple other recently developed techniques/ approaches. 


![](https://cdiac.ess-dive.lbl.gov/images/carbon_cycle2.png)

- Carbon Dioxide Information Analysis Center

- Pre combustion CCS: https://www.energy.gov/fe/science-innovation/carbon-capture-and-storage-research/carbon-capture-rd/pre-combustion-carbon
- Post coimbustion CCS: https://www.theguardian.com/environment/2011/mar/18/carbon-capture-storage-technologies#:~:text=There%20are%20three%20main%20types,%2C%20post%2Dcombustion%20and%20oxyfuel.


- Sorghum carbon capture by salk institute https://www.salk.edu/news-release/salk-institute-and-sempra-energy-announce-project-to-advance-plant-based-carbon-capture-and-storage-research/
- Salk HPI https://www.salk.edu/harnessing-plants-initiative/research/
- Algae https://www.energy.gov/sites/default/files/2017/09/f37/algae_cultivation_for_carbon_capture_and_utilization_workshop.pdf
- 

## Outline
- Intro: set up argument, introduce structure, get readers interested + invested. Should explain why carbon capture tech is relevant to us today
- P1: More about climate change. I need to give some background on the greenhouse effect and provide sources that support the idea of reducing atmospheric carbon/ cleaning emissions to control climate change. 
- P2: Carbon cycle/ where carbon is stored on earth - thinking of even including a diagram with % per each storage source - thinking like oceans, limestone, groundwater, clouds, atmospheric etc. Would provide good context and ouxtline where the opportunities lie.
- P3: Carbon capture - natural. There are some really exciting developments in the field of carbon capture that are focusing more on the natural side of things lately. My best source for this is actually some of the groundbreaking work Midd has been doing in the space. Very exciting, especially when combined with the idea of carbon credit generation.
- P4: Carbon capture - tech. This is going to be the eventual focus. Trees work pretty well, but aren't the full solution. I will try to reference some new technologies like the MIT article above.
- Conclusion: wrap up. Describe the way forward, risks & challenges and further questions.# PHYS106 Exam 3a[[PHYS106 Physics for Educated Citizens]]
# Midterm 2
[[PHYS106 Physics for Educated Citizens]]
# PHYS106 Assignment 10a
#school 

## CH10
### 2
C. China had the highest emissions since 2006, Chapter review

### 3
A. Americans use the most energy per capita, Fig 10.11

### 5
C. Coal. Section on clean coal.

### 7
C. Carbon dioxide levels are about 1/3 higher than their long term trend. Section on CO2

### 11
D. Clean coal is not in itself clean, but attempt to capture a similar amount of CO2 as is released into the atmosphere, thereby counteracting negative effects and potentially making it carbon neutral. Section on Solutions -> clean coal.

---
---
---
---
---
---
---
---
---


---
---
---




I have neither given nor received unauthorized aid on this exam.

Michael Calvey[[PHYS106 Physics for Educated Citizens]]
# PHYS106 Assignment 11a
#school 

## CH10
### 12
D. 80%, pg393

### 13
C. 20%, pg393

### 16
A. 3 years, pg390

### 17
A. IR Radiation, as it will heat up the surface of the earth. Section on greenhouse effect.

### 25
A. Nuclear power is the cheapest power source, although changing conservation 

___

___

___

___

___

___

___

I have neither given nor received unauthorized aid on this exam.
# PHYS106 PS1a
[[PHYS106 Week 1]][[PHYS106 Physics for Educated Citizens]]
I have read the course syllabus for PHYS 106
Michael Calvey

4. B - Kinetic energy can be measured in calories. The equation presented above `Muller pg29 - ( E = 1/2 mv^2 )`uses energy in Joules, and we know we can convert from 4200 Joules to 1 Calorie.

7. C - The author describes how hybrid cars rely on their electric motors for propulsion and use their gasoline engines to recharge their batteries. `Muller pg9`

8. B - Although hydrogen has a high level of energy per gram, it is not very dense so has quite low energy per cubic cm. This means that while it seems attractive as a fuel source, it takes lots of space to store a comparable amount to a gasoline car, presenting design problems. `Muller pg12`

9. C - `Muller pg 5` gasoline has 10 Calories per gram, U-235 has 20 million. 20 million / 10 = 2 million, closest to C

10. D - Uranium contains the most energy per gram of all the options with 20 million Calories per gram, compared touch smaller values for the rest (the highest is CCC at 6 Calories per gram)

12. A - Coal is by far the least expensive regardless of whether you use it for heat energy directly or for electricity, at 0.4c and 1.2c respectively. (Muller page 15)

14. A - Hundreds of years (Muller Ch1)

23. A - While the energy of hydrogen is 3x higher than that of gasoline per unit mass, it is 3x lower per unit of volume due to hydrogen’s lower density.

<!-- {BearID:DB445282-CF00-4086-9FE8-D9D1734F7B4A-405-00009F5B6AC3D142} -->
# PHYS106 PS1b
[[PHYS106 Week 1]][[PHYS106 Physics for Educated Citizens]]
Response to internet research topics (1, 2 or 6)

Electric cars have come an impressively long way in recent years, even since the Muller textbook was written. He talks about having to replace batteries once every couple hundred charge cycles, but modern EV batteries tend to be warrantied for the life of the car, or at least to the point where a comparable gasoline battery would be coming up for some serious servicing too. Looking at the Tesla website, we see a warranty for 8 years or 100,000 miles. Apparently, few owners these days need to do a full battery replacement during the life of their vehicle, outside user error or unusual wear. 

It is exciting to see how quickly some of the content in course materials has been developing. The cost of owning and driving an EV is coming down rapidly as well. In the book Muller refers to the Tesla Roadster, a luxury $100,000 supercar created to demonstrate the possibility of a performance EV. Today, you can buy an EV for less than $20,000, which while not cheap is much cheaper than before. It is still somewhat more expensive all told to drive an electric car, but many commentators are predicting this may change in 2021 as battery tech continues to improve and become more affordable. A recent review by caranddriver.com [EV vs. Gas: Which Cars Are Cheaper to Own?](https://www.caranddriver.com/shopping-advice/a32494027/ev-vs-gas-cheaper-to-own/) found that gasoline cars are really only $358 cheaper to own and operate over the first 3 years of vehicle life. This difference is almost negligible, and probably changes in areas with expensive gasoline.

<!-- {BearID:F12CDA2C-1E4A-454A-BC2E-37306BF98B1A-405-0000A2245E639878} -->
# PHYS106 PS2a
[[PHYS106 Week 2]][[PHYS106 Physics for Educated Citizens]]
*Chapter 1:*
6.  
	* Horsepower: P
	* kWh: E
	* watt: P
	* Calorie: E 
16.   
Muller, Chapter 1 review:
1 GW (a billion watts) in a square km
1 million square meters in a square km, so a thousand watts per square km or 1kW 
17.  
 B, Muller Chapter 1 review
19. C, I have one in front of me
20. B - 1GW, Muller Chapter 1 review
21. D - Muller Chapter 1 page 15

*Chapter 2:*
1. C - Chapter 2 review
2. A - 1000 ft/sec, or the speed of sound. Muller Chapter 2 review
3. B - Chapter 2 review

5. A, it warms the room since it uses a heat transfer from the fridge to the surrounding to create the cooling, and the pump emits some heat as well.

7. B, specifically the average kinetic energy is the same - Chapter 2 review

<!-- {BearID:6AC6118C-64A3-4A89-8C4D-3A0A89DA2D43-3006-0000716DA7B6CFE5} -->
# PHYS106 PS2b
<!-- #school/phys106 -->
[[PHYS106 Physics for Educated Citizens]] [[PHYS106 Week 2]]
[[Superposition]] [[Entanglement]]

The decade of quantum computing is upon us, writes WSJ columnist Jared Council. In a recent interview with a senior IBM Vice President of research, Council learned of several key innovations that are about to happen that have a high chance of paving the way to more mainstream adoption of quantum technology over the course of the next decade. Specifically, the advance in question is the ability to conduct error correction on the outputs of quantum computers through software, rather than the current approach of designing special, complex and expensive hardware to tackle the same issues. Joschka Roffe writes in a recent research paper that error correction will “play a central role in the realisation of quantum computing.” 

What makes quantum computing so special? Quantum systems take advantage of two core concepts of quantum mechanics to work: superposition and entanglement. In short, superposition is the idea that a quantum particle is able to exist in more than one state, or strictly a linear combination of multiple states, at once. Entanglement is the idea that quantum particles created simultaneously in a certain way cannot be fully understood independently - to measure and understand the full state of one particle, you must measure the entire system. Together, these two principles create the foundation for quantum computing technology. Unfortunately, maintaining and tracking quantum particles is a very difficult business, and even the best systems make frequent mistakes. So many in fact, that this has been the single largest roadblock to scaling systems from their current sizes. This is what makes IBM’s announcement of a software-based error correction system so compelling. IBM is not alone in looking to get a leg up in the software research space. A number of major corporations including Visa, JPMorgan Chase, Roche and Volks

## References
Council, Jared. “The Decade of Quantum Computing Is upon Us.” /Wall Street Journal/, www.wsj.com/articles/the-decade-of-quantum-computing-is-upon-us-ibm-exec-says-11614802190. 

“Quantum Superposition.” /Joint Quantum Institute/, jqi.umd.edu/glossary/quantum-superposition. 

Roffe, Joschka. “Quantum Error Correction: an Introductory Guide.” /Contemporary Physics/, vol. 60, no. 3, 2019, pp. 226–245., doi:10.1080/00107514.2019.1667078. 


Survey: While I would love to ask to study quantum physics, they are probably beyond the scope of this class. In absence of that, I am very interested to learn more about particle physics generally, including a deep look at chapter 4.


Honor code: I have neither given nor received unauthorized aid on this examination.

<!-- {BearID:EAE8812C-0CE6-42F3-AD3E-CA25EC336396-3006-000078B1F9CA7DD3} -->
# PHYS106 PS3a
[[PHYS106 Week 3]]

# CH2
12.  B

14.  B, since atoms are about 10^-8 (chapter review)

17.  A

19.  B

21.  C

29.  B, Muller states 1000th vol increase per 100,000th temp increase 

30.  C

32.  A

33.  A (Chapter review)

34.  D

# CH3
1.  C, since it scales quadratically

3.  B, since then they don’t displace the air anymore 

4.  D, this is done by the wings 

5.  C - technically, they push themselves off the fuel

6.  D

7.  D

14.  C

15.  D - gravity doesn’t change much, but increased air resistance will make them burn up.

17.  C

18.  C, especially for spaceflight, since the number of gees needed is about 3200. 10 is the most survivable by most humans.

I have neither given nor received unauthorized aid on this exam

<!-- {BearID:381C59F9-271E-40FF-85E2-9823111B8DF3-1476-000097E5292A2E68} -->
# PHYS106 PS3b
[[PHYS106 Physics for Educated Citizens]]
[[PHYS106 Week 3]]

# Internet Research on the Ion engine
Ion engines typically work by bombarding neutral fuel atoms with electrons, releasing more electrons in the process. This results in positively charged fuel atoms, which then mix with negatively charged electrons to produce plasma - a type of half gas that responds to electric and magnetic fields (NASA 2016). This plasma then passes through a number of engineered spaces, eventually being sped up to speeds of up to 90,000mph (NASA 2016). This explains why ion engines are more efficient than chemical fuel engines. Specifically, the exhaust particles for ionized plasma move far faster than burning liquid or solid fuel, creating a larger energy difference and thus delivering a higher amount of energy. Fuel efficiency is also better per unit of mass since individual atoms are charged and used, rather than relying on the expansion properties of combustion.

Recent breakthroughs in ion tech have been significant. Researchers at NASA testing a new prototype slated for Mars, the X3, recently achieved fantastic efficiency and power results with a test unit. Total power delivery capability for the engine was over 100kW, with a wide range of deliverable power from 5kW up to 105kW. Also, the engine was able to generate 5.4 Newtons of thrust, more than any other ion engine to date. This advance is significant in that it showed we can begin to approach the theoretical great efficiency of these engines, and potentially significantly alter the economics of space travel in the process. 

# Works Cited
Muller, Richard; 2010; Physics and Technology for Future Presidents

NASA; 2016; Explanation of ion engine
[NASA - Ion Propulsion | NASA](https://www.nasa.gov/centers/glenn/about/fs21grc.html)

Pultarova, Tereza; 2017; Ion thruster prototype breaks records in tests
[Ion Thruster Prototype Breaks Records in Tests, Could Send Humans to Mars | Space](https://www.space.com/38444-mars-thruster-design-breaks-records.html)







I have neither given nor received unauthorized aid on this examination

<!-- {BearID:9602BF68-E3B8-4DDC-9070-4C3E18AC3B1E-1476-000097F38AC67BB3} -->
# PHYS106 PS4
[[PHYS106 Physics for Educated Citizens]]

[file:FC072295-7445-49BD-AF83-57459EC40E4B-574-0001187F4CF102B3/PHYS0106_PS 4.pdf]

# CH 3
19.  B, roughly 66mph/s (Chapter 3)

24.  B, as this is the speed of gravity, assuming we are falling near the surface of the earth (less if further out, discounting the effects of atmosphere)

25.  A, applying a constant acceleration would create an equivalent sensation to gravity. Fine tuning this to output exactly 10m/s would make this feel like 1g.

27.  A, lone hydrogen tends to drift off, and is very reactive so doesn’t stick around in pure form. hydrocarbons and water are where most of the earth’s hydrogen is.

28.  C, geosynchronous

31.  D


# CH 4
1.  D - Chapter review

2.  A

4.  A

5.  C - Chapter review

7.  B, Gamma rays

8.  A

9.  B

11.  D, (1/2)^3

19.  B, C, D - We know it probably isn’t exact, but is a good safe way to estimate harm
 
20. C

<!-- {BearID:D33D7627-A14C-4902-9DBF-23307E11E23B-574-00011875F44B10E4} -->
[[PHYS106 Physics for Educated Citizens]] | [[PHYS106 Week 5]] | [[005 Active MOC]]
#school
# PHYS106 PS5a
## Chapter 4
13. C, page 120

16. D - all of the same isotope decay at the same rate, regardless of age. There is always a set probability of decay.

28. B - Chapter review

30. B - billions of years, section on isotopes

34. B - pg 113


## Chapter 5
5. D - chapter review

9. D - Chapter 4 review

11. C, although "fast breeding" reactors are not like this and use fast neutrons. pg177

14. B - enough to destroy the Soviet Union if even just 1% remained. pg180 & chapter review

17. B - pg177

24. B - he mentions this multiple times, ch4 review & ch5.

29. ?

31. D - Chapter review


## Additional Problem
Suppose that a CT scan to a person’s head corresponds to a 150 millirem dose to approximately 7% of the body. How many CT scans can you have before you increase the likelihood of excess cancer by approximately 0.1%? (For comparison, you can have approximately 100,000 dental X-ray exams before you increase the likelihood of excess cancer by 0.1%.)

The average full-body dose that it takes to induce cancer is 2500 rem. 150 millirem to 7% of the body equals roughly 10.5 millirem to the whole body. The dose required to increase excess cancer risk by 0.1% is 2500rem * 0.1% = 2.5rem, or 2500 millirem. Thus it would take 2500 / 10.5 doses ~ 238 to increase cancer risk by 0.1%


I have neither given nor received unauthorized aid on this exam[[PHYS106 Physics for Educated Citizens]] | [[PHYS106 Week 5]] | [[005 Active MOC]]
#school
# PHYS106 PS5b

**Question 6:** *The public has a fear of many things that relate to radioactivity. Give some reasons why people fear radioactivity and explain whether this fear is rational or irrational. What is the worst that can perceivably and in reality happen with a nuclear power plant? What has happened, historically?*

Nuclear power is feared by many people primarily because the underlying science is not readily understood. Radioactivity has a bad rep primarily because of a lasting association with nuclear bombs as well as several crucial mistakes that compounded into reactor meltdowns - Fukushima, Three Mile Island and Chernobyl. In his article on the topic, Shellenberger (Shellenberger 2018) argues that these accidents actually "demonstrated the relative *safety* of nuclear power, rather their danger." He does have a point - noone died in the Three Mile Island disaster and there are still only several deaths directly associated with Fukushima. Chernobyl was worse, but the plant was known for "a lack of a safety culture" (https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/appendices/chernobyl-accident-appendix-1-sequence-of-events.aspx) with weak safeguards in place, so modern western plants are almost incomparable. Muller also notes in Chapter 5 that the Chernobyl plant used carbon as a moderator instead of water. As the fission reaction went out of control, cooling water evaporated and the carbon ignited. Because of this, he terms it a "reactivity" accident (pg188) rather than a meltdown from loss of coolant.

Despite this, Shellenberger definitely takes some creative justice in trying to prove his point. He claims only 60 died, although estimates made from the linear hypothesis suggest otherwise, with Muller claiming tens of thousands of potential unrecorded cases of cancer caused by the radiation - the IAEA estimated a total of 60 million rem dose to the population of the world as a whole. We learned that this corresponds with 24,000 cancers worldwide. It is not known whether this estimate over or underinflates the truth, since the linear hypothesis is difficult to test. Clearly the true danger is worse than Shellenberger makes it out to be. However, he is not altogether wrong. It isn't just to consider the dangers of nuclear power in a vacuum. We must also consider the dangers of toxic coal power byproducts, produced in orders of magnitude more volume than nuclear waste. From a waste perspective, nuclear power is not much worse than existing alternatives. It is certainly a technical challenge to deal with nuclear waste, however the challenge is not insurmountable and should not be a fundamental block to continuing the development of nuclear power. 

How about fears about nuclear accidents? Much of the public's view is likely shaped by the several anecdotal disasters mentioned in the first paragraph. After agreeing with the rather large death estimate of ~24,000 for Chernobyl, Muller goes on to say that it is difficult to evaluate the true extent of damage done, since even the large spike in cancers predicted would have only increased odds of dying from cancer by about 1% for those exposed to 45 rem. It must be added that the extent of the Chernobyl accident was likely larger than anything that might happen today in a developed country nuclear power plant. Safety controls are better and there is a much stronger culture of safety and transparency in their operating procedures. There are however some valid worries about the age of many of the world's nuclear reactors. Many were commissioned in the 1960s, making some over 50 years old. It is now coming time to make necessary critical updates, however not much funding is available due to fears of disasters. Ironically, big future disasters are more likely to be caused by underfunding rather than further investment.


## Works Cited
Shellenberger, Michael. “If Nuclear Power Is So Safe, Why Are We So Afraid Of It?” _Forbes_, Forbes Magazine, 6 Sept. 2018, www.forbes.com/sites/michaelshellenberger/2018/06/11/if-nuclear-power-is-so-safe-why-are-we-so-afraid-of-it/?sh=7fa12ab46385. 

World Nuclear Association. “Nuclear Plant Safety - Chernobyl.” _Chernobyl Appendix 1: Sequence of Events - World Nuclear Association_, June 2019, www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/appendices/chernobyl-accident-appendix-1-sequence-of-events.aspx.[[PHYS106 Physics for Educated Citizens]] | [[Superconductivity]]
# PHYS106 PS6a

## Chapter 6
1\. C - amps. Chapter review
	
3\. D - Chapter review

4\. A - pg 214

7\. C - Chapter 6, edison vs tesla conflict

11\. A - Chapter 6, earth's magnetic field section

12\. A - Flow of liquid iron pg 224

18\. A - Chapter review

23\. D - pg 224

25\. D - although high voltage and current become dangerous in tandem. Chapter review

38\. C - Chapter review

39\. B - Chapter review

40\. C - Tesla v edison conflict section[[PHYS106 Physics for Educated Citizens]]
# PHYS106 PS7a
## Ch 7
2\. A - pg 272

6\.B - pg 270

9\. C - pg273, chapter review

16\. D - PG261

18\. B - since sound tends to bend towards the channel and effectively gets captured there. CH6 Review

21\. A - Chapter review

27\. D - Sound channel section, pg 253

40\. C - pg256

46\. D - Tsunami section, pg 247

47\. C - `v=L/T`, wave speed - pg248

48\. D, Doppler shift pg269

50\. C Atmospheric sound channel created, pg252[[PHYS106 Physics for Educated Citizens]]
# PHYS106 PS8a
#school 

## CH8
### 3
The index of refraction refers to the speed of light in a given substance. Slower substances will cause light entering at an angle to bend more. From the section on diamonds, dispersion and fire.

### 7
B, as the two are inversely related in the diffraction formula. Given in the section on diamonds, dispersion and fire.

### 16
B, as water has a different diffractive index (roughly 2/3c). Given in the section on diffraction.

### 18
B, and Shannon's law tells us that bits per second is correlated directly with frequency. Chapter review

### 19
C, since blue light has the highest frequency. Referencing the chart showing the frequencies of different colors in the visible spectrum

### 23
C, the ability of the lenses to focus precisely degrades over time, requiring reading glasses in old age. Chapter review

### 31
C, since with a small hole the diffraction effect kicks in and since D is in the denominator, spreading increases. With a large hole, light from different sources hits the same spot causing blurring as well. Chapter review + diffraction formula

### 35
C, Chapter review

### 39
C, 0.66 feet, since the refraction index of water is given as 2/3. Section on refraction

### 42
C, 20 feet, since the distance is very large the modified diffraction formula tells us that the resolution of a ~2.5m aperture telescope in GEO will be about that much. Section on diffraction & spy satellites.
















I have neither given nor received unauthorized aid on this exam
Michael Calvey
[[PHYS106 Physics for Educated Citizens]]
# Assignment 9a
#school 

## CH 9

### 8
D, the temperature will increase by a factor of 16 since total radiation is proportional to the fourth power of temperature. Total power equation, early in CH9

### 9
D, Tungsten Lightbulbs section

### 11
A, about 16% of the energy used by tungsten lightbulbs is converted to light, the rest is wasted as heat. Section on tungsten lightbulbs

### 18
A, since heat is really infrared radiation/ light. EM radiation summary section.

### 37
 C, pg 338

### 38
B, IR in the home section

### 41
C, specifically around 265 nm. Section on UV light

### 46
A, EM radiation recap

### 50
B, since most energy from the sun arrives as infrared radiation. This means it is white in infrared. From the section on infrared.

### 51
C, since UV is the most effective at destroying DNA, and thus disabling any bacteria from reproducing. 

An argument could also be made that microwaves would be effective at purifying water through boiling, but this would take much more energy.







I have neither given nor received any unauthorized aid on this exam


Michael Calvey
[[PHYS106 Physics for Educated Citizens]]

# Assignment 9b
#school 

When we think of the visible light spectrum, we really only mean the band of light visible to humans. Electromagnetic radiation (light) still exists outside of this band, and some animals are equipped with the necessary biology to detect and interpret different parts of the spectrum. Specifically, many arthropods (spiders, scorpions etc) have the ability to not only detect but also reflect UV light. (Pearson) The reason for the animals' reflectivity varies, but for most arthropods tends to result from flourescence, where shorter wavelength radiation is absorbed and re-emitted as UV or in some cases even visible light.

Another specific example more related to UV vision is insects detecting UV patterns in plants when hunting for food. It has been shown that a number of flowers have very distinctive UV patterns that are invisible to the naked human eye, yet clearly visible to animals such as bees, which use the UV indicators to find food. (Glass) A final example of animals that see in UV light is butterflies. Butterflies have been shown to have intricate UV markings on their wings that researchers currently believe are used primarily to identify mates. The most interesting thing about these animals that see colors we can't is that it is impossible for us to visualize what they would be seeing, since we have no frame of reference for what they would look like. According to Muller, there have been several cases where people, after having the lenses in their eyes removed through cataract surgery, were able to see UV light. (Muller)

___
___


## Works Cited
“CH9.” _Physics and Technology for Future Presidents: an Introduction to the Essential Physics Every World Leader Needs to Know_, by R. Muller, Princeton University Press, 2010. 

Glass, Don. “Insect Color Vision.” _A Moment of Science - Indiana Public Media_, indianapublicmedia.org/amomentofscience/insect-color-vision.php. 

Pearson, Gwen. “Luminous Beauty: The Secret World of Fluorescent Arthropods.” _Wired_, Conde Nast, 3 June 2017, www.wired.com/2013/11/arthropods-are-having-a-secret-rave/.

___
___
___
___



I have neither given nor received unauthorized aid on this exam.

Michael Calvey
# PHYS106 Physics for Educated Citizens

#### REF
#school/phys106 #school/class
[[005 Active MOC]]

#### Syllabus:
[[PHYS0106_syllabus_S21.pdf]]

[[052 Physics TOC]]
1. [[Energy]]
2. [[Atoms and Heat]] [[Atoms]]
3. [[Gravity, force and space]]
4. [[Nuclei and Radioactivity]] [[Electrons]] [[Radiation]] [[Radioactivity]]
5. [[Chain reactions, nuclear reactors and atom bombs]]
6. [[Electrostatic Force]]
7. [[Waves]]
8. [[Light]]
9. [[Invisible Light]]
10. [[Climate Change]]
 
# Homeworks
[[PHYS106 PS5a]] | [[PHYS106 PS5b]]

[[PHYS106 PS1a]] | [[PHYS106 PS1b]]

[[PHYS106 PS2a]] | [[PHYS106 PS2b]]

[[PHYS106 PS3a]] | [[PHYS106 PS3b]]

[[PHYS106 PS6a]] | [[Superconductivity]]

[[PHYS106 PS7a]]

[[PHYS106 PS8a]]

[[PHYS106 PS9a]] [[PHYS106 PS9b]]

[[PHYS106 PS10a]]

[[PHYS106 PS11a]]


## Exams
[[PHYS106 Midterm 2]]]

## Project
[[PHYS106 Annotated Bibliography]]
[[PHYS106 Project Plan]]

## Key Dates
* Exam 1 - Week 4 Friday, 12/3
* Exam 2 - Week 8 16/4
* Exam 3 - Week 12 14/5
* Final Presentation!

## Class Meetings

1. [[PHYS106 Week 1]]
2. [[PHYS106 Week 2]]
3. [[PHYS106 Week 3]]


[[Pasted image 20210324105545.png]]# PHYS106 Week 1

#### REF
[[PHYS106 Physics for Educated Citizens]]
<!-- #school/phys106 -->

[file:AB78F383-DBB0-4253-B136-9FF3ADDD03A6-2534-0000A24F1B42811C/Week1_Worksheet1.pdf]
[file:AAB91C42-B4DA-48A7-9DB8-1497D3ADC252-405-00000097459E9265/Week1_Worksheet2.pdf]
[[PHYS106 PS1a]][[PHYS106 PS1b]]

* Asteroid that hit during dino area was equivalent to 100 million Megatons of TNT
	* Hiroshima bomb was 13,000 tons of TNT

*GROUP 4??*
[[Energy]]


## Worksheet Answers
1. 
	1. Reservoir - potential energy, ability to do work that makes heat
	2. Wind turbine - kinetic to electrical, does work
	3. Oil well - heat/ both
	4. Asteroid - both
	5. Nuclear reactor - Heat
	6. Lava flow - heat
	7. Solar panel - heat
	8. Dynamite - Both
	9. CCC - heat
2. Heat, work and both

<!-- {BearID:E5728A7C-A11C-418F-B471-5FA682DDBD54-2534-0000A24C67AB4814} -->
# PHYS106 Week 2
<!-- #_reference/physics/quantum -->
<!-- #school/phys106 -->
[[PHYS106 Physics for Educated Citizens]]


## Worksheets
[file:9B67B46E-C98C-4B3B-B4D6-84156A21362C-398-00002B8CEC158341/Week1_Worksheet3.docx]
## Homeworks:
[[PHYS106 PS2a]]
[[PHYS106 PS2b]]

*group 5*
## Energy Conversions
* Combustion
* Nuclear decay
* (wind) Kinetic -> electrical
* Gravitational -> kinetic -> electrical
* Solar energy -> electrical
* Electrical -> heat
* Electrical -> kinetic
* Potential energy -> kinetic

## Why do we conserve energy?
* Heat energy can’t be harnessed easily, so must be conserved. Not all stores of energy give their energy off easily

<!-- {BearID:797FA068-CACE-45DF-A564-EC3A2C823ED1-398-00002B68C631A3AF} -->
# PHYS106 Week 3
<!-- #school/phys106 -->

[[Isaac Newton]]
[[PHYS106 Physics for Educated Citizens]]


[[PHYS106 PS3a]][[PHYS106 PS3b]]
[[PHYS106 Physics for Educated Citizens]] | [[005 Active MOC]]
# PHYS106 Week 5
#school

## Homework
[[PHYS106 PS5a]] | [[PHYS106 PS5b]]


![[PHYS0106_PS 5.pdf]]

## Topics Covered
[[Nuclei and Radioactivity]]
[[Chain reactions, nuclear reactors and atom bombs]]# PSet 11 reflection
[[Algorithms And Complexity]]
#school
Overall, this problem set was very successful. I again approached some of the problems in a fundamentally different way, but as a result of some of my prior observations, I now commit enough time at the start to just understanding the problem domain that I believe I now am much more consistent in delivering correct approaches. 

I think it is very interesting how I consistently approach some problems from different angles. I think this is not necessarily an issue at all, and I want to see what I can do to nurture this going forward, as this will give me an ability to think outside the box and supplement the solutions of others with my own ideas, as long as I can prove their efficacy. 

In terms of improvements to my process, I would really like to figure out a way to digitalize my homework. I am trying to see if using a tablet to handwrite my answers will give me this added flexibility, because I have some sort of weird issue with erasing and going back on my old work. I suppose it is a little late to try to fix that for this class, but moving forward, I think it will be a useful technique that will lend itself well to my future studies and work.

<!-- {BearID:8E929626-A437-4C44-8A44-C062BB2689D4-1155-00002EC20A10452E} -->
[[Algorithms And Complexity]]
# PSet 4 Reflection

I feel like I struggled a lot with this assignment. As soon as I saw tougher problems, I reverted back to some of my old bad test-taking habits and jumped through the questions, missing out obvious parts and not including some of the key low level justifications. I feel like this is partially a problem of approach, where I didn’t spend enough time thinking about what questions were really asking for and would try to answer hastily, as well as a lack of knowledge and understanding. Clearly, my recurrence relations are getting a little better, but are still lacking some aspects of their fundamental functions from my understanding. Moving forward, I will try to focus on several things. 

First, I would like to significantly improve my understanding of loop invariants. I feel like I understood them after class, and I thought I had approached them well, but it turned out that I was quite heavily misinterpreting what they were asking for. 

Second, I need to take more time to read the question, think about it and ensure I answer what is actually asked. This is a problem I have had to deal with for a long time, so it is a known issue, but it only comes out when my knowledge is being stretched very far. Being more methodical in general would be a good solution for this. First, I should write out parts of the problem that are given and perhaps draw a little diagram to help myself conceptualize, as I am a very visual learner. Then, I should take the time to ensure my response has every aspect of whatever it is trying to achieve. By this, I mean that I know in theory what an invariant should contain, as well as what determines the added runtime for a recurrence relation, but when I am under pressure I tend to put the first thing that comes to mind down and move on.

Finally, I need to focus on formalizing my work a little more. I tend to reach a conclusion in my head quickly, be happy with it, and only put down minimum justification for why it would work. If I take into account my second point, I can then go through the process step by step to create proofs that more clearly prove what they are trying to prove.

Overall, this was a very tough homework, although I did not appreciate how difficult it really was until I saw the solutions. If I spend longer thinking about the problems and making sure I understand them, I think I can better approach problems, even if they stretch my understanding of the topic.

<!-- {BearID:3B9F363F-5311-4BDF-9A35-5441EB445E53-312-000069B0F5060608} -->
# PSet 7 Reflection
[[Algorithms And Complexity]]
#school
Overall, my consistency and thought has improved significantly in the problem sets this semester, though I still had several minor issues this time that increasingly manifested themselves in the latter parts of questions 1 and 3.

With question 1, the issue here was interestingly quite different from my usual mistakes. I usually under think and as a result overwork many of my problems, but this time I actually did the opposite. My approach to solving this problem was very different from that taken in the solution, nevertheless I feel that I was not explicitly wrong. I think that in some ways this is a manifestation of my usual issue of not spending enough time planning problems, but this time I actually did spend a long time thinking conceptually about the problem before trying to actually solve it. I think that my approach is not necessarily worse, although it may take slightly more memory and is a little bit more confusing to understand and parse. I am confident however that it is algorithmically correct.

Question 3 was quite mixed. Parts of it went really well, and I was actually either spot on or very close with some of my answers. My big issue was that I got part c completely wrong by overthinking it too much, and this carried forward to have a meaningful effect on my ability to finish the rest of the problems.

My main takeaways for the future are still quite similar to before, although now I have one main new thing to think about. Follow-on questions often catch me out, and in hindsight it should have been obvious that my 3.c) was wrong, given how the last couple parts made so little sense. For the future, I will aim to not think of the problems in such a linear fashion, but to consider multi-part problems as a whole, where all of the solutions are related and enable each other.

<!-- {BearID:E65C8DFC-429C-4D31-A7E1-E55783D92D57-530-00001E7D669CF504} -->
# PSet2 Reflection
[[Algorithms And Complexity]]
#school
Overall, this problem set was slightly more successful than the last. I had a clearer understanding of each question going into them, and I feel like starting earlier played a big role in this. I got most of the work done several days before the deadline, coming back to the hardest questions on a new mind, keeping my approach fresh.

I made one crucial interpretational mistake this time, in question 4. I interpreted the question as asking to find how the algorithms provided were incorrect, whereas actually they were both correct, just non-optimal.  My general approach for creating dummy cases was not too far from the actual required approach. This issue shows that I had the correct approach, just an incorrect understanding of the question. Moving forward, I need to spend longer reading and understanding each question without jumping into trying to find the solution.

<!-- {BearID:EE42013B-91EB-4DE0-BDA5-1FFED50328B4-303-00005C828F731A22} -->
[[Statistics TOC]]
# Panel Data
#concepts
[[Notes to make]]

# Estimation Methods
## Pooled OLS
The most simple way of estimating panel data, where we assume homogeneity among slices of observations. Adds the fewest variables so degrees of freedom remain sane. 

## Fixed Effects
See [[Fixed Effects Model]]

## Random Effects
See [[Random Effects Model]]

<!-- {BearID:43FF4013-7EE4-4EFC-84BB-7D927E2F3A42-16983-000109E3BCFDCA38} -->
[[095 Journals MOC]]
# Papa Letter
#outputs/letter 

confirmation- 5179126

Дорогой папа.

Я долго-долго думал что писать. Я думаю, с начало начну со всем, что случилось в этом семестре, а потом поговорю про будушее. В Мидде, у меня во всех классах получается. Пока что получил только один результат с мидтерма, на экономическую статистику. 91%- Я рад, но не полностью. Готов стараться больше, и умнее. 

Я недавно начал переделывать свою систему организации. Я помню, как мы когда-то говорили про методику Эйзенхауэра. Ну, у меня совсем по другому получилось)) Я недавно начал читать Грит, и наконец начал понимать все, про что ты мне столько раз говорил. Только теперь Я понимаю, да какой степени важно не только стараться, но стараться стремительно. Теперь, Я себе создаю 1 главную цель, 3-4 средних целей, и не больше 10 маленьких целей. Все теперь впадает в свое место. Я не знаю, почему Я никогда не думал глубже о таких вопросах.

Я очень жду опять говорить про жизнь. Столько интересного случается каждый день. Я никогда тебе не говорил на сколько ты всегда был моей инспирацией. За последний год, Я думаю Я начал понимать на много больше про жизнь- и теперь понимаю, как мало Я пока-что знаю.

Со следующим годом, все выглядит хорошо. Я сейчас все-таки решил поступать в ЛСЕ, и пока-что кажется получится. Я написал очень классный персонал стейтмент. Тебе понравиться)). 

С летом, планы идут хорошо. Мне так повезло, что есть столько умных людей с которыми советоваться. Сейчас пытаюсь решить между морган стэнли, силиконовой долиной, и Каспи. Как ты считаешь? Все говорят разное, но я готов на все. Я думал, что шанс попасть в морган не возможно пропустить, но при этом работа в калифорние и в Казахстане была бы на столько- же уникальной. 

Самое важное, Я хотел сказать что ты должен очень гордиться своей женой. Мама работает так сильно, и организует всех. Без нее, так бы все не шло. Я никогда не видел маму такой, понятно с разу какая она сильная. Команда Калви за тобой стоит, всегда))))

Только сейчас, Я начал понимать скольку ты меня научил. Попытаюсь тоже прочитать братья карамазовы, чтобы мы все смогли обсуждать.

<!-- {BearID:E78B39F3-EF38-4270-A122-FBCD1178ACEA-294-000040914E56A41F} -->
# Papa XCAP Annual Report Feedback
[[XCAP 2020 Annual Report]][[071 XCAP MOC]]

The numbers for 2020 are awesome. The logic behind your strategies is also sound. 

Here are some comments on the report text:

1.	On the first page showing the chart with XCapital’s performance, it is very powerful. I would delete the chart underneath it, which shows the cash position, since this isn’t so important and it detracts from the key message about your outperformance. I would delete “Annual Share Dilution” from this page too, it isn’t that significant and may be confusing (what does it mean?). Better to focus on key numbers: starting NAV per share, ending NAV per share, percentage gain, and new capital raised.

2.	I understand what “Chained” refers to, but you don’t need to put it in the title of the performance chart. It is more impressive, and a lot simpler, to just refer to S&P500, etc.

3.	In the text about the Emerging Market component, I wouldn’t say that China is a “hotbed” for growth, this is a casual term and not what an investment professional would use. Can use “locomotive” or something like that. Also, you refer to “China and South Asia”, as if they are one thing, but it’s actually different. “South Asia” usually refers to India/Pakistan/Bangladesh, while “East Asia” refers to China/Korea/Japan, and “SouthEast Asia” is Vietnam, Thailand, Malaysia, Myanmar, Singapore, etc. Since you are invested only in China within all of Emerging Markets, why not just call it your “China” allocation?

4.	NMIH: you mention you bought the shares “when they were trading at a fraction of their real value”. I don’t know what “real value” means, it is imprecise and not a normal investment term. If you want to be precise, you could say (and it would be true) “when they were trading at a fraction of their intrinsic value (book value or cyclically adjusted earnings)”.

5.	Why did you sell PayPal? Worth a brief mention.

6.	“democratic victory” is wrong spelling when it refers to the Democratic Party, the “d” should be capitalized.

7.	You say you “purchased DIS in the days leading up to launch of their new streaming service because the streaming platform was an overwhelming success upon launching in the US.” This is a contradiction. I assume you either bought it because of a conviction that it would be an overwhelming success based on the brand and content, or you bought it after initial subscription figures showed it was an overwhelming success. 

8.	You say that VRRM’s cash flow “comes from rental cars”. This isn’t correct or precise. I assume it comes from sales of specialized equipment (tracking? Geolocation? Something else?) which are used for rental cars, among other segments”.

9.	You say that “…big tech stocks have been a fantastic counterweight…” Fantastic is not a good term for investment reports, better to use “powerful”.

10.	You say that “….absurd valuations for low probability growth outcomes.” Absurd is not a good investment term either. I would say “stratospheric valuations implying growth outcomes that are low probability.”

11.	On a substantive point, I don’t think tech companies will “lose out” during a cyclical upturn. It is just that the growth advantage which they enjoyed during 2020’s economic downcycle will be sharply reduced when the economy begins growing again.

12.	A Biden victory doesn’t “unlock your Bull Case.” Maybe it increases the probability of your Bull Case being realized, but it is still a long way from actually happening.

13.	In the Work From Home section, I would delete the “affected” in the second point, just say “Exposure to trends beyond the pandemic”. The point you make is a good one, but it wasn’t clear from the title of that section.

14.	I don’t like the title “Pandemic Scrap Hunting Strategy”. Maybe something like “Pandemic Bargain Hunting/Survivor Strategy” or something like that?

Overall it’s very good!

<!-- {BearID:E0756936-8CEE-481C-86E7-33977FA4E3FE-574-00008DFD17B21144} -->
[[005 Active MOC]] [[010 Mind MOC]] 
# Papa important learnings in college
Write clearly -> those who write clearly think clearly
Critical analysis
Where to get information# Paper Ideas
<!-- #school/intro-to-finance [[Fault Lines Plan]] -->

## Key ideas from CH2:
- Using foreign demand to increase domestic production, rather than relying on increasing domestic demand to do the same
	- This creates many problems such as stagnant domestic demand and lagging domestic services and organisation.
- The importance of organizational capital for development
- Lenin’s idea of the /Commanding Heights/ being applied in the creation of large state-owned enterprises that have protected monopolies in certain industries
	- Easy way to stimulate growth initially
	- Stifles competition down the line- can lead to malaise
	- Increases prices and hurts citizens
	- Creates poor incentives for innovation and growth
- Managed capitalism as a way for the government to oversee initial growth
	- Focusing on easy-to-make but labor-intensive consumer goods like textiles and garments to start with
	- Low costs of labor and limited regulation
	- Turning point where firms in smaller countries soon have to look outwards to continue to grow, while firms in larger countries get mired in domestic markets
- The necessity of stimulating domestic consumption
- Case studies
	- Japan- initially successful, but “missed the turn”
	- Germany
	- India- hugely missed the turn because of large, dormant domestic market. P
	- Taiwan- good incentivization of exports
	- Bangladesh



Potential question:

Managed capitalism in the modern world; the example of Bangladesh.
	- The model Rajan talks about is clearly in effect in Bangladesh. Since the writing of Fault Lines, Bangladesh has become the world’s second biggest exporter of clothing. 
	- [Bangladesh raises wages for garment workers | Reuters](https://www.reuters.com/article/us-bangladesh-garments/bangladesh-raises-wages-for-garment-workers-idUSKCN1LT2UR)
	- 

[file:6E41F896-3731-4DCB-A921-AF169C7029EA-1333-000001558A578BB6/image001]

<!-- {BearID:142A945D-7123-4BA0-ADC4-FA1032F45A67-312-000047C595F20085} -->
# Paper Research Question
<!-- #school/Economics/EC431 -->
[[Daily Log 2020\/10\/15]] [[ECON431 Final Paper]]

Several thoughts: nordstream, FDI, 

What is the economic opportunity for the US to become the primary supplier of LNG for Europe, as opposed to Russia?

EU imported 108 billion cubic meters of LNG in 2019
	- Qatar: 30 bcm
	- Russia 21 bcm
	- US 17 bcm


Given the high degree of US involvement in the ongoing Nordstream 2 pipeline, what is the estimated potential windfall for US LNG exporters if America were to become the top exporter of LNG to the EU? Can these political decisions interfering with EU policies be seen as economically or politically motivated, given these profit expectations?

Look at existing druzhba pipeline

<!-- {BearID:6049770C-1A9C-469F-BD44-76999955C4F2-37181-0002452364A67BE7} -->
[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Measurement]]
# Partial Quantum Measurement
#concepts

Say we have a state $\ket{\psi}_{AB}$ can we measure just $A$ or $B$? Yes! However in doing so we break [[Entanglement]].

Partial measurements cause partial [[Wavefunction Collapse]]. See also [[Projective Quantum Measurement]]

## Measurement Process
1. Group states so that each group represents a partial measurement outcome.
2. Normalize the states
3. Check the amplitude to understand the probability.

WRONG:
1.. Write state in following form *(to measure A system w/ bases $\{\ket{0},\ket{1}\}$):*
$$\ket{\psi}_{AB}=\alpha_0\ket{0}_A\ket{u}_B+\alpha_1\ket{1}_A\ket{w}_B $$
where:
$$\alpha_0, \alpha_1 = amplitudes$$
$$\ket{0}_A,\ket{1}_A= measurement\ basis\ states$$
$$\ket{u}_B,\ket{w}_B: normalized$$

2. Get result:

| Get Outcome | With Probability | State collapses to   |
| ----------- | ---------------- | -------------------- |
| 0           | $(\alpha_0)^2$   | $\ket{0}_A\ket{u}_B$ |
| 1           | $(\alpha_1)^2$   | $\ket{1}_A\ket{w}_B$ | 


### Simple Example:
$$ a\ket{0}_A\ket{H}_B+b\ket{1}_A\ket{V}_B$$
goes to:$$\frac{1}{\sqrt{2}}\ket{0}_A\sqrt{2}(a\ket{H}+b\ket{V})_B $$
so:$$|a|^2, |b|^2=\frac{1}{2}$$
and thus the sum of squared amplitudes matches our normalization condition.

For more examples see [[Quantum Bomb Game]].
Also some extra details in [[CS333 Week 5]].

[[Quantum Questions]]

I have good details in [[CS333 Week 5]]

[[Quantum Bomb Game]] demonstrates some of the uses/ power of partial measurements.

![[Pasted image 20210414100815.png]]
![[Pasted image 20210414101116.png]]
![[Pasted image 20210414101239.png]]
![[Pasted image 20210414101249.png]]
![[Pasted image 20210414101301.png]]
![[Pasted image 20210414101309.png]]

## Normalization
Need to make sure all the amplitudes absolute value squared equal 1[[030 People MOC]]
# Paul Johnson
#people 

British historian. Writes quite impartially, very deeply researched.

Papa loves this guy

The first thing I've read from him is [[Johnson - Socrates, A Man For Our Times]]# Paypal Investment Thesis
<!-- #finance/xcap/Investments -->
Life these days is taking a decided shift into the digital sphere. People transfer money, make purchases and keep track of their accounts online more and more. We believe PayPal is well positioned to take advantage of these changes. Their main merchant platform is mature and will continue to benefit from the absurd growth in e-commerce revenue, while their new venmo offering is competitive in the sphere of p2p transfers.

The big challenges for PayPal in our eyes come from the mainstream payment processors like American Express, MasterCard and Visa. While these companies have so far failed to actively take advantage of the shifts in purchasing power online, they are now realizing their mistakes and have significant capital with which to make moves. Not only this, but Visa for example handles almost 10x the number of payments per second, so they have a clear market advantage in terms of physical market share. It is doubtful whether they will be able to convert this into the online realm.

<!-- {BearID:5D1237A6-0C4C-45A8-A2B6-736B120908E0-406-0000024D616A67CE} -->
[[011 Mental Models MOC]]
# People Overestimate What They Could Do In A Dat, But Underestimate What They Could Do In A Year
#inputs/quotes

This is one of my favorite quotes. It speaks stronly to me about people's inherent inability to forecast timelines far into the future.

This is a manifestation of the [[Planning Fallacy]] and a partial example of [[Hofstadter's Law]].[[Machine Learning TOC]]
# Perceptrons
[[CS451 Machine Learning]]
#concepts

Perceptrons are [[Linear Models]], meaning they cannot solve the XOR problem. Tries to create a hyperplane that separates values.
[[Binary Classification]]

A perceptron is a type of machine learning model that attempts to find the best hyperplane to divide data along, almost like an iterative best fit approach. Decision boundaries can /only be linear/, unlike [[Decision Trees]] and combined perceptrons form [[Neural Networks]], though I don't fully understand this yet.

[[Machine Learning TOC]]
## Uses
* Perceptron can do classification and multi class classification decently well, ranking ok and not great at regression. Use [[Linear Regression]] Model for that

## Ways to improve output
* Average perceptrons by having multiple slightly different ones, and assign weights to determine final outcome [[Averaged Perceptrons]]
* Loop over data multiple times
* Shuffle the data between loops or random restarts - avoids local optimum! Makes sure you find true best line

## Implementation
```python
def train_perceptron(y, X, y_vali, X_vali, num_iter=100, seed=1231) -> LinearModel:
    rand = np.random.default_rng(seed)
    (num_examples, num_features) = X.shape
    assert len(y) == num_examples
    w = np.zeros((num_features, 1))
    b = 0.0
    indices = list(range(num_examples))
    for iteration in range(num_iter):
        rand.shuffle(indices)
        wrong = 0
        for i in indices:
            if y[i]:
                y_val = 1
            else:
                y_val = -1

            x_i = X[i, :].reshape((num_features, 1))

            activation = np.dot(w.transpose(), x_i) + b
            if y[i] != (activation > 0):
                wrong += 1
                # we got it wrong! update!
                w += y_val * x_i
                b += y_val
        if wrong == 0:
            break
        tmp = LinearModel(w, b)
        learning_curves["Perceptron"].add_sample(tmp, X, y, X_vali, y_vali)
    return LinearModel(w, b)
```
This returns an instance of: ![[Linear Models#Implementation]]

[[050 Concepts MOC]] | [[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Period Finding Algorithm
#concepts/cs/quantum  

This is how we do the [[Factoring Algorithm]].

The period finding algorithm seeks to find the period of an algorithm, or how many x values there are before repetitions.

We assume there are many repetitions in a given domain.

## Quantum Solution
The quantum solution involves using a QFT Gate [[Quantum Fourier Transform Gate]] to put n states into superposition. 

![[Pasted image 20210427101258.png]]

### $\ket{\psi_1}$ - Create superposition with QFT
QFT usually also adds a phase, but not in this case.
$\Large\ket{\psi_1}=QFT_n\otimes I \ket{0}\ket{0}$

### $\ket{\psi_2}$ - Apply $U_f$
$\Large\ket{\psi_2}=U_f\ket{\psi_1}$
![[Pasted image 20210427103246.png]]

### $\ket{\psi_3}$ - Partial measurement, regroup
Take a measurement on B. regroup and eliminate the B term. We now just care about A register.
$\Large$
![[Pasted image 20210427103328.png]]

### $\ket{\psi_4}$ - Apply QFT again
Applying QFT adds another summation. Also lots of reshuffling. Order of summations gets flipped. End up using geometric formula to eliminate one of the possible cases, leaving us with a prob of 1/r for each option $\ket{y}$
![[Pasted image 20210427103425.png]]![[Pasted image 20210429104242.png]]![[Pasted image 20210429104424.png]]


## Query Complexity
* Quantum: O(1)
* Classical: O(sqrt(r))

## Time Complexity of Factoring
Q: O(log^2(N))
C: e^(O(log_2(N)^1/3))

## Theory
This algorithm creates a superposition of all possible input states using a QFT. Then, it applies $U_f$

#todo!!![[051 Math TOC]] | [[Linear Algebra TOC]] | [[Matrices]]
# Permutation Matrix
#concepts/cs/ml 
Permutation matrices are square [[Matrices]] that have exactly one entry for each column and row, with zeros elsewhere.

All permutation matrices are also 

An example of such a matrix is the [[CNOT Gates#Matrix Representation]]# Personal Statement
[[007 Personal Development MOC]]
I am a double Economics and Computer Science major in my second year at Middlebury College in Vermont (USA). My greatest skill throughout my life has been my ability to apply my knowledge to practical, real world problems, and I have always enjoyed the process of seeing my ideas and concepts formalize and translate into the real world. Early on, this translated into a passion for coding and solving software problems, and gradually developed into an interest in the deeper, more technical aspects of the fields of technology and business. I aspire to use the tools I am now learning to create, develop and market my own ideas in the future. At this point in my academic career, my main goal is to pick up as many skills as I can, with a particular focus on entrepreneurship, economics and statistics. I believe LSE fits perfectly into this picture, as I will be able to supplement the relatively broad courses I have taken at Middlebury with courses focused more on the underlying maths and mechanics of economics.

Since arriving at Middlebury, I have focused on building and maturing my quantitative skills by taking classes in economics, discrete math, statistics and theoretical computer science. To supplement these highly technical courses, I have maintained a commitment to developing my ability to be creative and to write eloquently, by taking classes such as architectural design and Russia’s imperial borderlands, which pushed my understanding of writing and the design process to new levels. I also really enjoy learning more about international politics and economics; I strongly believe that to see some of the world’s greatest success stories, we must look outside of the continental US or Britain. Growing up going to school in Moscow and at Wellington College in the UK certainly set me up with this view.

LSE fits perfectly into this puzzle, with the available courses on Corporate Finance, Accounting, and International Economics that I intend to take if my application is accepted. I will devote myself to drilling down deeper into these subjects than is possible at my own university. My dad told me from a young age how formative his own LSE experience was; since then, I have worked to make attending LSE a reality for myself as well.

Last summer, I spent two months working at one of the most exciting e-commerce startups in Bangladesh focusing on this exact idea. I was tasked with solving the issue of recurring fraud on the company’s ride sharing platform, so I created a new algorithm for sorting drivers after a request for a ride was made, which effectively decreased the incidence of fraud on the system. I also applied my knowledge of graph theory to conceive of and eventually create a tool to visualize and detect fraud as it was occurring — something the company had not been able to do.

<!-- {BearID:DEF66B0F-9EBC-466F-B464-3CF65A367711-314-000010637E9C6E1B} -->
[[Quantum Computing TOC|Quantum Computing]]
# Phase Kickback
#concepts 

When the phase of a state gives you information about the overall quantum system. An example of this in use is [[Deutsch's Algorithm]]

![[Pasted image 20210426202045.png]]

## WHY?
![[Pasted image 20210512094551.png]][[Linguistics MOC|Linguistics]]
# Phonology
#concepts/linguistics 

The study of sound as it relates to [[Linguistics MOC|Language]].

**Defn:** Formation rules that govern how the sounds of languages are put together.

An accent is when someone carries over the rules of one phonology to another language or dialect.

There are lots of sound options that make language efficient.[[100 Projects MOC]]
# Photon 
#projects/photon

- Market sizing- what is the size in EX new england?
	- Product validation sources- ex reviewers, who would say its the most awesome?
	- That would deliver momentum for crowdsourcing
- Spend time on channels like amazon researching competitors, learn about the companies and explain how we position relative to everything else.



Photon is a portable smart light. It actively adapts to your needs and lifestyle by delivering different colors at different times to make you more wakeful in day, while sleeping more peacefully at night. Its high-capacity battery gives you the freedom to take it wherever, whenever. Whether you want to light a romantic dinner, go camping, or throw a beach party; Photon’s easy pairing system means you never have to worry about light again. Our proprietary platform allows multiple units to join the same network and communicate automatically. We are rethinking today’s lighting solutions using the latest technologies to create a modern ecosystem of easy-to-use and intelligent lighting.

- Having more energy or productivity/ better sex life
- Energy benefits/ health
	- jetlag
	- Business travelers

<!-- {BearID:280843F9-7ED8-4CC5-9F5E-16B15BB5AF1F-21279-000123032D8FA173} -->
[[100 Projects MOC]]
# Photon code structure
#outputs/code

- Lighting Controller
	- Initiates lighting actions
	- Lighting actions contain code for series of events to take place
	- Each event instructs a subgroup of LEDs to do something

[[Linguistics MOC]] | [[Natural Language Processing TOC|NLP]]
# Phrase Structure Parse
#concepts/linguistics #concepts/cs/ml 

This is a structure used to objectify the structure of a sentence. It isn't an easy problem, and can lead to some interesting ambiguities:
- "Teacher strikes idle kids" or
- "Violinist linked to JAL crash blossoms"

I should study more [[Grammar]]. 

![[Pasted image 20210805103148.png]]# Plan for regression project
[[ECON211 Regression]]
#school
Introduction


Start with a brief picture of the current state of the US from a socioeconomic perspective. Our context is being a decade after the great recession- we have seen only good times for the most part over the last 10 years. Yet, the situation has not improved commensurately for different kinds of people. Wage growth has been centered around the top 10%, and returns to education are only decreasing.

From here, build a picture of this increase in inequality tying into slower growth and a less robust economy, despite 10 years for the bull to run. https://www.epi.org/files/pdf/136654.pdf <- study on inequality and growth for proof. 

[Why income inequality is holding back economic growth, in one chart - MarketWatch](https://www.marketwatch.com/story/why-income-inequality-is-holding-back-economic-growth-in-one-chart-2018-04-05)
[Inequality, Growth, and Investment](https://www.nber.org/papers/w7038) Counterargument for why reducing inequality is important 

From here I link right into peoples’ ability to find a job. We can focus in on the question of inequality being tied closely into change in wages and the ability to work. Clearly, being able to work is not even a variable, but a prerequisite for reducing inequality. Then I would link right into (Barro 1999). He concluded that inequality helps growth in developed countries based on ideas around the kuznets curve and others, but essentially it was working on old data from an old time. Now that times have moved on, we have data that shows that this is not the case. This is where our literature comes in, where we look at the factors influencing people’s ability to work, which is clearly a driver of inequality. We conclude that there are systemic differences by race, and things are quite strongly unequal.  Mention study on employers’ replies to racial names.

[Employers’ Replies to Racial Names](https://www.nber.org/digest/sep03/w9873.html)
“a white name yields as many more callbacks as an additional eight years of experience.”


Beautiful introduction:

A substantial literature analyzes the effects of income inequality on macroeconomic performance, as reflected in rates of economic growth and investment. Much of this analysis is empirical, using data on the performance of a broad group of countries. This paper contributes to this literature by using a framework for the determinants of economic growth that I have developed and used in previous studies. To motivate the extension of this framework to income inequality, I begin by discussing recent theoretical analyses of the macroeconomic consequences of income inequality. Then I develop the applied framework and describe the new empirical findings. 


Kuznets’s idea, developed further by Robinson (1976), focused on the movements of persons from agriculture to industry. 







A substantial literature analyzes the impacts of wealth and income inequality on growth and development in the United States. Much of the old literature (pre-2008) was focused on the idea that inequality could be good for growth in certain situations, particularly in developed economies (Barro 1999). Many of these ideas built upon Kuznets’s idea of the “Movement of persons from agriculture to industry,” and how this necessary transition would inevitably bring about an era of inequality, before the whole country enters the latter stages of development and inequality disappears. In practice, however, many of these ideas are not holding up to the current economic situation of the United States.

We know that a systemic inequality exists between people along lines of race and ethnicity in the United States (Bertrand 2003), specifically centered around peoples’ ability to find gainful employment. A significant older literature analyzes the link between inequality and race, however a strong argument can be made that the current economic and job market creates a very different background for racial-based employment opportunity. With the unemployment rate nearing 3.5%, the US economy has not experienced this level of employment in many decades, but labor force participation statistics obscure part of the story. The reality is not as rosy as the basic statistics may seem. There still exist clear divides by race, education and housing status, that mean this economic recovery has not been an equal recovery for all.

Following the basic ideas set out by Donald Freeman (2011) and others as detailed above, we examine the effects of race, marital status, education, health, disability, and housing status on ones chance of being employed. All of these variables arguably have an impact on ones chance of employment. Health will clearly have an effect on employment, as sick people are unable to work commensurately to healthy people, if at all. Having a stable family can also be argued to significantly increase one’s prospects of being employed, as support from home makes it easier to maintain a full time job. Disabilities fare similarly, but in reverse, as disabled people often have significant difficulties in a large swathes of occupations, especially labor-based ones. Living in public housing is used to control for any skew from being born in different neighborhoods of the country, which have a significant impact on nearby employment opportunities.[file:234B418F-5713-4E26-A913-F4AEFE6EA24D-2531-0000034516DCD9AA/Academic Transcipt.pdf]


Hello this is the best way to work. Full size keyboard, shortcuts and anything you want

<!-- {BearID:A3645335-4B8A-43B9-B5A1-DBFD6145ED41-287-00000003D7C4BEEA} -->
[[Cognitive Biases TOC]]
# Planning Fallacy
#concepts/cognitive_biases 

Ok, turns out it exists folks. Can't make shit around here...

Anyway, this referes to people's difficulty with time predictions when complex processes are involved. This is what is referenced by [[Hofstadter's Law]]

This bias is one I've coined myself, and it deals with people's perception of time. People are inherently bad at forecasting into the future, and our [[Brain]] physiology drives us to be overconfident about our abilities in the short term while being underconfident in our abilities in the long term. This stems from one of my favorite quotes, "[[People Overestimate What They Could Do In A Day, And Underestimate What They Could Do In A Year]]"

This is actually a very interesting concept, with lots of related prior work. Think [[Hofstadter's Law]] and [[Brooks's Law]].# Plato
#concepts #concepts/philosophy

Documented the life of [[Socrates]], was his student.
Has famous writings - I should read.
# Playlists
#my/music
[[005 Active MOC]][[]]

[[Back For More]]
[[A New Force]]
[[Bene]]
[[Bickback]]
[[Blues
[[Bump 2.0
[[Bump Chill
[[Driving Hype]]
# Portfolio Adjustment Thoughts
[[XCAP Writing]] [[XCAP Current Positions]]

* *NMI:* Good cyclical stock but it's now run up a lot, meaning we're really holding a lot more housing market risk than we were before. With the current trajectory of the pandemic, although it will be over pretty soon, things are not looking great for the moment forbearance programs end in March. Although the housing MKT has so far been propelled by the lowest mortgage rates we've ever seen, idk how sustainable this will be. Obviously there's arguments for both sides here, but the higher the stock goes, the better of an outcome in the housing market we're holding, meaning we are really holding lots of risk that it goes the other way, for a much less substantial reward at this point. I'm still bullish overall, but this is a good place to raise cash now.

<!-- {BearID:BDE792C3-9C87-4987-8404-E90FD6C8A07C-5857-00012E69E59AAFF5} -->
[[Linguistics MOC]]
# Poverty of the Input Argument
#concepts/mental_models #concepts/linguistics 

This was used by [[Noam Chomsky]] to demonstrate that children have an innate sense of [[Grammar]] built into them. Namely, they were able to generalize questions from sentences such as "The man who is tall is in the room" -> "Is the man who is tall in the room?" rather than "Is the man who tall is in the room?"[[095 Journals MOC]] [[040 Interests MOC]]
# Pp download
West wing
Silicon valley last 2 seasons
Roots tv show
Brians song
Master and commander
[[Linguistics MOC]]
# Pragmatics
#concepts/linguistics 

The study of the use of [[Linguistics MOC|Language]] in conversation.

How do we understand language in context, using our knowledge of the world?

## Cooperative Principle
The idea that a conversation partner is working with us to further meaning.

Think of "Give me a hand"

Our knowledge of human behaviour, interactions and relationships shapes our understanding and interpretation of language in realtime. This is perhaps one of the hardest things to impart on a computer.# Presentation thoughts
[[ECON431 Economics of the EU]]
#school

### Alice Hudson - Energy balance
	- Good use of sources - liked the data used to show change in energy balance
	- Idea: expand more on the idea of societal/ cultural norms - how can these be quantified?
	- Thought: can you really draw conclusions on growth purely based on the energy mix/ balance?
	- Question: who pays for accomplishing environmental goals?

### Adrienne Coslick - French Women’s participation in populist movements
*+:*
	- Interesting niche topic
	- Lots of great context discussion

*-:*
	- No clear purpose to paper
	
### Thomas Tarantino - Investigating the Italian jobs act’s effects on unemployment

<!-- {BearID:0564D038-CDAC-4BF4-83FE-FDE4AF6F97C2-16983-000137C6CA3039B9} -->
# Presentation
<!-- #finance/xcap/Presentations -->
Need to give a slick and fresh view of rise. Must introduce practical changes that will seriously change the way we run it. 
[[Shareholder Meeting]]

### New vision
The rise fund will focus on the core concept of innovation as its investment ideal. We will find companies at the nexus of the technology, aerospace, food and services industries that we believe have the potential to redefine the way people live or do certain things.

### Organisational structure
*Existing roles:*
* Chief Executive Officer (CEO): Michael Calvey
* Chief Financial Officer (CFO): Carl Langaker
*New roles:*
* Chief Investment Officer (CIO)
* Analyst positions

## Moving forward
### Weekly Meetings
### Shared Knowledge
### Shared Research

<!-- {BearID:4C95CDB7-4225-498D-8761-BBB1F753F8EF-7124-000015BED50BB1CA} -->
[[Cognitive Biases TOC]]
# Priming
#concepts/cognitive_biases 

The priming bias dictates that people's decisions are influenced by the order in which information is presented. This is closely tied to [[Framing]], except that focuses more on how information is presented. [[053 Computer Science TOC]]
# Probabilistic Computation
#concepts 

Bits can be interpreted as having probabilities. This can be simulated on classical systems, but is more powerful with [[Quantum Computing TOC|Quantum Computing]]

In classical probabilistic computation, we use [[Left Stochastic Matrices]] to represent gates.

In quantum states, we use [[Quantum Gates]] (Unitaries)
![[Pasted image 20210415103730.png|600]][[Algorithms And Complexity]]
# Problem Set 8 Reflection

This problem set was relatively successful. In the first question, I was correct up to some of the sum evaluations later on. I clearly need to revisit my ability to expand and work with sums, and this will be one of my main topics for further study. For the second question, my issue was quite different. I took a very different approach to the one taken in the actual answer. My algorithm was recursive, and would recursively calculate the maximum possible price from every proposed maximum-value cut. In my eyes, my algorithm is still quite successful, although most likely it was slower than I am anticipating. My approach attempted to fit the knapsack solution to a multi-part approach. The problem with my reasoning here was not an error of application, but rather selecting the most efficient algorithm for carrying out the task at hand.

<!-- {BearID:F46F5DDA-CDA8-4ED0-A9EB-C780EA830DD6-21279-00004019D6F188B3} -->
# Product spec
[[Photon Project]]

* Figure out what the optimal chip to run the whole thing is
* Decide required connectivity features
*

<!-- {BearID:28513C38-447A-4B54-A6F3-0119A76F895A-6895-00000536414B199B} -->
[[076 Algo MOC]]
# Production Server
#outputs/code #projects/old/xcap


IP: 64.227.13.200

### Virtual Env
`source /opt/myenv/bin/activate`
`deactivate`
# Progenics Post-Mortem
<!-- #finance/xcap/Post-Mortem -->

<!-- {BearID:8BD9C96B-5998-40A0-AB1F-169C5EFEAA2B-406-00000228F92E3F8D} -->
[[Quantum Mechanics TOC]] | [[Quantum Measurement]] | [[Partial Quantum Measurement]]
# Projective Quantum Measurement
#concepts #🌱 

1. Take complete measurement $M=\{\ket{\phi_1}, ... \ket{\phi_n}\}$. There are n possible outcomes.
2. Group basis states into disjoint subsets - no repeated states
![[Pasted image 20210506100145.png]]
4. Create Projector from each subset
![[Pasted image 20210506100204.png]]

![[Pasted image 20210506100211.png]]
Now fewer outcomes!
Check onenote and transcribe!!

![[Pasted image 20210506100241.png]]

Think about it as collapsing not all the way to classical information, but still quantum! [[Wavefunction Collapse]]

## Big Idea
Partial collapse tells you about the error (whether it occurred or where) without fully collapsing the quantum state!

When you measure with respect to bases where one of them is a full error and the other is the correct state, you detect errors because the state will collapse to one or the other. Even if the actual error was just a small rotation. [[Quantum Error Correction]]

#todo 
- [x] [[Projective Quantum Measurement]]!!! Need to transcribe class(es) lmao

## Error Correction
Doing projective measurements on systems of "logical qubits" - where more than one qubit is used to represent a single qubit, allows us to correct X errors, Z errors and Y errors. It can only correct when one of 9 qubits is wrong, any more and it stops working.

It doesn't matter which qubit had the error though, it's much easier to correct than identify!

[[Quantum Error Correction]]

Quantum Error Correction is nearing critical threshold - we need to keep improving codes as well as failure rates.[[Standard Model TOC]] | [[052 Physics TOC]]
# Protons
#concepts 

Small sub-atomic particle that makes up an [[Atoms]] [[Nucleus]].

Positive charge

Weighs 2000x more than electron# Pset 3 Reflection
[[006 School MOC]]
#school
This problem set was fairly successful. I struggled a lot with many parts of question 2, and my flailing efforts revealed significant inadequacies in my log operations. I would often approach problems correctly, but arrive at the wrong answer due to poor log manipulation. This time, I focused on spending longer to really understand what a question was asking, and how the information we are given feeds into that. On that front, I was very successful since I did not misinterpret questions in the same was as last time. I also struggled with 3b, although it seemed to be quite easy at the time. This revealed a deficiency in my understanding of the MWIS, since I still don’t understand clearly why my counterexample doesn’t work. I will need to follow this up with additional work. The proof was quite successful this time, definitely an improvement over my earlier attempts. Moving forward, I need to focus on stratifying my thought process by introducing more concrete variables and formulae to help prove subproblems more unequivocally. 

Overall, my thought process and approach was significantly improved from the two earlier problem sets. I have a clear set of goals to focus on, specifically creating technical supporting materials for proofs and better log comprehension. My overall thinking is now in the right vein, which is a very promising improvement.

<!-- {BearID:87F35A9C-67F8-4CB3-B6D2-32B9F5F92023-406-00008CB366A7DD83} -->
# Pset 9 Reflection
[[Algorithms And Complexity]]
This problem set was pretty good overall. I really liked the way I approached the first question. My grasp of loop invariants is clearly improving over the last few problem sets. My graphical display made it much easier for me to think about, and hence easier to solve. 

My second problem was a little more problematic. I started thinking along the right lines by thinking of the total number of recursive calls from each node, but I used the average number of edges value rather than the edges from the first node. To do this better in the future, I should try to stop and rationalize what the algorithm will look like at the beginning as well as at any given point during running.

<!-- {BearID:6CC9B7C9-7AF4-4CF2-A784-7778C4E7AC37-21279-00008C9284D50E72} -->
[[Linguistics MOC]]
# Psycholinguistics
#concepts/linguistics 

The study of how [[Linguistics MOC|Language]] is processed in realtime. concerns the [[Brain]], [[054 Neuroscience MOC|Neuroscience]] and has 
applications to [[Natural Language Processing TOC|NLP]]. 

Our brain builds the illusion that words are separate when spoken, however the actual waveform is linked. Word boundaries are pretty intersting and complicated. Not embedded in the waveform itself. Language is generally filled with [[Ambiguous Sentences]], and can't be eaily judged. [[052 Physics TOC]] | [[055 Coding TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# Qiskit
#outputs/code 

## Simple coin flip Circuit
[[Hadamard Gates]]
Code:
```python
import numpy as np  
from matplotlib import pyplot as plt  
from qiskit import QuantumCircuit, transpile  
from qiskit.providers.aer import QasmSimulator  
from qiskit.visualization import plot_histogram  
  
simulator = QasmSimulator()  
  
# Create quantum circuit with 2 qubits, 1 traditional bit  
circuit = QuantumCircuit(2, 1)  
  
# Applies Hadamard basis to qubit 0, putting it ifnto superposition state  
circuit.h(0)  
  
# CNOT entangles control qubit 0 with target qubit 1  
circuit.cx(0, 1)  
  
# Measuring only last qubit and saving ith measurement in ith classical bit.  
circuit.measure([1], [0])  
  
# compile the circuit down to low-level QASM instructions  
# supported by the backend (not needed for simple circuits)  
compiled_circuit = transpile(circuit, simulator)  
  
# Execute the circuit on the qasm simulator  
job = simulator.run(compiled_circuit, shots=1000)  
  
# Grab results from the job  
result = job.result()  
  
# Returns counts  
counts = result.get_counts(circuit)  
print("\nTotal count for 00 and 11 are:", counts)
  
# Draw the circuit  
d = circuit.draw(output="mpl")  
plt.show()
```

Diagram:
![[Pasted image 20210412165822.png]]# Quantum Algebra
#concepts 
[[Quantum Mechanics TOC]]

[[Todo]] #todo Transcribe into Matlab from bear image!
Whenever bras and kets have different subscripts, can switch their order!

# Conjugate Transpose (dagger)

# Tensor Product
This is how we take a combined measurement of different photons/ qubits

[[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# Quantum Algorithms
#concepts 

Can think of quantum algorithms as exploring paths that different basis states take. This becomes powerful since we can first put particles into [[Superposition]], allowing us to explore multiple paths simultaneously. With a [[Probabilistic Computation]] machine, it would simulate taking all the paths simultaneously.

The goal of quantum algorithms is to do some calculations while preserving quantum information. Think of them like probabilistic circuits!
[[Quantum States]]
[[Quantum Gates]]
[[Quantum Measurement]]

There are a few types of quantum algorithms
* Phase kickback style
* Period finding style (hidden subgroup) See [[Period Finding Alrogithm]]


## Algorithms
[[Deutsch's Algorithm]]
[[Factoring Algorithm]]# Quantum Bomb Game
#concepts
[[Quantum Mechanics TOC]][[Quantum Measurement]]

Imagine there is a box that explodes some of the time when a photon goes through. We want to figure out ahead of time if a box will explode, without blowing that shit up.

### Beam Splitter
The [[Beam Splitter]] is one of the possible [[Quantum Gates]]. It selectively allows photons to pass based on their polarization.
![[Beam Splitter#Beamsplitter effect on Quantum States Orthonormal Bases]]


This is one of the possible [[Quantum Gates]]

### The Bomb
A single photon encountering the bomb makes bomb explode. Bomb forces a [[Partial Quantum Measurement]] and will make the [[Photons]] go through measurement and pick a discrete direction of travel - either horizontal or vertical.

### Diagram
![[Pasted image 20210323104429.png]]

The bomb forces a [[Partial Quantum Measurement]]! If the photon chooses it was going down, bomb explodes! Otherwise bomb does not explode. If the bomb *does not* go off, detector will go off with 50% prob.


Otherwise the bomb could also be a dud. In this case it does not explode, and is never registered

[[Quantum Gates]]
[[Quantum Measurement]]
[[Quantum States]]


### Advanced Protocol
![[Pasted image 20210330093812.png]]

When the photon enters the beamsplitter, it will end up in both places at once, following both arms at once. The photon parts continue and hit the mirrors, reflecting off and changing their arm of travel.

We now need to make sure there is one term with $\ket{V}_A$ and one term with $\ket{H}_A$. If not, gather terms. We are now ready to conduct measurement

THEN the bomb is encountered. This does only a partial measurement. If state collapses to bottom arm, bomb explodes. Otherwise, state collapses to top arm and circuit completed. Is reset if it passes through.

**States:**
* $\ket{0}_P\ket{V}_A$ w/ prob $cos^2(\pi/n)$
* $\ket{1}_P\ket{H}_A$ w/ prob $sin^2(\pi/n)$

Then the probability of never exploding $=(cos^2(\pi/n))^{n/2}=1-\frac{\pi}{2n}$

If detection: live bomb
If no detection: dud[[Quantum Computing TOC]]
# Quantum Computer Hardward Implementations
#concepts/cs/quantum #🌱 

Lots of questions remain about which architecture will end up having lowest error rates and be the most scalable.

Quantum volume: Number of [[Qubits]] times depth of [[Quantum Gates]]

## What do we need to build a quantum computer?
- Qubits (eg photons)
	- Initializing states
	- Must be good (low error rates), maintains quantum info, isolated from environment, long lived "coherence time"
- Gates (single & 2 qubit gates)
	- Fast
- Measurement


## Technologies
### Superconducting Qubits
![[Pasted image 20210518103403.png|500]]
Transmon qubits - two-level quantum system
Use microwave photons to 
Electrons are either in or not in a box, can be partially in ie superposition
Can only do multi-qubit connections between adjacent qubits
[[Planck's Constant]]
#### Disadvantages:
- Limited connectivity

#### Main Players:
- IBM ibm.com/quantum-computing/what-is-quantum-computing/
- Google
- Rigetti

### Trapped Ion Qubits
- Seem the most exotic
- Essentially single atoms that are levitated above a chip
- Shuttling allows you to move qubits around

#### Main Players:
- Honeywell
- IONQ ionq.com/technology
- AQT

### Photon Qubits
Don't use polarization, instread entangle photons by using number of photons
Generally very linear circuits, fixed depth. Can still program because gates can be turned on/ off. 
#### Advantages:
- Room temperature!
- Can fabricate specialized chips pretty easily
#### Disadvantages:
- Less programmable than other architectures.
- Less scalable

#### Main Players
- Xanadu xanadu.ai/hardware
---
aliases: ["Quantum Computing"]
---
[[053 Computer Science TOC]] | [[052 Physics TOC]] | [[Quantum Mechanics TOC]]
# Quantum Computing Bridge
#TOC
#concepts
Quantum computing is the application of [[Quantum Mechanics TOC|Quantum]] concepts to help solve challenging problems that classical computers cannot solve.

[[CS333 Quantum Computing]]

Quantum computers operate on [[Qubits]] rather than classical bits. These particles are able to store a superposition of all possible states - creating infinite possible states a qubit could be in. 

It is not possible to easily measure a qubit and learn this state, however. The best you can do is what is taught in the idea of [[Quantum Measurement]], where you force the qubit to undergo [[Wavefunction Collapse]], exiting its [[Superposition]] state and taking one of orthonormal measurement bases used for the measurement

[[CHSH Game]]
[[Quantum Gates]]
[[Quantum States]]
[[Quantum Computer Hardware Implementations]]

## Qiskit
Nice library created by IBM. Big plans, can currently simulate or run on the real hardware.
[[Qiskit]]

## Algorithms

### Deutsch's Algorithm
[[Deutsch's Algorithm]]
[[Factoring Algorithm]][[052 Physics TOC]] [[Quantum Mechanics TOC]] [[Quantum Computing TOC]]
# Quantum Cryptography
An application of Q mechanics
Develops intuition for Q measurement

[[CS333 Quantum Computing]] -> [[CS333 Week 1]][[CS333 Week 3]]
[[Quantum Mechanics TOC]][[Quantum Algebra]]
#concepts


# Content
* How to send a bit string and encode it?
	* Alice wants to send bit string to Bob
	* Eve is trying to eavesdrop
	* How can Alice communicate the info over an open channel and stop Eve from hearing?

Public & private key cryptography
*Traditional Cryptography w/ classical computing:*
[[Cryptography]]
*Computational hardness assumption*

## Public Key Cryptography
1. Bob generates 2 keys, k_pub & k_priv
2. Bob shares public keyr
3. Alice has some message m to send to Bob, puts it through a function f(m, K_pub), giving us an encoded message m_bar
4. Alice sends m_bar to Bob, Eve also intercepts
5. Bob uses a function f'(m_bar, k_priv)=m

This only holds up because we assume factoring is hard, or the computational hardness assumption!

## BB84 Cryptography Protocol
1. Alice chooses random polarization a, b in {0, 1}^n randomly at the ith second, sends photon a_i basis bit and b_i bit
2. Bob chooses random filter c_i in {0, 1}^n randomly at ith second
		1. c_i -> 0: vertical filter -> photon detector
		2. c_i -> 1: right diagonal filter -> photon detector
3. Bob records outcome in string d
	1. Detection d_i = 0
	2. No detection d_i = 1
	
* QUESTION: Is “random” 50/50?

* if a == c: polarization always in the same plane, b_i = d_i
* else if a != c: b_i = d_i the same half of the time
* So if bob gets a detection he doesn’t yet know whether 

4. Alice and Bob publicly announce a and c (basis choice), keeping b and d private
5. Throw out all bits of b and d where a != c, leaving b’ and d’, both still private and they should match

* What about Eve!?
*Eve possible strategy:*
		* Eve chooses string e in {0, 1, 2}^n
			* If she measures e == 0, she sets up a vertical polarizer 
				* If she gets a detection through it she passes on a vertical bit to Bob and sets f_i = 0
				* If she gets no detection, passes a horizontal photon and sets f_i = 1
			* If she measures e == 1, she sets up a diagonal polarizer
				* If she gets a detection, passes right diagonal photon to Bob f_i = 0
				* If she gets no detection, passes a left diagonal photon to Bob f_i = 1
			* If she measures a 2, she does nothing
What does Eve’s interference achieve? Without interference, b’ and d’
-> By trying to get info out, Eve interferes and changes what Bob records, making him throw out his correct result 50% of the time when Eve picks a different basis from Alice & Bob’s
-> The more Eve tries to interfere, the more b’ doesn’t equal d’

6. Alice and Bob make public subset of bits of b’ and d’ and compare. 
	1. If enough disagreement, ABORT
	2. If they mostly agree, know Eve hasn’t made many measurements
7. Do error correction: s~ = b~ = d~
	* Note: error correction shortens string!
8. Alice and Bob do privacy amplification s~ -> s
9. s is the secret key for XOR secret key protocol! This way they ensure they are able to share a sufficiently secret key

	* Eve can’t win, but even then she has to be fast af
	* Photon creation is probabilistic, so Eve has some wiggle room

## Key Particle - Photon
[[Standard Model TOC]] [[Photons]]
Photons (think of light as a wave) are the core particle used for quantum cryptography applications. Especially good for crypto, not really used anywhere else
		* Fast
		* Lots of existing infrastructure - fiber optic cable


## XOR (Exclusive OR)
See [[Logic Gates]]
XOR is addition mod 2!!
Useful cause you use same function to encrypt/ decrypt
Lossless
Any input could be any output

x	y	x(xor)y
0	0	0			
0	1	1			
1	0	1
1	1	0

## CHSH Game
Two qubits made from diamond nitrogen vacancy method, using lasers
[[CHSH Game]][[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Quantum Error Correction
#concepts #🌱 

Errors are the main reason we don't already have better quantum computers!

## Classical Errors
Happens at rate of 1 bit flip / 4GB / day because of cosmic rays. See [[Radiation]]

### Classical Error Correction
Run each operation 3 times, make sure each agrees

## Quantum Errors
- Laser shape/ frequency/ focus bad.
- Other atom bounces into your atom
- System is actually qudit - represented as binary qubit, however in reality state is infinitely more complex. Higher dimensional
- Magnetic fields
- Electric fields
- Heat (anomalous heating)

### Quantum Error Correction
![[Pasted image 20210504100351.png|500]]
$\Large\ket{\psi_1}=(a\ket{0}+b\ket{1})\ket{00}=a\ket{000}+b\ket{100}$

$\Large\ket{\psi_2}=a\ket{000}+b\ket{110}$

$\Large\ket{\psi_3}=a\ket{000}+b\ket{111}$

We now have 3 physical qubits representing 1 logical qubit!

Errors occur so fast you cannot correct before another happens!

![[Pasted image 20210504104402.png]][[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Quantum Errors
#concepts #🌱 

Errors are the main reason we don't already have better quantum computers!

## Classical Errors
Happens at rate of 1 bit flip / 4GB / day because of cosmic rays. See [[Radiation]]

### Classical Error Correction
Run each operation 3 times, make sure each agrees

## Quantum Errors
- Laser shape/ frequency/ focus bad.
- Other atom bounces into your atom
- System is actually qudit - represented as binary qubit, however in reality state is infinitely more complex. Higher dimensional
- Magnetic fields
- Electric fields
- Heat (anomalous heating)

### Quantum Error Correction
![[Pasted image 20210504100351.png|500]]
$\Large\ket{\psi_1}=(a\ket{0}+b\ket{1})\ket{00}=a\ket{000}+b\ket{100}$

$\Large\ket{\psi_2}=a\ket{000}+b\ket{110}$

$\Large\ket{\psi_3}=a\ket{000}+b\ket{111}$

We now have 3 physical qubits representing 1 logical qubit!

Errors occur so fast you cannot correct before another happens!

![[Pasted image 20210504104402.png]][[Quantum Mechanics TOC]] | [[Quantum Computing TOC]] | [[Quantum Gates]]
# QFT Gates
#🌱 #concepts/cs/quantum  
![[Pasted image 20210427101315.png]]

[[Period Finding Alrogithm]]
[[Phase Kickback]][[052 Physics TOC]] | [[Quantum Mechanics TOC]]
# Quantum Gates
#concepts
[[CS333 Week 4]] [[CS333 Week 5]]

[[Beam Splitter]]
[[Mirror]]
[[Interferometer]]

Quantum gates are *reversible* quantum operations. **NOT** a measurement! Must be bijective (one-to-one in both directions).

A gate operation can be thought of as being somewhat analogous to a unitary matrix operation on a vector.
* U is unitary if $UU\dagger=I$
* If $U\dagger U = I$ 

If U acts on $\ket{\psi}$, state becomes $U\ket{\psi}=\ket{\psi'}$
For a bra: $\bra{\psi'}=\bra{\psi}U\dagger$

Always reversible since can use the conjugate transpose to reverse! Can always apply $U\dagger$ after applying $U$ which essentially just applies the identity matrix.

$$
\bra{\psi}U\dagger U\ket{\psi}=\bra{\psi}I\ket{\psi}
$$

## Single Qubit Gates

### Pauli gates
The Pauli gates are the fundamental building block operations of quantum circuits.
See [[Outer Products]].

- $\Large I:\ket{0}\bra{0}+\ket{1}\bra{1}=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}$
	- The identity transformation. Keeps things the same
- $\Large X:\ket{1}\bra{0}+\ket{0}\bra{1}=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}$
	- Negation operation
- $\Large Y:-\ket{1}\bra{0}+\ket{0}\bra{1}=\begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix}$
	- $\Large Y=ZX$ A combination of negation and phase change
- $\Large Z:\ket{0}\bra{0}-\ket{1}\bra{1}=\begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}$
	- Changes relative phase of a superposition in the standard basis


### [[Hadamard Gates]]
Creates superposition state
$\Large H=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1 \\ 1 & -1\end{pmatrix}$

$\large\ket{0}\rightarrow\ket{+}$
$\large\ket{1}\rightarrow\ket{-}$

### T Gates
Add complex phases
$\Large T=\begin{pmatrix}1 & 0 \\ 0 & e^{i\pi/4}\end{pmatrix}$

## Multi Qubit Gates
### CNOT (CX)
Control not, basically a conditional flip operator that uses one qubit as the control and flips the other if it is 1.
$\Large CNOT=\begin{pmatrix}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0\end{pmatrix}$

### QFT - [[Quantum Fourier Transform Gate]]
$QFT_t$ is a t*t matrix 
Defined as:
$\Large QFT_t\ket{x}=\frac{1}{\sqrt{t}}\Sigma^{t-1}_{y=0}{e^{2\pi ixy/t}\ket{y}}$

So we will get outcome $\ket{y}$ with probability $1/t$! Since amplitude is $\frac{1}{\sqrt{t}}$

With a 1 qubit system use 2x2 [[Matrices]]
With [[Multi Qubit Systems]] w/ 2 qubits use 4x4 [[Matrices]]





3. Ketbra notation [[Linear Algebra TOC#Inner Product]] but Outer product! [[Inner Products]] [[Outer Products]]
$\ket{\psi}\bra{\phi}$


It's generally the easiest to use ketbra notation and just remember that there's a whole bunch of matrix multiplication 

### Unitary on 1 part of a 2-qubit state
![[Pasted image 20210330103230.png]]
![[Pasted image 20210330103406.png]]

### Example
[[Quantum States]]
$I\otimes Z$

$\ket{00}\rightarrow \ket{00}$
$\ket{01}\rightarrow -\ket{01}$
$\ket{10}\rightarrow \ket{10}$
$\ket{11}\rightarrow -\ket{11}$

Can think of the I affecting first qubit, Z affecting second qubit.

#### Example
![[Pasted image 20210330095828.png]]
![[Pasted image 20210330104513.png]]
![[Pasted image 20210330104524.png]]


1 photon sent with a "right horiz" wiggle and a "horiz" direction

![[Pasted image 20210401103222.png]][[Quantum Mechanics TOC]]
# Quantum Hype
#🌱 

Where does the true potential of [[Quantum Computing TOC|Quantum Computing]] end and the hype begin?

- Superposition
- Billed as a single, easy solution to intractable problems
- Scaling difficulties ignored
- Difficulty of getting quantum information out of the system
- Errors
- Money poured in off weak promises
- People are poor at making timing expectations - [[People Overestimate What They Could Do In A Day, And Underestimate What They Could Do In A Year]][[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# Quantum Interference
#concepts 

Interference happens when [[Quantum Phases]] cancel each other out or reinforce each other to change the final outcome probabilities. This means that paths of an algorithm can self-reinforce or negate other paths! Very crazy stuff. See [[Deutsch's Algorithm]].

![[Pasted image 20210420104200.png|700]][[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Quantum Measurement
#concepts
**Some great notes on measurement in the more general multi qubit state in the apr15 class note!!!**

One of the 3 core elements of Quantum Computing alongside [[Quantum Gates]] and [[Quantum States]]. Measurement forces quantum information to become classical information. Namely, superpositions collapse into one of the orthonormal states used in the measurement with a certain probability.

In Dirac's notation, we say the probability of obtaining $\ket{0}$ is $|\braket{0|\phi}|^2$  and the probability of obtaining $\ket{1}$ is $|\braket{1|\phi}|^2$. This can be generalized. See [[Inner Products]] for the definition of $\braket{a|b}$ 

Started learning in [[CS333 Week 2]]

## Measurement Explanation
* When a particle is forced to “choose” between one of several states, and the state is changed by the measurement.
	* This is called Collapse
	* This can be likened to when the act of “measuring” some element of a particle’s state happens - say by another particle coming in, or an observer
* For example when you take a diagonally polarized photon and pass it through vertical filter, half of the photons come out as vertically polarized photons.

Given that qubits can take on any one of infinitely many states, one might hope that a single qubit could store lots of classical information. However, the properties of quantum measurement severely restrict the amount of information that can be extracted from a qubit. Information about a quantum bit can be obtained only by measurement, and any measurement results in one of only two states, the two basis states associated with the measuring device; thus, a single measurement yields at most a single classical bit of information.

We aren't sure about who is really an observer.

See The Church of the larger Hilbert space - collapse never happens, we only just witness one part of the supersposition

## Measurement Process
1. To measure a qubit state $\ket{\psi_B}$, we must first choose a [[Quantum States|basis]] to measure in, for example the generic bra:

	$$\bra{\psi_B}=\{a^*\bra{0},b^*\bra{1}\}$$

2. We then define the state of the qubit to be measured as $\ket{\psi_A}$ as the generic ket:
	
	$$\ket{\psi_A}=\{a\ket{0},b\ket{1}\}$$
	
3. We can now take the braket of the total to find our answer:

	$$\braket{\psi_B|\psi_A} = 	(a^*\bra{0}+b^*\bra{1})\ \ (a\ket{0}+b\ket{1})
	$$
	$$ = a^*a + b^*b = a^2+b^2=1$$

*Note that in all cases the sum of $a^2+b^2=1$, since the state can only go to one of the two orthonormal bases.*

### Measurement Example


## Partial Measurement
The [[Quantum Bomb Game]] forces a [[Partial Quantum Measurement]]
$\ket{\psi}_{AB}$
![[Pasted image 20210325094213.png]]


---
aliases: ["Quantum Mechanics"]
---
[[000 Life MOC]] | [[050 Concepts MOC]] | [[052 Physics TOC]] | [[053 Computer Science TOC]]
# Quantum Mechanics TOC
#🌲
#TOC
#concepts 


My central location for all things quantum mechanics. This is the theory that underlies [[Quantum Computing TOC]], the application of quantum concepts to solve problems classical systems cannot.

For more see class: [[CS333 Quantum Computing]]

Quantum mechanics is the combination of disciplines!! Notice how closely this area intersects with [[052 Physics TOC]], [[051 Math TOC]], [[053 Computer Science TOC]], and even stuff like [[061 Philosophy TOC]]!!

As far as we know, quantum mechanics really do describe reality! No way to confirm for now though. I bet we can begin to really answer the most interesting questions of life with powerful quantum systems.

[[Quantum Hype]]
[[Quantum Questions]]

## Theoretical Topics
[[Bell’s Theorem]]
[[Interpretations of Quantum Mechanics]]
[[Superposition]]
[[Entanglement]]
[[Quantum Measurement]] and [[Partial Quantum Measurement]]

## Qubits and States
See [[Qubits]] and [[Quantum States]]
[[Multi Qubit Systems]]
[[Quantum Phases]]
[[Quantum Gates]]
[[Quantum Error Correction]]
[[Projective Quantum Measurement]]

## Measuring Qubits
[[Wavefunction Collapse]]
[[Quantum Measurement]] | [[Partial Quantum Measurement]]
[[Quantum Bomb Game]]
[[Qubit Normalization]]

## Quantum Algorithms
[[Deutsch's Algorithm]]
[[Factoring Algorithm]]
[[Quantum Random Walks]]

## Quantum as Linear
See [[Linear Algebra TOC]] for more on the matrix math that goes into quantum
[[Quantum Cryptography]] is an example of applied linear algebra in quantum mechanics. All states, gates and measurements can be defined algebraically as matrices.

## History
[[EPR Paradox]]

## Polarization states
Qubits can have polarizations. We can write this in terms of vectors or kets, though kets are preferred. We typically only use states that are orthonormal bases for ease of computation.
*(ie they are orthogonal and normalized)*
![[Quantum States#Orthonormal Bases]]
[[Quantum Mechanics TOC]]
# Quantum Phases
#concepts 
[[Quantum Questions]]

Phases of states multiply part or the whole of the state by some value. This is what leads to [[Quantum Interference]]. 

## Relative Phases
![[Relative Phases]]
## Global Phases
![[Global Phases]][[051 STEM TOC]] [[052 Physics TOC]] [[053 Computer Science TOC]]
# Quantum Questions
#concepts
[[CS333 Quantum Computing]] [[Quantum Mechanics TOC]] [[Quantum Computing TOC]]
* *Relative Phases versus Global Phases* - the idea that more than one vector could represent a single actual state.
	* [[Relative Phases]] [[Global Phases]] [[Quantum Phases]]
* What can act as an observer? Spooky! Perhaps as observers we just pick a superposition, but there is no collapse?!?!
* How can [[Quantum Phases]] cause [[Quantum Interference]]??? Need to reunderstand this one
* What are Paulis?
* [[Exponential Growth]]/ e/ exponential function! Should really revisit
* 

[[052 Physics TOC]] | [[053 Computer Science TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC|Quantum Computing]]
# Quantum Random Walks
#concepts 

Quantum Random Walks (QRWs) take advantage of the concepts of [[Entanglement]] and [[Superposition]] to simulate [[Random Walks]]. The results are quite different from the classical case, with self-reinforcing action present in the distribution of expected positions.

The usual implementation involves several components. Two quantum registers, one for the coin outcome and one for the positions. A coin flip gate which is really a [[Hadamard Gates]] does the "coin flip", and a shift operator conditionally changes the qubit in the position register depending on the qubit in the coin register

## Algorithm Description
### Input
* Two quantum registers
	* Coin register
	* Position register
* Number of steps T

### Output
* State of the quantum walk after T steps

### Procedure
1. Create the initial state, which depends on the use case. For quantum search algorithms, this is a superposition state.
2. ```
```
for 0 <= k <= T:
	Apply the coin operator C the the coin register
	Apply the shift operator S. This shifts the walker based on the coin state.
Measure
```

## Implementation

![](https://raw.githubusercontent.com/qiskit-community/qiskit-community-tutorials/6de54e7033edc4233142caecda257ed72a6735f5/terra/images/quantum_walk/implement_toffoli.png)[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Quantum States
#concepts

We have some conventions for what bases we use for measurement. You don't have to use these, but keeping them consistent makes it easy. Technically even non-normalized states could be used, but then you can't interpret the amplitudes easily. 

These bases are typically used to make a full [[Quantum Measurement]] or a [[Partial Quantum Measurement]].


## Standard Bases
| Orthonormal Pair | Name              | Ket Abbreviation | Ket Notation                             |                                           Vector Notation |
|:----------------:| ----------------- | ---------------- | ---------------------------------------- | ---------------------------------------------------------:|
|        1         | Vertical          | $\ket{0}$        | $\ket{0}$                                |                    $\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ |
|        1         | Horizontal        | $\ket{1}$        | $\ket{1}$                                |                    $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$ |
|        2         | Right Diagonal    | $\ket{+}$        | $\frac{1}{\sqrt{2}}\{\ket{0}+\ket{1}\}$  |  $\frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ |
|        2         | Left Diagonal     | $\ket{-}$        | $\frac{1}{\sqrt{2}}\{\ket{0}-\ket{1}\}$  | $\frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix}$ |
|        3         | Clockwise         | $\ket{i}$        | $\frac{1}{\sqrt{2}}\{\ket{0}+i\ket{1}\}$ |  $\frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ i \end{pmatrix}$ |
|        3         | Counter Clockwise | $\ket{-i}$       | $\frac{1}{\sqrt{2}}\{\ket{0}-i\ket{1}\}$ | $\frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -i \end{pmatrix}$ |

*Note that all quantum states also have representations as wave functions*

*Note also that this generalizes well for non-infinite dimensional states. The real case is what is called the Hilbert space, roughly an infinite complex vector spaces. In information processing for [[Quantum Computing TOC|Quantum Computing]], these tend to be finite complex spaces.*

## Bloch Sphere Representations
Single qubit states as well as [[Quantum Gates]] and [[Quantum Measurement]] can be represented as points on a 3d sphere:
 
$$ 
|\psi(\theta,\phi)⟩=\begin{pmatrix}cos(\theta) \\ e^{i\phi}sin\theta\end{pmatrix}
$$

[[CS333 PS6]]

## Normalization
All states must be normalized. Can leave as is if just a single standard basis, as the second item below, but with superpositions must normalize amplitudes so they sum to 1.
![[Pasted image 20210427104423.png]]

## Multi-Qubit States
See [[Multi Qubit Systems]].
States made up of multiple [[Qubits]] can be represented in a number of ways.

Instead of:
$$
\{[\ket{0}...\ket{0}\ket{0}],[\ket{0}...\ket{0}\ket{1}],[\ket{0}...\ket{1}\ket{0}],...[\ket{1}...\ket{1}\ket{1}]\}
$$
We can simply write:
$$
\{\ket{0...00}, \ket{0...01}, \ket{0...10},... \ket{1...11}\}
$$

We can also represent these numbers as decimals for ease of use, taking into account the fact that each combined state is itself an overall state. For example with a 2-[[Qubits]] system:

$$
\{\ket{00},\ket{01},\ket{10},\ket{11}\}=\{\ket{0},\ket{1},\ket{2},\ket{3}\}
$$
For this type of notation to work, the number of qubits must be clear from context. 

EX:
$\ket{011}=\ket{3}=\begin{pmatrix}0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0\end{pmatrix}$

## Notes 
![[Pasted image 20210401101028.png]][[Standard Model TOC]] | [[052 Physics TOC]]
# Quarks
#concepts
#🌱[[052 Physics TOC]] | [[Quantum Mechanics TOC]] | [[Quantum Computing TOC]] | [[Quantum Measurement]] | [[Partial Quantum Measurement]] | [[Quantum States]]
# Qubit Normalization
#concepts/cs/quantum 

#🌱
[[Quantum Mechanics TOC]] | [[Quantum Computing TOC]]
# Qubits
#concepts 

The qubit, short for *quantum bit*, is the core information carrying unit in [[Quantum Computing TOC|Quantum Computing]]. Can bee seen as a [[Quantum Mechanics TOC]] generalization of a classical computer bit. It is a two-dimensional quantum system defined as:
$$
\ket{\phi}=\alpha\ket{0}+\beta\ket{1}
$$
Where $\alpha$ and $\beta$ are complex numbers such that: $|\alpha|^2+|\beta|^2=1$ *(normalized)* and $\ket{0}$ and $\ket{1}$ are orthonormal.

For more on this notation see [[Quantum States]]. Performing a [[Quantum Measurement]] changes the state. Measuring forces a quantum collapse to the classical state $\ket{0}$ with probability $|\alpha|^2$ or $\ket{1}$ with probability $|\alpha|^2$


[[CS333 Week 1]]


# Intro
* Qubits can take two states, 0 or 1, written: (known as states, vectors or kets)
*|0> and |1>*
* Can exist in either or both states
* Also has a *superposition state:* 
`|Superposition state> = alpha|0> + beta|1>` where alpha and beta are complex numbers. 
	* The modulus squared of alpha or beta gives us the probability of the qubit being in that state, so
`|alpha|^2 + |beta|^2 = 1`
This is a crucial property!!
	* We obtain these modulos by doing ex`(alpha) * (alpha*)` - alpha times the complex conjugate of alpha
	* 
 
* Qubits are polarized photons! [[Particle Physics/Photon]]

* Qubits have uncountably infinite many states!

## Types
There are different types of quits.

* *Diamond Nitrogen Vacancy* - has several spare electrons, use these to store info
	* Has magnetic spin info
* *Photon*


# 2 Qubit States
See [[Quantum States]]
![[Tensor Products#n -Qubit Systems]]
![[Multi Qubit Systems]][[063 Economics TOC]]
# Questions
[[200 Outputs MOC]]
One of the biggest challenges I see to funding sustainable development is mostly centered on developing countries. It feels difficult coming from a successful nation that rose to prominence in large part due to a huge highway system and reliance on oil, and telling developing nations that they must follow a different, far costlier path. How do you think we should go about reconciling these difficulties?


I’m really curious about the potential for significant sustainable investment from large companies interested in improving their image, as has been the case with big tech firms recently. Facebook and google for  Do you think these type
# Quiz 1 Revisions
[[Algorithms And Complexity]]
I have lots of revisions to do, and will be going over the entirety
# Quote from designated survivor 
I have no inclination to elaborate on the disposition of our collective indisposition

<!-- {BearID:BF552CEB-75A5-412A-801C-0BFC7FADA436-3022-000003EB5ECE6C94} -->
[[090 Lists MOC]]
# Quotes
“Never let a good crisis go to waste” - Winston Churchill

“The most influential organizations are the ones that are invisible” - unknown to me
![[Pasted image 20210504192052.png]]
[[Quote from designated survivor]]
[[Interesting quote from reddit]]

"Remove yourself from what is perceived as comfortable, because that's where the magic happens" - [[Robert Rubin]]# Reli Week 7
#school 

- With Sonya & Rati for the project 

Similar: I thought the Bigelow piece reminded me of the multireligious shrine/tombsite to Amma & Abba. Connects & binds ppl from other feelings & faiths. In the shrine, we'd see people of all faiths, genders and denominations join to perform a ritual to commemorate the dead. Kind of a vernacular way to show faith.

Different: Quite the contrast to the more modern day nationalist vision. Very non-binary approach back then, unlike now. Definitely a contrast to some of the more recent readings we've done, including on Hindutva.

Question: How do different communities evolve different standards for dealing with alterity of beliefs? It is interesting how different locations in the same nation could have such different standards for dealing with otherness. What could cause this difference? Language? Culture? Size of settlement - leading to more familiarity and community dependence?


Stories are often more about who we want to be than who we are.

How do rituals map to narratives??

## Essay Feedback Discussion
- Actual textual citations
- More clarity - started late
	- Start earlier
	- Make more of a plan
	- Have an argument
	- Tie examples into my argument more clearly[[RELI254 Islam in South Asia]] [[005 Active MOC]]
# RELI254 Essay 1
#school 
Due Friday 26 March!

[[RELI254 Islam in South Asia]]
[[files/RELI254 Week 4]]
[[RELI254 Week 4]]
[[RELI254 Week 3]]
[[RELI254 Week 2]]
[[RELI254 Week 1]]


*Prompt:* /How does studying Islam in South Asia help us to understand the concept of “religion”?/
* An opportunity to think through things
* Need 3 sources, preferably primary
* Make an argument about the relationship between the 3 pieces of evidence

*DUE: 11:59 Friday March 26*

### Sources
https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444351071.wbeghm285
In Amma's Healing Room
Vinay Lal - # [Veer Savarkar: Ideologue of Hindutva](https://southasia.ucla.edu/history-politics/hindu-rashtra/veer-savarkar-ideologue-hindutva/ "Permanent Link: Veer Savarkar: Ideologue of Hindutva")

### Ideas
Read through class notes!
Will need to do three readings (Loma)

What does it mean to be Muslim in SA?
How does vernacular Islam shape religion and culture in SA?
Are religion and culture distinct categories? Is religion even a helpful category when studying SA?
How have western perceptions of religion affected life in SA?

Western conceptions of religion are dramatically different to those held by people in South Asia, namely modern India, Pakistan and Bangladesh. The whole region has been shaped over recent centuries by oppressive British rule, with a number of western cultural phenomena becoming ingrained in the societal conception of religion. In the west, South Asia is often viewed as being more spiritual or more religious than the west. Much of this misunderstanding likely comes from the fact that religion is much more ingrained in culture, language and tradition in SA than western Protestant regions of the world. Professor Morgenstein Fuerst stated that "religion is not a native category" suggesting that even using the lens of religion to examine South Asian culture frames things in an inherently western way. Is there any value then to using the lens of religion specifically when looking at other cultures, or should we instead focus our energy on more general cultural anthropology combined with a study of politics and history? This paper will explore this question through several historical examples from the region. First, we will start by looking at the partition of India, and how British conceptions of religion set the stage for significant turmoil. We will then contrast this by looking at the example of Amma's healing room, where we are presented with a view of vernacular Islam that suggests distinct religious categories are not particularly helpful. Finally, we will turn to the example of modern India, and questions raised by the emergence of the BJP and their "India is for Hindus" policy.

The partition of India occurred in 1947 in the wake of WW2. The British realized soon after the conclusion of the war that they did not have the resources or national drive to maintain a colonizing presence around the world, and began the process of establishing two independent nations: India and Pakistan, along religious lines, with India being a "Hindu nation" while Pakistan would serve as a "Muslim nation". What ensued was one of the largest refugee crises in history - Zamindar estimates that up to a million people died in the violence following the new national boundaries and newfound association between state and religion. Ethnic cleansing happened on a large scale, and up to 20 million people were displaced in the process. Why was the partition so violent, leaving a wake of decades of further violence and instability? While there is no single answer to that question, it can be argued that the violence resulted from the religionization of national boundaries in a way that was not innate to the region - not a native category, as Fuerst would say. While quite a blunt example, this demonstrates why it can be dangerous to apply or judge the region from a western religious perspective, and we would argue that it is impossible for those of us from the west to disentangle our view of religion from that created over time in the Protestant west. 

In fact, we can take a more micro approach and come to the same conclusion about why looking at South asian culture from the perspective of western religions is an imperfect tool. An excellent example of the irreconcilability between vernacular Islam and the binary western view of religion can be found in the ethnographic work *In Amma's Healing Room*. In the book, we are presented with an extremely unconventional view of Islam, which Flueckiger refers to as "Vernacular Islam," referring to Islam as it is practiced in time and space rather than in its more abstract, often-studied form. Vernacular Islam as a concept can help us untangle the implications of isolating the idea of religion when studying South Asia. In particular, Amma's story makes it clear that the reality of religion is very different from the abstract conception of it, with lines being much harder to draw between different faiths and different aspects of life. Amma for example claims to be a Sufi Muslim, yet Flueckiger remarks that she exhibits traits of Sufi, Suni and even Hindu traditions. The rituals she performs and abides by are imbibed as needed, and tend to stem from function moreso than strict definition. As readers and scholars of religion, it is tempting for us to read such texts and jump to conclusions about the characters - pigeonholing them into a specific category. This does not accurately mirror the reality, however, and is in fact more a leftover from colonialist racialization and westernization of South Asian religion. To the British, there was no way one person could ascribe to more than one religion, so they enforced their view, starting with the national census.

The final example we can turn to to illustrate the dangers of westernizing religion can be found generally with the example of modern Indian politics. Modern Indian politics have been trending towards radical Hindu nationalism for decades now, and the 2014 election of PM Narendra Modi solidified religious conservatism as a successful mainstream political tactic in the country. This has resulted in a rash of anti-Muslim violence throughout the country, increasing tensions with Pakistan and a general renewed national focus on religion. Interestingly, up until near the end of the 19th century, religion was not thought of in such binary terms. We can turn to the classic work of Damodar Savarkar titled *Hindutva: Who is a Hindu?* which lays out some of the first formal delineations claiming India as a Hindu state. Savarkar describes Hinduness as a cultural, political and ethnic identity. It becomes more than a rough description or grouping of beliefs, and much moreso a question of race. This is supported closely by the work of Professor Morgenstein Fuerst, who argues that all religions are racialized, becoming more a question of power and wealth than about culture and rituals. This is a fascinating concept and helps us further unravel the nature of religious study.

In conclusion, there is a definite need to be wary of using the lens of religion to examine foreign cultural contexts. All three examples demonstrate at different levels the impacts of westernizing our view of religion. In our first example, we are presented with the approach to partitioning India taken by the British, and the brutal violence that ensued as a result. In our second example, we scale down and look at the very specific example of Amma from *In Amma's Healing Room*. In this book we are presented with a rather idiosyncratic view of religious practice in reality termed "vernacular Islam". Through this example, we see that it is neither necessary nor particularly helpful to try to make absolute delineations on religious terms. Finally, we examine the more recent example of modern-day Indian politics. This serves primarily as an example of what happens if the more western view of religion is deployed in a nearly militaristic fashion. All these examples prove slightly different points, however when brought together they start to illuminate the potential true nature of religion in South Asia. Clearly, there is some utility to using western labels and ideas, however we must be consciously diligent to avoid falling into the pitfalls this creates. Understanding the fact that religions in practice differ from textbooks is a helpful starting point.[[RELI254 Islam in South Asia]] [[RELI254 Formal Response 3]]
# Essay 2
#concepts

Last: [[RELI254 Essay 1]]
**Due: Friday, May7**

# What is the distinction between ethnicity and religion?
And is it helpful to think in these terms?

## P1 - Ring
- Pakistan
- **Ethnicity as a fluid construct with roots in religion, birthplace and more. Can we nail down a definition?**
- Negotiations of ethnicity are used to keep the peace.
	- Peace doesn't just break out - it is consciously maintained through the negotiation of tensions between identities. 
- Clearly there is more to it than just religion, as a high-level understanding of the partition of Pakistan might suggest - religion is constant!
- Most interestingly, a clear definition for ethnicity does not emerge from this study!
- **Intra-religious strife** stopped 
- Local vs regional tension
- Veiling & coming out of purdah
![[Pasted image 20210506142501.png]]
![[Pasted image 20210506163909.png]]

## P2 - Uddin
- Bangladesh
**- Ethnicity = language**
- Strong sense of identity built in BD out of distinctive use of "bangla"
- Partition from PK - same religion. Clearly this is not the differentiator!
- "19th century reformers and muslim elites attempted to create a unified subcontinental Muslim culture and community based around Urdu, but also inadvertently reinforced a uniquely Bengali vision of Muslim community." (Uddin pg#117)
- Ethnicity - how well can it be controlled? Something organic! 
	- Appears to be very different from the view presented by Ring.


## P3 - Amar, Akbar Anthony
- India
- **Ethnicity, language constant, religion varied**
- Caricatured example, but demonstrates a lot about identity. A "Ceteris Paribus" comparison - all else held equal. Language, ethnicity the same in this case. Or is it? 
- Religion a clear, major point of identity. 
- Clearly, no matter the overlap, there is still something completely unique to religion itself when it comes to identity. 
- Ethnicity = identity? Fluid? *Chosen?*



# Sources
Ring - CH 3, Intro, CH5, Conclusion. Ethnic strife, gender, understanding broader discourses through smaller ones
	- Tensions exist more broadly than just religion
	- Ethnicity informs inter-religious tension

Kirmani - Village vs City
Amar Akbar Anthony - religious contrast
Uddin - the humanity/ reality of religion, different from broad lens

[[RELI254 Islam in South Asia]]
# RELI254 Formal Response 1
- Uddin, CH4 - Bangladesh
- 19th century reformers and muslim elites attempted to create a unified subcontinental Muslim culture and community, but ended up reinforcing the distinct national differences in Bangladesh
- "Alienated the Bengali Muslims from their co-religionists" and "the alianation only became"
- "Not merely a Bengali community... but rather a Bengali-informed Muslim community"
	- V interesting interlinking 
- Bengali vs Urdu speaker divide emerged after unification
- Awami Muslim League founded by several key figures including Sheikh Mujibur Rahman
	- Sought to protect cultural distinctiveness
	- Emphasis on religion
	- Meanwhile Pakistan trended towards more national politics - focusing on the nation and language etc.
		- Wanted Urdu to be in east & west pakistan
	- Won 167 of 169 seats in 1970 gen election in East Pakistan (bd). West Pakistan had much weaker consensus.
- Declare independence 26 March 1971
	- New country founded on 4 principles:
		1. Democracy
		2. Socialism
		3. Secularism
		4. Nationalism

Wild combo lmfao

Uddin raises a very interesting discussion in chapter 4 on some of the clashes between ethnic and religious nationalism. I hadn't really considered the point in the past, but it was really interesting to hear about how then-East Pakistan was shaped in a very different way from East Pakistan during the 19th century. Uddin comments that "19th century reformers and muslim elites attempted to create a unified sucontinental Muslim culture and community based around Urdu, but also inadvertently reinforced a uniquely Bengali vision of Muslim community." (Uddin pg#117) Clearly, the differences were able to emerge organically despite higher-ups' attempts to shape culture otherwise. I think this is a very telling point about the nature of history being less individual-centric than I typically recognize, although that is a conversation for another time. I think it is particularly interesting how the two nations gradually diverged by focusing on different flavors of nationalism culturally: one more on religion and language, and the other more on ethnicity and nationality.

Overall, this reading is particularly relevant to much of our classwork throughout the semester because it demonstrates the "humanness" of religion and culture. We have a tendency to look at history from a leader-centric point of view (at least I know I do), but in many cases focusing only on vision and not on the actuality of life for people ends up causing alienation, discontent and in the case of Bangladesh national indepence. It makes me wonder more about history generally, and where we may be conflating serendipity or complex human activity with individual leaders.

[[RELI254 Islam in South Asia]]
# Formal Response 2
#school 
The reading for today's class by Ring puts forward an interesting idea for using the concept of the home (*ghar* or *zenana*) as a metaphor or lens through which to understand broader ethnic and political discourses. Ring states that "the everyday, intimate negotiation of *ethnicity* and *nationalism*... is deeply implicated in the broader conditions of possibility of ethnic violence in the city, as well as the possibility of peaceful coexistance," (Ring, pg3) suggesting that the *ghar* not only serves as a metaphor to the broader discourse, but in fact has a self-reinforcing link whereby strife in the *ghar* translates to an increased possibility of broader ethnic or political violence. This is a powerful idea because it gives us a concrete framework through which to analyze ethnic strife in the broader sphere. Ring goes on to acknowledge that "we actually know very little about the mechanics of coexistance," (ibid) suggesting that this private analogue has significant use and should be explored if we seek to understand conflict broadly.

This reading helps us understand why some of the more detailed ethnographic works we've read this semester are particularly relevant. Initial readings of a source such as *In Amma's Healing Room* might object to the content of the work by calling it meaningless as a result of its lack of breadth. While breadth is certainly important for us to paint a complete ethnographic picture, Ring's argument that conflict on the local level can be used to understand conflict broadly applies well here to demonstrate the use of the piece: conflict in the broader sphere is likely to be reflected in the domestic setting and vice versa, therefore studying the domestic setting (Amma's healing room, in this case) gives insights into the broader theme. Although I agree in large part with Ring, I would still push back a bit on drawing such a forceful comparison between the *ghar* and the broader city or nation. First and foremost, it is difficult to understand a whole society simply by looking at individual examples. It is impossible to separate the things that are unique to particular people from those intrinsic to a whole society, making the process failure-prone.[[RELI254 Islam in South Asia]]
# Formal Response 3
#school [[RELI254 Formal Response 2]]

The reading in Ring ch5 for today discusses the question of intimacy through the contrast or coherence of one's "insides" vs one's "outsides" (Ring pg 137), and how the process of reconciling these two potentially opposing ideals helps explain "women's commitment to the intense daily labor of individual and collective emotional regulation." (Ring pg138) A lot of the reading centers around the idea of the *purdah*, or *veil,* that separates female society and specifically the *zenana* from the rest of the *ghar*. In this chapter, Ring extends the ideas discussed in the first several chapters of the piece. Before, Ring emphasized the value of using the local context - or the *ghar*, to understand broader urban and national ethnic conflict. In this chapter, Ring changes the focus from the separation of home/ outside to the separation of the *zenana* from the *mardana* (men's space) via the *purdah*. More specifically, Ring uses discussions of the *purdah* in different context to highlight how veiling is used both to distinguish women from men, as well as to establesh intra-female relationship dynamics. Traditionally, Ring argues, the *coming out of purdah* was seen as breaking gender boundaries by mixing with non-kin men, however this view does not fully encapsulate the nuances of either being *in purdah* or coming out of it.

This reading relates interestingly with the other readings we've done for this semester. First and foremost, I see it as a warning sign against taking individual interpretations of causality too seriously, especially when drawing conclusions from a narrow ethnographic context. This speaks particularly to our reading of *In Amma's Healing Room,* since Flueckiger uses that ethnographic work to draw a number of conclusions about religion, gender and identity. Ring's work doesn't contradict this, but rather urges us to look beyond the simplest answer when understanding novel contexts. In the context of Ring's piece itself, this warning is evident in the discussion of "who is... worthy of veiling for" (Ring, pg142) as being more significant in certain veiling customs than the place or act itself. This plays out in stark contrast to the traditional view outlined above, and raises interesting questions about the validity of our ethnographic conclusions.[[005 Active MOC]] | [[006 School MOC]]
#school/class 
# RELI254 Islam in South Asia

[Canvas site](https://middlebury.instructure.com/courses/8193)
 
## Syllabus
[[Islam in South Asia Syllabus_2021.pdf]]

## Upcoming Deadlines

[[RELI254 Essay 2]]


## Essay Prompt!
* An opportunity to think through things
* Need 3 sources, preferably primary
* Make an argument about the relationship between the 3 pieces of evidence

## Coursework
* Need to write 4 (1-2 paragraph) responses to readings posted by 9am on the day that reading is discussed
* Need to write 4 (1-2 parag) responses to peers’ responses by 10am
* 2 Interpretation papers - 4-6 pages in response to a question posed by the prof
* Final presentation - group project to analyze 
* Final Op-ed/ reflection paper
[[RELI254 Formal Response 1]]
[[RELI254 Formal Response 2]]
[[RELI254 Formal Response 3]]
[[RELI254 Essay 1]] 

## Classes
* [[RELI254 Week 1]]
* [[RELI254 Week 2]]
* [[RELI254 Week 3]]
* [[files/RELI254 Week 4]]
* [[RELI254 Week 5]]
[[RELI254 Week 7]]

## Paper
How does Islam in India help us think about religion?
[[RELI254 Essay 1]]
[[RELI254 Essay 2]][[RELI254 Islam in South Asia]]
# RELI254 Op Ed and Reflection[[RELI254 Islam in South Asia]]
# RELI254 Week 1
#school
[[RELI254 Week 2]]

* This class will explore mostly islam in India
* Hinduism will also be discussed
* Majority religion of India is Hinduism, BJP is a Hindu nationalist party
* Exploring how people use language to refer to and understand religion
* It is a unique euro-americas idea to separate out religion from culture, history and politics
*
[[RELI254 Islam in South Asia]]
# RELI254 Week 2
#school 
[[Mughal Empire]][[Religions]][[Islam]]
[[RELI254 Week 3]]


* Earliest muslim communities in India in the 7th century - within lifetime of Mohammed 
	* People came along trade routes. Not military
* Arabic is a sacred language in Quran
	* Amma speaks Arabic
* North and South India very different
	* Religion
	* language
	* Food
	* Culture
* Mughals came around 14th century - 19th century - Conquest!!
	* By the end muslims rule everything
	* EVERYTHING
	* Not uniform practices!

*Day 2 Questions*
* How does Amma garner authority?
	* Abba needs to give permission for Amma to practice
		* Despite Amma being the breadwinner
		* Also some shared authority
	* Money? may change the dynamic
* What does it mean to be muslim in the book/ context?
	* Being part of the community
	* Neighborhood becomes domestic space - women only put on burkha when leaving neighborhood
	* People make up their own rules for religion as they go based on personal boundaries and their relationships with people around them
* Why didn’t flueckiger ask abba about the relatationship
* Why did Flueckiger spend SO DAMN LONG with the same ppl? Does she realize how unrepresentative they are?

What’s missing?
	* Not a mosque
	* Not an Imam
[[RELI254 Islam in South Asia]]
# RELI254 Week 3
#school
[[RELI254 Week 2]]
[[RELI254 Week 4]]

*Class 1*
Joyce Flueckiger drops in
Are there ways religious identity has developed in a way that is unique in south asia?

Question from JF:
	* Where do we start?


Updates since the book was published:
	* Great granddaughter going to be a doc! 
	* JF didn’t know she was going to find Amma
	* Politics totally changed, more radical, more violent
	* Mosques destroyed by Hindu nationals
	* Indian Islam is trending towards regular traditional islam
		* Wahabi influence is becoming apparent
			* Saying what veiling is allowed
			* Illegal to go to shrines in other places
			* Islam can be interpretive 
	* Akbar is the president of the mosque and Sheikh 



Questions for JF:
	*  Where does “the house” extend beyond, with respect to face covering
	* How do you decide between variety and depth
		* Strength AND weakness is depth
		* Her projects vary - first project very wide
		* Depends on how much fieldwork delivers to you
			* Sometimes randomness defines outcomes
				* You have to lean into it
		* Initially JF wanted to do patient-focused research, but quickly saw how much power Amma has
		* Why didn’t she go to other healers?
			* Amma was not into it lmao
	

Notes on book:
	* Abba is the center in some traditions, has lots of authority, especially outside the healing room. Very mutual relationship
	* All grown women now wear Niqab
	* Amma NEEDED male authority to do what she did - when Abba died, it changed everything and tons of disciples dipped
	* By definition, you create narratives that adhere to a certain worldview, and gradually come to represent reality in different ways
	* There’s no private spaces in India - can just knock, ask, interrupt NO SENSE OF INDIVIDUALITY
	* Spiritual forces Amma can heal often overlap with the physical
	* Amma would often say she and JF loved to learn
	* Amma could read and write Arabic script!!

Vernacular Islam: Religion as it is practiced in a time and place


Politics and religion - closely intertwined! People vote by religion
	* EG BJP in India
	* Muslim Personal law - these laws are part of its legal tradition!
		* Marriage
		* Divorce
		* Inheritance
		* Cherity

*Ismaeli* - Branches within Shia Islam
# RELI254 Week 4
#school
[[RELI254 Islam in South Asia]]

[[RELI254 Essay 1]]
[[RELI254 Week 3]]

# Tuesday Prof Ilyse Morgenstein Fuerst
* *Religion is not a native category* Comes from a European/ Christian history
	* A tool of domination!
* WESTERN VIEW!
* “Civilized” white people live under Protestantism
* Ilyse is obsessed with power
	* Went to India to study abroad, lived with Muslim family
* After Muslim rebellion, Jihad was a dangerous deal (untruly). Muslims became suspicious after the uprising. Cannot opt out! Converting don’t matter
* Religion was racialized and showed what people “inherited”. Little to do with belief
	* Deadnaming - even if people chose western/ christian names, still considered muslim
* *QUESTIONS:*
	1. What are we losing/ missing when we give the British too much power to define religion/ religious identity in South Asia?
		* Binary religious view - can only ascribe to one religion
		* Pigeonholing - regardless of what was put on census, only got tallied in one box
		* Racialization
		* How can one identify with more than one religion?
		* Ability to parse complex identity
	2. One of the premises of my work is that religions are racialized. What does that mean to you? What might that mean in the context of South Asia?
		* Racialization seems like all that remains. Skin color SUPER important
		* Is religion racialized or race religiofied?
		* Over last 300 years theories of race invented, deployed and institutionalized 
			* “India is for Hindus” - Racialization!


# Thursday
Emergence of the state of Pakistan - after WW2. British drained

* Contradictions within religion/ politics?
* Jinnah Tried to create homeland for muslims in Pakistan - not an Islamic state
[[RELI254 Formal Response 1]]
# RELI254 Week 5
* Partition of Pakistan -> Pk & Bd
* Nationalist ideology must agree on something - with Pk chose religion
*  19th century reformers and muslim elites attempted to create a unified subcontinental Muslim culture and community, but ended up reinforcing the distinct national differences in Bangladesh
- "Alienated the Bengali Muslims from their co-religionists" and "the alianation only became"
- "Not merely a Bengali community... but rather a Bengali-informed Muslim community"
	- V interesting interlinking 
- Bengali vs Urdu speaker divide emerged after unification
- Awami Muslim League founded by several key figures including Sheikh Mujibur Rahman
	- Sought to protect cultural distinctiveness
	- Emphasis on religion
	- Meanwhile Pakistan trended towards more national politics - focusing on the nation and language etc.
		- Wanted Urdu to be in east & west pakistan
	- Won 167 of 169 seats in 1970 gen election in East Pakistan (bd). West Pakistan had much weaker consensus.
- Declare independence 26 March 1971
	- New country founded on 4 principles:
		1. Democracy
		2. Socialism
		3. Secularism
		4. Nationalism


**Construction of nation happens by creating rituals, traditions and such! Uniting ppl thru common practice. Maybe also language.**

Where are the boundaries between ethnic and religious identity?

Nationalism vs fundamentalism - get out vs assimilation

There may be no right answer - shits be messy.

What kinds of people have time to focus on stupid issues?? Telling of the kind of person

[[Secularism]]

Relationship between class and religion/ caste in India/[[RELI254 Islam in South Asia]]
# Week 7
#school 

* Ring reading on peace in the intimate space = peace in the broader sphere. 
* [[RELI254 Formal Response 2]]

* Female power = female burden. Women are charged with keeping peace, but this is also a responsibility and flex of their power.
* Men's anger - women have to make the peace because men are accepted as being more angry.
* Holding tension - it is critical to have spaces where it is ok for tension to be held and let itself play out. Masculinity can be understood as the will to immediately resolve tension, whereas women could be understood as trying to hold the tension to build peace.
* Can women's exchange of flour help us understand where violence may come?

[[Statistics TOC]] | [[Machine Learning TOC|ML]]
# Receiver Operating Characteristic Curves
#concepts/cs/ml 

ROC is the curve of the false positive rate plotted against the true positive rate. The predictions are first sorted by [[Model Confidence]] with most confident first. A good model would have more true positives to start with, but some false positives must happen in order to cover all the data. All models start at $(0.0,0.0)$ and end at $(1.0,1.0)$. A perfect classifier is a square that goes up vertically to $(0.0,1.0)$ before going horizontally $(1.0,1.0)$. A random classifier will have a right horizontal straight line.

If we take the ROC curve and compute the area under the curve, we get [[AUC]], a single number that represents our model hitrate. 0.5 is random, 1 is perfect. 

![[Pasted image 20210331160158.png]]
[[052 Physics TOC]]
# Radiation
#concepts 

Radiation is the emission of energy as electromagnetic waves or as moving subatomic particles, especially high-energy particles which cause ionization. Radiation is emitted from the [[Nucleus]] of an [[Atom]] that is [[Radioactivity|Radioactive]], or was hit by another radioactive particle. All particles above 0K emit radiation! Generally hotter particles = shorter wavelengths emitted. This is the principle of black body radiation.

One of the common units of measurement for radiation is the [[rem]]

![[rem#Radiation Poisoning]]

## Black Bodies
A 'black body' is an idealized physical body that absorbs all incident electromagnetic radiation and emits radiation ([[Electromagnetism]]) depending on it's temperature, or [[Energy]] level. The wavelength of the energy emitted varies depending on the temparature of the body. This allows us to predict surface temperatures.

Higher temperatures correspond with more shaking of [[Electrons]], creating [[Electromagnetism]] in the form of [[Waves]]. More excited electrons have higher frequency and shorter wavelength.

## Temperature
Higher temperature actually means higher [[Energy]], which will increase radiation significantly.

Total amount of radiation output is proportional to the fourth power of temperature:
$$\Large
P=A\sigma T^4
$$
* P: power
* T: absolute Kelvin temperature
* $\sigma$: constant
* A: Surface area in square meters

## Types
#### Alpha rays
Particles consisting of 2 [[Protons]] and 2 [[Neutrons]] moving at high velocity. When they slow down enough, they attract an [[Electron]] and become helium atoms.

#### Beta rays
Energetic [[Electrons]], much lower mass than alpha rays, which aare actually particles. At high velocity, the energy is still comparable.

#### Gamma rays
Packets of energized [[Photons]]. Travel at speed of light. Carry a million times as much energy as visible light photons.

#### Neutrons
Literally ejected [[Neutrons]]. Important to the chain reaction. Tend to cause lots of illness with low damage to buildings, leading to ethical questions arround the Neutron Bomb.

#### X-rays
Also packets of energized [[Photons]] but with 10 - 100x less energy than gamma rays. X-rays pass freely through low atomic number elements but are blocked by high atomic number elements like lead and calcium. This is why they are used to detect tooth decay.

#### Cosmic rays
Released from stars, consist of protons, electrons, gamma rays, x-rays and muons.

#### Fission fragments
Released when the nucleas undergoes [[Nuclear Fission]]. Highly radioactive, heavy combos fo protons and neutrons. They are themselves radioactive, and are prone to redecay. They are the most dangerous component of radioactive waste.

#### Cathode rays
Emanate from metal with high voltage applied. Actually just electrons

#### Neutrinos
Mysterious. Pass through anything - including whole earth, with low prob of stopping. Emitted alongside beta rays (electrons). Very small mass so move pretty much at speed of light.



---
aliases: ["Radioactive"]
---

[[052 Physics TOC]]
# Radioactivity
#concepts 

Refers to explosions of the [[Nucleus]] of [[Atoms]]. Creates [[Radiation]], which is similar but not the same thing.

[[Carbon Dating]] relies on Radioactive decay properties of Carbon 14.# Random Effects Model
#concepts
[[Notes to make]] [[Statistics TOC]]
The random effects model is a method of [[Panel Data]] regression that attempts to capture some of the unobserved heterogeneity among observations. 

The RE model can be seen as an extension of the [[Fixed Effects Model]], with the added assumption that variation in the dependent variables is explained by observation heterogeneity that cannot be connected with individual variables, as we would in the FE model through CS or TS dummies.

*Common Assumption:* 
	* The individual unobserved heterogeneity is uncorrelated with the independent variables.

<!-- {BearID:CAD96201-85E6-41BA-A4BD-CC570FE4867F-16983-000109AB72062D51} -->
[[Machine Learning TOC]] | [[ML Models TOC]]
# Random Forest
#🌲 #concepts/cs/ml 

Random Forest models are actually weighted combinations of [[Decision Trees]]. Typically this results in better performance than individual D-trees because the weighted average prediction is less sensitive to the specific examples learned than an individual tree. This helps avoid overfitting.

RFs are examples of [[Bagging]][[011 Mental Models MOC]] | [[053 Computer Science TOC]]
# Random Walks
#concepts

Random walks are theoretical frameworks we can use to apply a general paradigm for sampling and exploring a graph like network. The applicability of the classical random walk to other algorithms and simulations is high. Monte Carlo simulations are actually random walks. 

## Properties of classical random walks
- If two-sided, follows a normal distribution
- If one-sided with a capture zone, never escapes the capture zone.
- The further you get from the start, the lower the probability. 
- Expected distance scales with log of iterations


## Quantum Random Walks
[[Quantum Random Walks]] apply the same paradigm to the world of [[Quantum Mechanics TOC]]. They take advantage of the interference and self reinforcement effects that emerge when applying certain gates on a superposition state.# Ranking
#concepts 
[[Machine Learning TOC]][[CS451 Machine Learning]]

* Trying to put a set of objects in order for relevance
# Raps
[[040 Interests MOC]]
Callin in through the phone yo 
Yo you think im the foe
But really im the enemy
Dont you fuck with my flow yo

Sweated in anonimity
Help stop these folks from gettin me
Im runnin but they onto me
Yo faster faster cant you see

<!-- {BearID:AE3366CF-F07E-450B-9F8A-7C616C907B7B-939-00000116B6E9D34A} -->
[[090 Lists MOC]]
# Reading List TOC
[[Company Research]]
* Diplomacy
* WSJ Real Estate guide
* Sleep
* The good stress
* Breaking the habit of being yourself
* 7 brief lessons on quantum mechanics
* The immortal life of Henrietta lax
* Ledoux - emotional brain
* 
http://index-of.es/
- reference on tons of CS related stuff. 
http://index-of.es/
- reference on tons of CS related stuff.

<!-- {BearID:88B5E34F-2502-4BF7-90F5-A6E8B2607627-16983-00013DDC0852DD3C} -->
# Recap of my Morgan Stanley experience
<!-- #work/ms -->
### Michael Calvey - Summer 2019

This summer I spent 6 weeks working at Morgan Stanley Wealth Management. The first 3 weeks were at a branch in New York with Ron Basu and Team Global, followed by another 3 weeks with the Digital team under Paul Vienick.

*Responsibilities with Team Global:*
* Took notes and asked questions in investor calls on behalf of the team with Elliott Capital, Greyhound Capital, Oak Hill Advisors, as well as a number of internal Morgan Stanley economic reviews and opportunity analyses for various strategies and key topics like long term growth, distressed credit, opportunity zone investment and the changing global technological landscape.
* Conducted risk reviews of several of the teams’ portfolio positions. Specifically, examined the potential upside of a demerger of Prudential Plc from its Asia unit, and attempted to estimate Johnson & Johnson’s potential liability in the ongoing opiate litigation cases.
* Created research reports on several potential investments, comparing each of the companies to peers using technical valuation metrics (P/E, Dividend yield, EPS growth CAGR, etc) and attempted to create an overall investment hypothesis for why current conditions might support a certain investment.  In particular, I looked at the current risks from trade / tariff uncertainty and sensitivity of various markets to political unpredictability.
	* Analyzed TERP, a distributed renewable energy yieldco with an attractive return profile and long-term average contract length. 
* Worked with GIMA team for several days, during which period I had significant exposure to a variety of different fund strategies and fund managers. Met with representatives from Merian Partners, Oak Hill Advisors, Investis, Anthemis Capital and several others, and got valuable experience in evaluating differing strategies, as well as comparing them to various benchmarks on size and consistency of past returns on a relative basis.
* Carried out a systematic review of opportunities in the global cannabis market at the request of a prominent MS client. The 25 page report examined the size of the existing market opportunity based on various independent estimates, as well as forecasts for future market growth ($60 billion annual revenue for US market by 2023). Also made a deeper analysis of roughly 20 significant players in the US and Canadian markets, concluding with a proposed portfolio of 8 stocks that provide exposure to various verticals and geographies, while spreading the risk inherent in an early stage industry. 

After this, I was posted for 3 more weeks with the Digital team at 1 New York Plaza, where I was tasked with preparing a report that analyzed the effectiveness of Digital tools in improving FA workflow. This was particularly interesting to me given my prior background in coding and software development, as well as some experience (from the previous weeks at MS) working with financial analysts who are the “end users” of the various digital platforms. 

*Responsibilities for Digital Team:*
I started by using my knowledge of working in the field to lay out a rough map of the main communication channels in terms of impact, before speaking to roughly 15 other FAs, CSAs, training specialists and branch managers.  This helped me to understand the areas that had potential to be effective methods of knowledge transfer, but were being hampered for various reasons. I helped connect some people from the digital team with the training team and some FAs, to better understand how the platforms are actually being used and areas for improvement. Finally, my report recommended 4 actionable points that I presented to members of the digital team: 1) Centralizing and improving the browsability of online resources, 2) accentuating distinct online resources for giving feedback in a discursive and open way, 3) improving the connection between the training teams and branch management,  and 4) having members of the digital team occasionally shadow FAs and CSAs to improve Digital’s understanding of their target customer.
In all, my time at Wealth Management was instructive and interesting, giving me the opportunity to see two sides of an extremely successful business. Combining the two shorter experiences proved valuable, as I had enough time to get into details and deliver concrete deliverables, while having enough breadth to properly understand how the supporting elements of the business fit together. This has inspired me to try to work again within either Wealth Management or a different unit of Morgan Stanley next summer to continue to expand the breadth of my knowledge. I would particularly enjoy the opportunity to work with either Shareworks or equity research, as the skills I developed this past summer would hopefully enable me to immediately work on meaningful projects.

<!-- {BearID:73B03BF0-D108-47E7-A251-D47EA5A0C04A-683-00000052AA72C9E8} -->
[[Heuristics and Biases]] | [[Cognitive Biases TOC]]
# Recency Bias
#concepts #concepts/mental_models 

Recency bias inclines people to put more emphasis on recent memory when making decisions or analyzing outcomes of past events. This causes a disproportionate emphasis on the last few trials.

This is probably a key component of [[Learning]] broadly, however can also be a dangerous pitfall when thinking more broadly.[[053 Computer Science TOC]]
# Recursive Sets
#concepts/cs

An element of the [[Chomsky Hierarchy TOC]]

We define recursive sets to be sets accepted by some total TM M. See [[Turing Machines TOC]] for more on total TMs, but essentially these are TMs that always halt, meaning they always enter either a reject or accept state. 

Sets that can be accepted but not rejected are called [[Recursively Enumerable Sets]]

In other words, this means that we can always determine whether a given input is a member of a recursive set, or not. We call this property [[Decidability TOC]], and say recursive sets are [Decidability - Decidable properties](bear://x-callback-url/open-note?id=A95B9AEC-4AE3-49DE-934D-A26FABB92339-5857-00013869E55DA986&header=Decidable%20properties) since we can say either yes or no for a given input.

* Recursive sets are closed under complement!
* Sets which, alongside their complements, are RE, are recursive.

<!-- {BearID:011CA630-8A12-40EB-97E4-4E55FFCC1EB8-5857-00012DE585A6D2D2} -->
[[053 Computer Science TOC]]
# Recursively Enumerable Sets
#concepts/cs

These are another element of the [[Chomsky Hierarchy TOC]]. We can define Recursively Enumerable (RE) sets to be the sets that are accepted by some TM M. See [[Turing Machines TOC]]. RE sets are ones that are always accepted by a TM, but not always rejected. This makes these sets [Decidability - Semi-decidable properties]

[[Quantum Computing TOC|Quantum Computing]] is hypothesized to be able to solve this class of problems, mostly using two entangled provers^[https://www.quantamagazine.org/landmark-computer-science-proof-cascades-through-physics-and-math-20200304/]

* RE sets are *not* closed under complement - instead, their complement is [[Co-RE Sets]] - properties of Co-RE sets are called Co-semidecidable properties.
* RE sets *are* closed under intersection
*  If set A and it’s complement A_not are both RE, set A is recursive.
	* Can easily show this logically: take RE sets A and B, where B accepts the inputs that would be rejected by A, if it were recursive. We can then create a separate total TM that simulates both sets at once on a given input, and is able to say either yes or no to an input based on which of the two simulated TMs accepts a string. Thus we can say yes or no, so the combined set is a [[Recursive Sets]].

## Examples
* Membership Problem: `L(U) = {M#x | M accepts x}`
* Halting Problem: `L(U) = {M#x | M halts on x}` See [[Halting Problem]]
* Complement of Null: `{M | L(M) =/= empty set}`

[[053 Computer Science TOC]] | [[051 Math TOC]]
# Reductions
#concepts/cs

Reductions are used to show equivalence between properties, referring to [[Decidability TOC]]. 

We write that `A reduces to B as: A <= B`

## Proving Undecidability
We can create a reduction from [[Halting Problem]] (HP) to the desired property. If we can build a valid reduction, the desired property is shown to be undecidable as well. This works for some properties such as:
	* Accepts the null string E
	* Accepts any string at all
	* Accepts every string
	* Accepts an infinite set
However for certain others, we must reduce instead to HP_not, the complement of [[Halting Problem]]

All of these topics are covered in detail in https://middlebury.instructure.com/courses/6762/pages/notes-and-reading-19-some-decidable-and-undecidable-problems?module_item_id=382574

### Reducing to HP
	* Given M#x, we construct a new [[Turing Machines]]M’ via some transition function sigma.

	* For example let us examine the property: `Whether a TM accepts the null string E`
		* We set up our assumptions so that if we can accept this property as decidable, we must also accept HP to be decidable. Since this is a contradiction, we know the property is also not decidable.
		* Given M and x, build a new M’ that on its input y will:
			* Run M on x
			* If M on x halts, accept y
				* `If M halts on x -> L(M') = Sigma*`
				* If  this were true, M’ would:
					* Accept the null string E
					* Accept any string at all
					* Accept every string
					* Accept an infinite set
			* If M on x loops, M’ will loop
				* `If M loops on x -> L(M') = {}`
				* On the other hand if this is true M’ would:
					* Not accept the null string E
					* Not accept any string at all
					* Not accept every string
					* Not accept the infinite set
			* If we could accept any of these conditions for M’, we would also immediately know whether M halts. As we know this is undecidable via the proof in [[Halting Problem]], we know the other conditions cannot be valid. Thus we do not know which outcome happens, and cannot know any conditions/ properties.

### Reducing to HP_not
We would reduce to HP_not to prove properties such as the following are undecidable:
Given some TM M, do we know if L(M):
	* Represents a regular set
	* Represents a CFL
	* Represents a recursive set

	* For this, we start with looping, which we know is undecidable
		* We set up a condition whereby if a property P is decidable, looping is also decidable. As this is not true, we know P is also undecidable.
		* Given M and x, build a new M’ that on its input y:
			* Runs M on x
			* If M on x halts, run K on y where K is a TM accepting HP
				* Accept y if K accepts y, `L(M') = HP`
				* M’ would then accept:
					* non-Regular set
					* non-CFL
					* non-Recursive set
			* If M on x loops, `L(M') = {}`
				* M’ would then accept:
					* Regular set
					* CFL
					* Recursive set
			* Thus if the decision procedure M’ really worked, we could figure out whether M halts on x, something we know to be impossible.

## Formal Proof
We use this proof to show a sets are [[Recursively Enumerable Sets]] or [[Recursive Sets]]

* Given `A in Sigma* and B in Delta*`
* A reduction is a mapping of `items in A to items in B`, as well as `items in Sigma*-A to Delta*-B`
	* via some *computable* function `sigma: Sigma* -> Delta*`
		* Computable means that a total TM can compute sigma(x) given x
* In other words: 
```
x in A <=> sigma(x) in B
x not in A <=> sigma(x) not in B
```


## Theorem
1. If `A <= B and B is RE --> A is RE`
2. If `A <= B and B is recursive --> A is recursive`

And their contrapositive corollaries:
1. If `A <= B and A not RE --> B not RE`
2. If `A <= B and A not recursive --> then B not recursive`


### Proving 1
	* We have that A reduces to B via a computable function sigma, and that B is RE:
		* Given M such that B = L(M)
		* Design N such that A = L(N)
		* On input x, N will:
			* Run M on sigma(x)
			* Accept x if M accepts sigma(x)
		* Thus:
```
N accepts x <=> M accepts sigma(x)
			  <=> sigma(x) in B
			  <=> x in A
So A = L(N), and A is RE
```


### Proving 2
```
B recursive means B and B_not are RE
A <= B <=> A_not <= B_not, so by part 1
A and A_not are RE, so A recursive
```

<!-- {BearID:2AD2C2F0-3666-4F2F-BBCE-60DFD632F0B7-5857-000140E518B1A60B} -->
[[052 Physics TOC]] | [[Waves]] | [[Light]] 
# Refraction
#concepts/physics

Refraction is the effect that occurs when a wave enters a material with a different refractive index, meaning the speed of the wave is different, changing the angle of the wave unless it entered perpendicularly to the surface. This is what happens in diamonds, or when white light enters a glass prism.

See also [[Diffraction]]
# Regression HW10
[[Linear Regression]]

```
-------------------------------------------------------------------------------
      name:  <unnamed>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw10.log
  log type:  text
 opened on:  30 Apr 2019, 13:51:49
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\Baltagi_EER1983.dta"
. xtset ctry_code year
       panel variable:  ctry_code (strongly balanced)
        time variable:  year, 1960 to 1978
                delta:  1 unit
. summ lny lnx2 lnx3 lnx4 year ctry_code
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
         lny |        342    4.296242    .5489071   3.380209   6.156644
        lnx2 |        342   -6.139425    .6345925  -8.072523  -5.221232
        lnx3 |        342   -.5231032    .6782225  -2.896497   1.125311
        lnx4 |        342   -9.041805    1.218896  -13.47518  -7.536176
        year |        342        1969    5.485251       1960       1978
-------------+---------------------------------------------------------
   ctry_code |        342         9.5    5.195729          1         18
. corr year lny lnx2 lnx3 lnx4 ctry_code
(obs=342)
             |     year      lny     lnx2     lnx3     lnx4 ctry_c~e
-------------+------------------------------------------------------
        year |   1.0000
         lny |  -0.2450   1.0000
        lnx2 |   0.3285  -0.3457   1.0000
        lnx3 |  -0.0886  -0.0313   0.3954   1.0000
        lnx4 |   0.3934  -0.6938   0.5539  -0.3916   1.0000
   ctry_code |   0.0000   0.2512  -0.2188  -0.2121  -0.1555   1.0000
```
First, we briefly examine the sample characteristics to understand our data. We will be running a regression in Stata to identify the variables that affect gasoline demand in a country. We have data by years for 18 different countries, for the years 1960-1978 (19 years), for a total of 342 observations. All of our data is already in log format, meaning we will only be working with LogLog models, and hence our coefficients will be constant elasticities of demand.

We have data on the gasoline consumption per car, per year, per country, with data on the average price of gasoline, average number of cars per capita and real income per capita for each observation. 

```
. * Model 1 LogLog Model
. reg lny lnx2 lnx3 lnx4
      Source |       SS           df       MS      Number of obs   =       342
-------------+----------------------------------   F(3, 338)       =    664.00
       Model |  87.8386024         3  29.2795341   Prob > F        =    0.0000
    Residual |  14.9043581       338  .044095734   R-squared       =    0.8549
-------------+----------------------------------   Adj R-squared   =    0.8536
       Total |  102.742961       341  .301299005   Root MSE        =    .20999
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .8899616   .0358058    24.86   0.000     .8195313    .9603919
        lnx3 |  -.8917979   .0303147   -29.42   0.000    -.9514272   -.8321685
        lnx4 |  -.7633727   .0186083   -41.02   0.000    -.7999754   -.7267701
       _cons |   2.391326   .1169343    20.45   0.000     2.161315    2.621336
------------------------------------------------------------------------------
```
a) Here we run our first model in Stata, 
Lny = B1 + B2lnX2 + B3lnX3 + B4lnX4 + u
Without explicitly telling stata to use our Panel data, this regression does a pooled regression, dropping the specific effects of time and country into u, our error unit. This may make the results quite unpredictable.

```
. * Model 2- Fixed Effects
. tab ctry_code, gen(country_)
  Ctry_Code |      Freq.     Percent        Cum.
------------+-----------------------------------
          1 |         19        5.56        5.56
          2 |         19        5.56       11.11
          3 |         19        5.56       16.67
          4 |         19        5.56       22.22
          5 |         19        5.56       27.78
          6 |         19        5.56       33.33
          7 |         19        5.56       38.89
          8 |         19        5.56       44.44
          9 |         19        5.56       50.00
         10 |         19        5.56       55.56
         11 |         19        5.56       61.11
         12 |         19        5.56       66.67
         13 |         19        5.56       72.22
         14 |         19        5.56       77.78
         15 |         19        5.56       83.33
         16 |         19        5.56       88.89
         17 |         19        5.56       94.44
         18 |         19        5.56      100.00
------------+-----------------------------------
      Total |        342      100.00
```
To run our fixed effects model by hand, we must first generate dummies for each country. 

```
. reg lny lnx2 lnx3 lnx4 country_2-country_18
      Source |       SS           df       MS      Number of obs   =       342
-------------+----------------------------------   F(20, 321)      =    586.56
       Model |   100.00647        20  5.00032352   Prob > F        =    0.0000
    Residual |  2.73649024       321  .008524892   R-squared       =    0.9734
-------------+----------------------------------   Adj R-squared   =    0.9717
       Total |  102.742961       341  .301299005   Root MSE        =    .09233
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .6622498    .073386     9.02   0.000     .5178715    .8066282
        lnx3 |  -.3217025   .0440992    -7.29   0.000    -.4084626   -.2349424
        lnx4 |  -.6404829   .0296788   -21.58   0.000    -.6988726   -.5820933
   country_2 |  -.1203045   .0341494    -3.52   0.000    -.1874895   -.0531196
   country_3 |   .7559844   .0407455    18.55   0.000     .6758224    .8361464
   country_4 |   .1036004   .0366047     2.83   0.005      .031585    .1756157
   country_5 |  -.0810844   .0335634    -2.42   0.016    -.1471165   -.0150524
   country_6 |  -.1359874   .0318796    -4.27   0.000    -.1987067   -.0732681
   country_7 |   .0512539   .0415296     1.23   0.218    -.0304507    .1329585
   country_8 |   .3064697   .0352937     8.68   0.000     .2370335    .3759059
   country_9 |  -.0533077   .0371126    -1.44   0.152    -.1263224    .0197069
  country_10 |   .0900717   .0386066     2.33   0.020     .0141179    .1660256
  country_11 |  -.0510644   .0335761    -1.52   0.129    -.1171214    .0149925
  country_12 |  -.0691552   .0404078    -1.71   0.088    -.1486528    .0103423
  country_13 |  -.6040788   .0912201    -6.62   0.000    -.7835437    -.424614
  country_14 |    .740487   .1800842     4.11   0.000     .3861927    1.094781
  country_15 |    .116647   .0347125     3.36   0.001     .0483543    .1849396
  country_16 |   .2241324   .0476443     4.70   0.000     .1303978    .3178669
  country_17 |   .0595918   .0301882     1.97   0.049     .0002002    .1189834
  country_18 |    .769395   .0445764    17.26   0.000     .6816962    .8570938
       _cons |   2.285856   .2283235    10.01   0.000     1.836657    2.735056
------------------------------------------------------------------------------
. xtreg lny lnx2 lnx3 lnx4, fe
Fixed-effects (within) regression               Number of obs     =        342
Group variable: ctry_code                       Number of groups  =         18
R-sq:                                           Obs per group:
     within  = 0.8396                                         min =         19
     between = 0.5755                                         avg =       19.0
     overall = 0.6150                                         max =         19
                                                F(3,321)          =     560.09
corr(u_i, Xb)  = -0.2468                        Prob > F          =     0.0000
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .6622498    .073386     9.02   0.000     .5178715    .8066282
        lnx3 |  -.3217025   .0440992    -7.29   0.000    -.4084626   -.2349424
        lnx4 |  -.6404829   .0296788   -21.58   0.000    -.6988726   -.5820933
       _cons |    2.40267   .2253094    10.66   0.000     1.959401     2.84594
-------------+----------------------------------------------------------------
     sigma_u |  .34841289
     sigma_e |  .09233034
         rho |  .93438173   (fraction of variance due to u_i)
------------------------------------------------------------------------------
F test that all u_i=0: F(17, 321) = 83.96                    Prob > F = 0.0000
. est store fe
```
b) The results of this LogLog regression are above. Again, our coefficients are constant elasticities, however now, with both methods we account for the fixed effects of different countries. In the manual method above, the coefficients on each country tell us what the average impact of that country is, when we add that value to the constant 2.28. 
The alternative ‘xtreg … fe’ command does the same thing, however the constant it displays is the average of the 18 constants computed by our by-hand method. The main difference in the specification beyond the dummies is that our error term here will be Vit, which is composed of a time effect, a cross sectional effect, and a classical error term.

```
. * Model 3- Random Effects
. xtreg lny lnx2 lnx3 lnx4, re    
Random-effects GLS regression                   Number of obs     =        342
Group variable: ctry_code                       Number of groups  =         18
R-sq:                                           Obs per group:
     within  = 0.8363                                         min =         19
     between = 0.7099                                         avg =       19.0
     overall = 0.7309                                         max =         19
                                                Wald chi2(3)      =    1642.20
corr(u_i, X)   = 0 (assumed)                    Prob > chi2       =     0.0000
------------------------------------------------------------------------------
         lny |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lnx2 |   .5549858   .0591282     9.39   0.000     .4390967    .6708749
        lnx3 |  -.4203893   .0399781   -10.52   0.000     -.498745   -.3420336
        lnx4 |  -.6068402    .025515   -23.78   0.000    -.6568487   -.5568316
       _cons |   1.996699    .184326    10.83   0.000     1.635427    2.357971
-------------+----------------------------------------------------------------
     sigma_u |  .19554468
     sigma_e |  .09233034
         rho |  .81769856   (fraction of variance due to u_i)
------------------------------------------------------------------------------
```
c) The random effects regression above assumes that the effects of each region are distributed normally around a population mean, meaning we have many more degrees of freedom, since we have to use less variables in the regression. If our assumption that there are not discrete effects from countries is false, however, our coefficients will be biased. As such, we must must do further testing to determine which model most closely fits our data.

d) All of our models regress the natural log of gasoline demand per car (lny) against the log of real income per capita (lnx2), the log of real gasoline price (lnx3) and the log of the number of cars per capita (lnx4), for the 18 OECD countries over the course of 19 years. As such, our resulting coefficients can be interpreted as constant demand elasticities, meaning they can be interpreted as the percentage change in Y from a 1% change in each variable X. 

The coefficient of Lnx2 represents the income elasticity of demand. As gasoline is a normal good, we would expect the coefficient for this variable to be positive, since people will consume more with a higher income.

The coefficient of Lnx3 represents the price elasticity of demand. The law of demand for normal goods states that the demand curve is negative with respect to price, as less of a good will be demanded at a higher price, all else equal. As such, we can expect a negative coefficient.

The coefficient of Lnx3 represents the elasticity of demand with respect to car ownership. We would expect this elasticity to be negative, since more cars per person means fewer miles travelled on each car, assuming travel requirements are constant. 

Our first model closely fits our expectations. Our signs all match our theoretical expectations, with high R^2 and Rbar^2 values of .8549 and .8536 respectively, as well as high overall significance as seen from the high overall F statistic. All the individual tests of significance also suggest a high degree of significance at the a=5% level. However, this regression does not model for the changes by country or over time, so there may be an omitted variable bias in our coefficient estimates that is not evident from the individual significance results. 

Our second model also fits our expectations, but we have a significant improvement in significance. Firstly, our overall significance improves, since R^2 and Rbar^2 increase to 0.9734 and 0.9717 respectively, a very significant increase. Clearly, our added degrees of freedom do not significantly limit our Rbar^2 value. Individual significances are difficult to compare with the past model, but we can see that most of the added country dummies are statistically significant at a=5%. 

e) First we shall test the validity of our fixed effects regression using a Chow/Wald special F-test.
Using our estimation from above, let us call the coefficients of dummies alphai-alphan

H0: for all alphai , alphai = 0
HA: H0 is not true
	F17321= [(14.9043581 - 2.73649024)/17]/[2.73649024/321]
	F = 83.699

We see that our F observed exceeds our F critical, so we can reject our null hypothesis that the added alpha coefficients are not significant, meaning our fixed effects model is statistically better than regular OLS in this case.

Furthermore, we notice a much lower root MSE and much higher R^2 and Rbar^2 on the fixed effects regression than with OLS. The fixed effects regression is able to explain 97.34% of the variation in our dependent variable. The fixed effects model correctly assumes that different countries have different intercepts, with Austria serving as the baseline. 

```
. * Testing
. xttest0
Breusch and Pagan Lagrangian multiplier test for random effects
        lny[ctry_code,t] = Xb + u[ctry_code] + e[ctry_code,t]
        Estimated results:
                         |       Var     sd = sqrt(Var)
                ---------+-----------------------------
                     lny |    .301299       .5489071
                       e |   .0085249       .0923303
                       u |   .0382377       .1955447
        Test:   Var(u) = 0
                             chibar2(01) =  1465.55
                          Prob > chibar2 =   0.0000
. hausman fe
                 ---- Coefficients ----
             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
             |       fe           .          Difference          S.E.
-------------+----------------------------------------------------------------
        lnx2 |    .6622498     .5549858         .107264        .0434669
        lnx3 |   -.3217025    -.4203893        .0986868        .0186143
        lnx4 |   -.6404829    -.6068402       -.0336428        .0151597
------------------------------------------------------------------------------
                           b = consistent under Ho and Ha; obtained from xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from xtreg
    Test:  Ho:  difference in coefficients not systematic
                  chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =      302.80
                Prob>chi2 =      0.0000
                (V_b-V_B is not positive definite)
. hausman fe, sigmamore
                 ---- Coefficients ----
             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
             |       fe           .          Difference          S.E.
-------------+----------------------------------------------------------------
        lnx2 |    .6622498     .5549858         .107264        .0475786
        lnx3 |   -.3217025    -.4203893        .0986868        .0219471
        lnx4 |   -.6404829    -.6068402       -.0336428          .01706
------------------------------------------------------------------------------
                           b = consistent under Ho and Ha; obtained from xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from xtreg
    Test:  Ho:  difference in coefficients not systematic
                  chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =       24.77
                Prob>chi2 =      0.0000
. hausman fe, sigmaless
                 ---- Coefficients ----
             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))
             |       fe           .          Difference          S.E.
-------------+----------------------------------------------------------------
        lnx2 |    .6622498     .5549858         .107264        .0460065
        lnx3 |   -.3217025    -.4203893        .0986868        .0212219
        lnx4 |   -.6404829    -.6068402       -.0336428        .0164963
------------------------------------------------------------------------------
                           b = consistent under Ho and Ha; obtained from xtreg
            B = inconsistent under Ha, efficient under Ho; obtained from xtreg
    Test:  Ho:  difference in coefficients not systematic
                  chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)
                          =       26.50
                Prob>chi2 =      0.0000
```
Next, we examine the effectiveness of our RE model. We will first use a BPGChi^2 test to test for significance.
For this test, we will use:

H0: There are no random effects
HA: There are random effects

With these hypotheses,  from our xttest0 we get:
Chibar^2(01) =  1642.20
Therefore as our observed > critical, we can reject our null hypothesis that there are no random effects, accepting our RE model in favor of Pooled OLS. 

At this point, we have seen that FE is better than OLS, and that RE is better than OLS. To determine which is better between these two models, we will use a hausman test. This test uses hypotheses:
H0: var(intercepts) = 0 
HA: H0 not true
If H0 is true, we know that our fixed effects model is not significantly better than our random effects model for estimating the effect of different countries, so it is better to use RE. 
Our resulting Chi^2 is 302.8, well above our Chi critical value, letting us reject the null in favor of the alternative. This means that our Fixed Effects model is the most accurate at estimating the demand function for gasoline.

<!-- {BearID:02D18A7A-ABB5-48C8-B841-2CA5F673EF7B-21279-000115D354DBB5C1} -->
# Regression HW5
#school 
## a)
First, we take a look at the variables and data we are working with. Our dependent variable is per capita consumption of chicken. Looking at our sample, we see a mean of just under 36.46, which we can interpret as each person in the population consuming 36.46 pounds of chicken per year. Our range is not as large as might be expected at 21.4 to 55.6. The standard deviation of 10.34 supports the other answers, since we see that most of the values are indeed encapsulated by 2 standard deviations, and 3 covers all values, as suggested by our knowledge of the normal distribution. 
Furthermore, our independent variables are the price of chicken, disposable income, log of disposable income, the price of beef and the prime rate. We will be running 3 regressions using combinations of these variables as follows:
Model 1: yt = B0 + B1pct + B2ydt + B3pbt + error1t
Model 2: yt = L0 + L1pct + L2lnydt + L3pbt + error2t
Model 3: yt = Th0 + Th1pct + Th2lnydt + Th3pbt + Th4prt + error3t

Using our understanding of economics for intuition, we can hypothesize that price of chicken will have a negative coefficient, disposable income and log thereof positive, price of beef positive and prime rate might be positive, although the prime rate is more spuriously connected to chicken consumption.

Running a correlation tabulations, we can determine how closely correlated the independent variables are to the dependent ones, and whether there is a high level of correlation between independent variables. All of the dependent-independent correlations support our suppositions above. Among the independent variables there is only one that stands out; the disposable income is highly correlated with price of beef and prime rate, as is to be expected.

See attached stata result for full regression results.


## a and b)
```
. * Model 1
. reg y pc yd pb
      Source |       SS           df       MS      Number of obs   =        34
-------------+----------------------------------   F(3, 30)        =    288.96
       Model |  3413.29738         3  1137.76579   Prob > F        =    0.0000
    Residual |   118.12477        30  3.93749233   R-squared       =    0.9666
-------------+----------------------------------   Adj R-squared   =    0.9632
       Total |  3531.42215        33  107.012792   Root MSE        =    1.9843
------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          pc |  -.7919335   .0839231    -9.44   0.000    -.9633274   -.6205396
          yd |   .0020373   .0003435     5.93   0.000     .0013356    .0027389
          pb |   .2209139   .0642384     3.44   0.002     .0897216    .3521061
       _cons |   31.14503   1.343291    23.19   0.000     28.40166     33.8884
------------------------------------------------------------------------------
. ovtest
Ramsey RESET test using powers of the fitted values of y
       Ho:  model has no omitted variables
                  F(3, 27) =     19.81
                  Prob > F =      0.0000
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |         34 -127.1765  -69.41539       4    146.8308   152.9362
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
This is our first regression model described above. We can see that the price has a fairly large negative coefficient, disposable income is very small but positive, and the price of beef is positively related to chicken consumption. All of these results support our initial suppositions about the signs of coefficients. Furthermore, we have great R^2 and Rbar^2 values, suggesting that our model explains a high degree of the variation in chicken consumption. Not only this, but our root MSE is fairly small compared to our earlier stated Ybar values. Finally, our overall test of significance F statistic is 288.96, which results in a 100% confidence that the model has at least one significant variable. Examining our t-values, we can see that every coefficient is significant to 99.8% at least.

```
. * Model 2
. summ yd
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
          yd |         34    4103.088     2896.58       1465      11257
. reg y pc lnyd pb
      Source |       SS           df       MS      Number of obs   =        34
-------------+----------------------------------   F(3, 30)        =    613.41
       Model |  3474.77523         3  1158.25841   Prob > F        =    0.0000
    Residual |  56.6469224        30  1.88823075   R-squared       =    0.9840
-------------+----------------------------------   Adj R-squared   =    0.9824
       Total |  3531.42215        33  107.012792   Root MSE        =    1.3741
------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          pc |  -.4508497   .0724407    -6.22   0.000    -.5987933   -.3029062
        lnyd |   12.24209   1.189684    10.29   0.000     9.812436    14.67175
          pb |   .1165359   .0474516     2.46   0.020     .0196268     .213445
       _cons |  -60.75823   8.866163    -6.85   0.000    -78.86535    -42.6511
------------------------------------------------------------------------------
. ovtest
Ramsey RESET test using powers of the fitted values of y
       Ho:  model has no omitted variables
                  F(3, 27) =      7.23
                  Prob > F =      0.0010
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |         34 -127.1765  -56.92202       4     121.844   127.9495
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
Here are the outputs of the second model we described above. It is the same except it uses log of disposable income, instead of disposable income itself, as a predictor. This is due to the tapering-off effect we can expect with increases in income. Our signs are the same as before, and coefficients very close to the first model. Our F statistic is still very good at 0.0000, and our R^2 and Rbar^2 are even slightly better than before. The only slight downgrade is the t statistic for the price of beef, which rose from 0.002 to 0.02. This is still well beyond our target of 0.05 for 95% two tail significance, so we can still deem the price of beef to be significant. Root MSE is almost a third lower than before.

```
. * Model 3
. reg y pc lnyd pb pr
      Source |       SS           df       MS      Number of obs   =        34
-------------+----------------------------------   F(4, 29)        =    451.87
       Model |  3475.65769         4  868.914422   Prob > F        =    0.0000
    Residual |  55.7644596        29   1.9229124   R-squared       =    0.9842
-------------+----------------------------------   Adj R-squared   =    0.9820
       Total |  3531.42215        33  107.012792   Root MSE        =    1.3867
------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          pc |  -.4732315   .0802222    -5.90   0.000    -.6373043   -.3091587
        lnyd |   12.39855   1.222571    10.14   0.000     9.898109    14.89899
          pb |   .1376957   .0571721     2.41   0.023     .0207657    .2546258
          pr |   -.146476   .2162211    -0.68   0.503    -.5886979    .2957458
       _cons |   -61.6503   9.043603    -6.82   0.000    -80.14655   -43.15406
------------------------------------------------------------------------------
. ovtest
Ramsey RESET test using powers of the fitted values of y
       Ho:  model has no omitted variables
                  F(3, 26) =      7.99
                  Prob > F =      0.0006
. estat ic
Akaike's information criterion and Bayesian information criterion
-----------------------------------------------------------------------------
       Model |        Obs  ll(null)  ll(model)      df         AIC        BIC
-------------+---------------------------------------------------------------
           . |         34 -127.1765  -56.65511       5    123.3102    130.942
-----------------------------------------------------------------------------
               Note: N=Obs used in calculating BIC; see [R] BIC note.
```
The third model is the same as the second one, except it also adds the prime rate as a predictor, on the hunch that borrowing rates might influence purchasers decisions. We can see that this may not be the case however. As we expect, R^2 and Rbar^2 do not decrease as they cannot, and our F-statistic still says that parts of the model are effective. Root MSE is also only marginally larger than before. However, if we look at the individual t statistics, we see a different story> We can see that the prime rate is insignificant at any reasonable degree of significance, with a t statistic of 0.503. The price of beef is also a worse predictor than before. This suggests that we have introduced an irrelevant variable bias with the introduction of prime rate.

## c)
Models one and two are the same, with the exception of how they treat the disposable income. Model 1 suggests that as disposable income increases, chicken consumption will increase linearly, whereas Model 2 suggests that there will be a tapering effect, instead comparing percentage change in disposable income to chicken consumption. Based on our economic intuition as well as our understanding of consumer spending, we can posit that Model 2 will be a more accurate predictor of chicken consumption, and this supposition is supported by the results of our models. We can see that R^2 values increase, and root MSE decreases when going from model 1 to model 2. The other variables are all very similar, and only change slightly by introducing the log space.

## d)
### Using model 1: yt = B0 + B1pct + B2ydt + B3pbt + error1t
Demand Elasticity wrt price:
where x = pr
[(dy / dx) * (X / Y)] = [B1 * (X / Y)] = [-.7919335 * (12.58235 / 36.45882)] = -0.273305183
This can be interpreted to mean that as the price of chicken goes up by 1%, the per capita consumption of chicken will go down by .273%, making demand fairly inelastic wrt price.

Demand elasticity wrt income:
where x = yd
[(dy / dx) * (X / Y)] = [B2 * (X / Y)]  = [0.0020373 * (4103.088 / 36.45882)] = 0.2292784347
This can be interpreted to mean that as the per capita disposable income increases by 1%, chicken consumption increases by 0.229%, meaning demand elasticity wrt income is also fairly inelastic

Cross price demand elasticity wrt price of beef:
where x = pb
[(dy / dx) * (X / Y)] = [B3 * (X / Y)]  = [0.2209139 * (31.32029 / 36.45882)] = 0.1897781501
This means there is a positive cross-price-elasticity of demand for chicken with respect to the price of beef. This suggests that chicken and beef are substitutes, and it is saying that if the price of beef increases by 1%, demand for chicken will rise by .190%

### Using model 2: yt = L0 + L1pct + L2lnydt + L3pbt + error2t
Demand elasticity wrt price using Model 2:
where x=pr
[(dy / dx) * (X / Y)] = [Lambda1 * X / Y] = [-.4508497 * (12.58235 / 36.45882)] = -0.1555933166
As the price of chicken increases by one percent, demand for chicken decreases by .1556%. Interestingly, this value is quite a bit lower than the one from the first model.

Demand elasticity wrt income using Model 2:
where x=lnyd
[(dy / dlnx) * (1 / Y)] = [Lambda2 * (1 / Y)] = [12.24209 * (1 /  36.45882)] = 0.3357785578
Using our new LinLog model, we get a higher demand elasticity wrt income. The value above can be interpreted to mean that as the log of per capita disposable income increases by 1%, demand rises by 0.336%

Cross price elasticity of demand using Model 2:
where x = price of beef
[(dy / dx) * (X / Y)] = [Lambda3 * (X / Y)] = [.1165359 * (31.32029 / 36.45882)] = 0.1001112538
This result suggests that as the price of beef increases by 1%, the demand for chicken increases by 0.1%. This indicates a positive cross price elasticity, supporting our earlier supposition that beef and chicken are substitutes.

### Using model 3: yt = Th0 + Th1pct + Th2lnydt + Th3pbt + Th4prt + error3t
Demand Elasticity wrt price:
where x=pr
[(dy / dx) * (X / Y)] = [Theta1 * X / Y] = [-.4732315 * (12.58235 / 36.45882)] = -0.1633175282
We can interpret this value as meaning that if the price of chicken increases by 1%, demand for chicken will fall by 0.163%.

Demand elasticity wrt income:
[(dy / dlnx) * (1 / Y)] = [Theta2 * (1 / Y)] = [12.39855 * (1 / 36.45882)] = 0.3400699748
This value means that as the log of per capita disposable income rises by 1%, demand will increase by 0.34%.

Cross price demand elasticity wrt price of beef:
[(dy / dx) * (X / Y)] = [Theta3 * (X / Y)] = [.1376957 * (31.32029 / 36.45882)] = 0.1182887777
This result means that as the price of beef rises by 1%, the demand for chicken will increase by 0.118%. This value supports our earlier findings suggesting a positive cross price elasticity, and hence elucidating that the two goods are substitutes.

## e)
If I had to choose one of these models as the best predictor of chicken consumption based on the available variables, I would choose Model 2. Firstly, it has the highest R^2 and Rbar^2 values between models 1 and 2, and does not significantly increase with the introduction of another variable in model 3. Furthermore, Model 2 has the lowest root MSE of all the models, meaning our predictions will tend to be closer to the actual values. The F-statistics for all models are very good, so we cannot easily use them to help our choice. T-statistics are actually slightly worse in model 1 than model 2 only for the price of beef, which we can reason to not be the main determinant of chicken consumption based on the elasticities we calculated earlier. Finally, we can compare BIC and AIC values to get the most accurate measure of comparison. Here Model 2 is again the best. Model 2’s BIC value of 127 is lower than 130 and 152. for models 3 and 1 respectively, and Model 2’s AIC value of 121 is also better than 123 and 146 for models 3 and 1 respectively. Based on all of these information criteria, we can select Model 2 as the best model for describing this relationship. The change from Model 1 to Model 2 of using the log of disposable income is very intuitive given the decreasing effect rising income has on consumption, and the variable added between Models 2 and 3 only introduces some extra irrelevant variable bias. To finalize this observation, we run a Ramsey’s RESET test on Model 2 above, which demonstrates that there are no major omitted variables. Thus we settle on Model 2.

<!-- {BearID:616F3953-90AE-40D3-B160-8166326D960E-312-0000E109339485E4} -->
# Regression HW8
[[ECON211 Regression]]
## Michael Calvey
#school
```
------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  N:\Classes\Spring19\ECON0211B\Workspace\MCALVEY\hw8.log
  log type:  text
 opened on:  12 Apr 2019, 17:27:27
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\cons9.dta"
. * Data setup / Summary
. tsset year
        time variable:  year, 1945 to 2006
                delta:  1 unit
. summ
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
        year |         62      1975.5    18.04162       1945       2006
         con |         62    4101.113    2494.729     1061.5     9821.7
         dpi |         62    4612.635    2695.691     1312.9    10595.4
         aaa |         62    2.593065      3.4445     -11.79       8.84
. corr 
(obs=62)
             |     year      con      dpi      aaa
-------------+------------------------------------
        year |   1.0000
         con |   0.9673   1.0000
         dpi |   0.9750   0.9992   
         aaa |   0.5432   0.4402   0.4543   1.0000
```
# Preamble/ Summary
We will be regressing disposable income and interest rates against consumption, working with time series data. From the summary table above, we can see that we are working with 62 years, from 1945 until 2006. Our consumption, denoted by *con*, has been growing over time, starting at 1061.5 and finishing at 9821.7. The mean is roughly in the middle of these values, suggesting constant but gradual increase. Disposable income has a similar story, ranging from 1312.9 to 10595.4. The high correlation results between year and consumption and dpi suggest both increased over time. Interest rates on the other hand did not change so consistently with time, with a correlation coefficient of 0.5432. Interest rates are interpreted as percent, since they range from -11.79 to 8.84. Consumption and dpi have an extremely high correlation of 0.9992, suggesting that there might be a case of autoregression.

We will be predicting the following model:
Using our economic intuition, we can predict that disposable income will have a positive sign since more income results in higher consumption, while rates may have a negative sign, since higher rates mean more expensive borrowing, and hence lower consumption.

```
. * Model 1- naive model
. reg con dpi aaa
      Source |       SS           df       MS      Number of obs   =        62
-------------+----------------------------------   F(2, 59)        =  22803.17
       Model |   379153531         2   189576766   Prob > F        =    0.0000
    Residual |  490503.303        59  8313.61531   R-squared       =    0.9987
-------------+----------------------------------   Adj R-squared   =    0.9987
       Total |   379644035        61   6223672.7   Root MSE        =    91.179
------------------------------------------------------------------------------
         con |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         dpi |   .9320223   .0048612   191.73   0.000      .922295    .9417496
         aaa |  -12.54195    3.80443    -3.30   0.002     -20.1546   -4.929304
       _cons |  -165.4441   23.09188    -7.16   0.000    -211.6509   -119.2374
------------------------------------------------------------------------------
```
a)
Above we see the results for this first naive model. From our results, we can see that disposable income has a large positive decimal coefficient. We can interpret this as the marginal propensity to consume, and the predicted value fits in the theoretical range between 0 and 1. The standard error is very low, at 0.0049, resulting in a very large t value of 191.73. Interest rate has, as expected, a negative coefficient of -12.5, suggesting that as rates rise, consumption falls. The standard error is larger as a proportion of the coefficient, with a t value of -3.3 and a resulting p value of 0.002, the highest of the predicted coefficients. 

The model has a very high degree of statistical accuracy, if interpreted naively. The F statistic is 22803.17, which results in a p value of 0.0000. This suggests that our overall model significance is very high. Our R squared and adjusted R squared values are 0.9987, both really high, supporting our supposition of high accuracy. Our Root MSE is fairly low at 91, since consumption ranges from 1061 to 9800, this makes up up to 10% of the predicted values depending on the time. All of the individual t statistics have:
H0: B = 0
HA: B /= 0
The resulting t values are statistically significant for all of the coefficients to a high degree of statistical accuracy.


b)
If we assume our model suffers from first order serial correlation, we first determine that it is likely pure serial correlation, since our model is specified on sound economic theory. Furthermore, we have three main likely consequences. First, it is unlikely that our coefficient estimates will be biased, meaning that they are not likely to be either negatively or positively skewed. However, second, because the model attributes some of the variation that comes from time to the independent variables, our variance increases. As a direct result of this, third, our standard errors are likely to be negatively biased, meaning we cannot trust the outcomes of the significance tests in this model.
```
* AC detection
. predict ehat, resid
. gen ehatlag = L.ehat
(1 missing value generated)
. corr ehat ehatlag
(obs=61)
             |     ehat  ehatlag
-------------+------------------
        ehat |   1.0000
     ehatlag |   0.8031   1.0000
* Plotting
. scatter ehat year
. ac(ehat)
. estat dwatson
Durbin-Watson d-statistic(  1,    61) =  2.047155
. estat bgodfrey
Breusch-Godfrey LM test for autocorrelation
---------------------------------------------------------------------------
    lags(p)  |          chi2               df                 Prob > chi2
-------------+-------------------------------------------------------------
       1     |          0.143               1                   0.7054
---------------------------------------------------------------------------
                        H0: no serial correlation
```
c)
First, we generate our lagged error variables for testing. Before we interpret further evidence, we can recall that our time variable was closely correlated with disposable income and consumption, suggesting autocorrelation may be present. Running a simple correlation between ehat and ehatlag suggests a correlation coefficient of 0.8031, a very high result. This means our lagged residuals are closely correlated with our residuals.




Our durbin-watson d statistic is 2.047155.
H0: Rho = 0
H1: Rho > 0
For N=62 and K=2, our DL is 1.525 and DU is 1.655. Since our d statistic is larger than our DU, we can reject our null hypothesis that Rho is equal to 0, hence concluding that there is likely autocorrelation.

Our Breusch-Godfrey autocorrelation test has a similar conclusion.
H0: Rho = 0 
H1: Rho /= 0
Our chi2 statistic is 0.143, and with one degree of freedom, our resulting p value is 0.7054, meaning we reject the null hypothesis and again conclude that there is likely autocorrelation.


```
* Rho estimation
. reg ehat ehatlag, noconstant
      Source |       SS           df       MS      Number of obs   =        61
-------------+----------------------------------   F(1, 60)        =    108.54
       Model |  313710.552         1  313710.552   Prob > F        =    0.0000
    Residual |  173414.534        60  2890.24224   R-squared       =    0.6440
-------------+----------------------------------   Adj R-squared   =    0.6381
       Total |  487125.086        61  7985.65715   Root MSE        =    53.761
------------------------------------------------------------------------------
        ehat |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     ehatlag |   .8166858   .0783894    10.42   0.000     .6598836     .973488
------------------------------------------------------------------------------
```
d) 
To test whether there is a significant relationship between ehat and ehatlag, we will run a regression without a constant term to attempt to estimate Rho. 
As such, our model will be: ehat = Rho * ehatlag
Running the above model, we get a coefficient of 0.8166858, which can be interpreted as a very strong relationship between the two sets of residuals. This means that we can predict 81.7% of a residual if we know the previous residual, with only 18.3% of the total value actually being a classical error term. This test runs with 61 observations since we can have at most one less lag residual. With this in mind, our overall f statistic is 108.54, resulting in a p value of 0.0000, suggesting a very significant result. Our R squared suggests a good degree of significance at 0.6440, while our adjusted R squared value is decently lower, suggesting that our degrees of freedom have an impact on the final significance. In an individual significance test of our Rho value, we also see very high significance. Our standard error is 0.078, which as a proportion of our original coefficient is very small. This results in a t statistic of 10.42, and a resulting P value of 0.000. All of these measures of significance suggest that there is a significant relationship between our residual and our lag residual.

e)
Since we know that we are likely dealing with pure first order correlation, our process is somewhat simplified. We can use the Generalized Least Squares (GLS) process to find the true value of predicting coefficients, after taking into account the degree of autocorrelation. First, we need to generate a prediction for Rho, using a similar technique to our approach in d). Then, we use this Rho value to predict our original model. We repeat this process until our Rho value settles and no longer changes significantly with additional iterations. Thus, we end up with a Rho value, representing the amount of relation between serial error terms, as well as predicted coefficients for the independent variables.

```
. 
. prais con dpi aaa, corc
Iteration 0:  rho = 0.0000
Iteration 1:  rho = 0.8167
Iteration 2:  rho = 0.8578
Iteration 3:  rho = 0.8603
Iteration 4:  rho = 0.8605
Iteration 5:  rho = 0.8605
Iteration 6:  rho = 0.8605
Iteration 7:  rho = 0.8605
Cochrane-Orcutt AR(1) regression -- iterated estimates
      Source |       SS           df       MS      Number of obs   =        61
-------------+----------------------------------   F(2, 58)        =   1985.00
       Model |  10459009.6         2  5229504.78   Prob > F        =    0.0000
    Residual |  152801.283        58  2634.50487   R-squared       =    0.9856
-------------+----------------------------------   Adj R-squared   =    0.9851
       Total |  10611810.8        60  176863.514   Root MSE        =    51.327
------------------------------------------------------------------------------
         con |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         dpi |   .9301259   .0147714    62.97   0.000     .9005579     .959694
         aaa |  -4.812111   2.739311    -1.76   0.084    -10.29544    .6712165
       _cons |  -155.5887   95.05387    -1.64   0.107    -345.8597    34.68229
-------------+----------------------------------------------------------------
         rho |   .8605269
------------------------------------------------------------------------------
Durbin-Watson statistic (original)    0.385767
Durbin-Watson statistic (transformed) 2.075110
```
f)
The above regression implements the GLS Cochrane-Orcutt process described in e). Our result suggests quite strongly that Autocorrelation has either been eliminated or hugely reduced. First, we notice that while the p value for the f statistic is the same at 0.0000, our actual F statistic has decreased significantly from our first regression. We notice that our coefficient estimates have not changed significantly from before, but our new standard errors are much larger, resulting in lower t values for every one of the estimated coefficients. 

We can again interpret the coefficient of DPI to be the marginal propensity to consume. The t value has fallen from 191.73 to 62.97, consistent with what we would expect when correcting for serial correlation, since we now attribute some of the change in our dependent variable to Autocorrelation, whereas before all of it was attributed to our independent variables.  Our p value is still 0.000 here however, suggesting that the relationship between disposable income and consumption is still highly significant. 

Our interest rate coefficient has changed quite significantly however, from -12 to -4.8. This suggests that some of the variation in consumption that we originally attributed to interest rate sensitivity was in fact coming from first order Autocorrelation. As such, our t value increases dramatically, to a point where we need to pick hairs to determine significance, as we can only achieve 95% significance with a one tailed test, based on our p value of 0.084.

Finally, our Rho value is predicted to be 0.8605269 after 7 iterations of this process. This suggests that a large proportion of our error term is correlated strongly with our lagging error term. Hence, our First Order Autocorrelation appears to be corrected. 

g)
The above analysis contains a comparison of each individual coefficient and statistic, as well as its significance on our overall conclusions from the model. To summarize, after correcting for Autocorrelation, we see that our overall significance has decreased somewhat, and individual significance generally decreased quite significantly, since we are now correctly attributing some of the dependent variable variation to autocorrelation.

```
. * Correcting
. newey con dpi aaa, lag(1)
Regression with Newey-West standard errors      Number of obs     =         62
maximum lag: 1                                  F(  2,        59) =   14080.30
                                                Prob > F          =     0.0000
------------------------------------------------------------------------------
             |             Newey-West
         con |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         dpi |   .9320223   .0077924   119.61   0.000     .9164298    .9476148
         aaa |  -12.54195   4.365897    -2.87   0.006    -21.27809   -3.805813
       _cons |  -165.4441   32.21908    -5.13   0.000    -229.9144   -100.9739
------------------------------------------------------------------------------
. 
end of do-file
```
```
. * Model 1- naive model
. reg con dpi aaa
      Source |       SS           df       MS      Number of obs   =        62
-------------+----------------------------------   F(2, 59)        =  22803.17
       Model |   379153531         2   189576766   Prob > F        =    0.0000
    Residual |  490503.303        59  8313.61531   R-squared       =    0.9987
-------------+----------------------------------   Adj R-squared   =    0.9987
       Total |   379644035        61   6223672.7   Root MSE        =    91.179
------------------------------------------------------------------------------
         con |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         dpi |   .9320223   .0048612   191.73   0.000      .922295    .9417496
         aaa |  -12.54195    3.80443    -3.30   0.002     -20.1546   -4.929304
       _cons |  -165.4441   23.09188    -7.16   0.000    -211.6509   -119.2374
------------------------------------------------------------------------------
```
h)
For our marginal propensity to consume, our standard error under the Newey-West model is slightly larger than the naive model, going from 0.0049 to 0.0078, but much lower than our cochrane-orcutt result of 0.014. For our interest rate sensitivity, our standard error is again slightly increased by the Newey-West method as compared to the naive model, going from 3.8 to 4.4. In this case, our Cochrane-Orcutt method actually had the lowest, with a value of 2.7. This suggests that our original model and our Cochrane-Orcutt may have overestimated the standard error for interest rate sensitivity.
For our constant, our standard error increases almost by 50% going from 23 in the naive model to 32 in the new model. The Cochrane-Orcutt result here gives a much higher value than either; 95. In general, there is strong reasoning for using the Newey-West standard errors, since our initial assumptions about Autocorrelation dictate that our coefficient estimates will be unbiased, it makes sense to keep the Bhat estimates, while changing the standard errors to reflect the correction for autocorrelation.

<!-- {BearID:47B73E33-E1B4-431B-8593-6770D42B9E99-299-00001506D80BDAA0} -->
# Regression HW9
<!-- #school/Linear-Regression -->
# a) 
Based on our economic intuition, we can reason that each of the coefficients will be negative. Crime rate will clearly decrease the value of a home, so the sign on c must be negative. Higher nitric oxides concentrations suggest the presence of heavy industry, which again negatively influences home values. Proportion of owner-occupied units built prior to 1940 is an interesting metric. We could reason that it would be positive as older homes might have historic value, however in general for this model we will assume that older homes will also tend to be more dilapidated, resulting in lower value. Larger distances from employment centers would also result in lower value. Tax is also not totally clear, since we could reason that higher tax rates are a result, rather than an inhibitor of, higher value homes, however in this case we shall use the assumption that higher taxes might drive buyers away, decreasing home values and acting as a proxy for upkeep costs.

### Variable Summary
```
. estat sum
  Estimation sample regress                Number of obs =        506
-------------------------------------------------------
      Variable |         Mean      Std. Dev.         Min          Max
  -------------+-----------------------------------------------------
           lnv |     3.034513      .4087569     1.609438     3.912023
             c |     3.613525      8.601545        .0063      88.9762
            nt |     .5546951      .1158777         .385         .871
             a |      68.5749      28.14886          2.9          100
             d |     3.795043       2.10571       1.1296      12.1265
           tax |     408.2372      168.5371          187          711
  -------------------------------------------------------------------
. corr v c nt a d tax
(obs=506)
             |        v        c       nt        a        d      tax
-------------+------------------------------------------------------
           v |   1.0000
           c |  -0.3883   1.0000
          nt |  -0.4273   0.4210   1.0000
           a |  -0.3770   0.3527   0.7315   1.0000
           d |   0.2499  -0.3797  -0.7692  -0.7479   1.0000
         tax |  -0.4685   0.5828   0.6680   0.5065  -0.5344   1.0000
```
We will be initially running a regression with the following model: lnv = c + nt + a + d + tax. In this regression, we are using crime, nitric concentrations, home age, distance to employment and property tax to predict median home value. This specification seems economically sound, and there is good reasoning for including each of the variables added. 

A brief summary of our variables reveals that the log of value is distributed as we would expect for a logarithmic variable, with a mean of 3.03. Our crime rate is measured on a per capita basis and has a mean of 3.6, ranging from about 0 to 89. Nitric oxides concentration is measured in PPM, and ranges from 0.385 to 0.871. Our Home age will be measured as the proportion of owner-occupied units built prior to 1940, and ranges from 2.9 to 100, with a mean of 68. Distances are simply weighted distances to employment centers, and these values range from 1.13 to 12.13, with an average of 3.80.

Our correlation matrix does not suggest any serious multicollinearity, however we do notice that our nitric oxides and age as well as nitric oxides and distance are quite closely correlated. We could reason that areas with more old homes might also be close to industrial developments, and this idea is supported by the large negative correlation of -0.75 between age and distance. Based on our correlation results as well as our economic intuition, we could hypothesize that our coefficients will have the following signs:
* c = negative since more crime would reduce home value
* nt = negative since more nitric oxides means more industry and pollution
* a = negative since older homes could be reasoned to have lower value
* d = negative since larger distance to work would result in lower value
* tax = negative, since lower tax essentially can be equated to lower maintenance cost.

### Model 1: Naive Regression
```
. set more off
. use "N:\Classes\Spring19\ECON0211B\Handouts\cleanair.dta"
. 
. * Model 1 - Naive Regression
. gen lnv = log(v)
. reg lnv c nt a d tax
      Source |       SS           df       MS      Number of obs   =       506
-------------+----------------------------------   F(5, 500)       =     78.91
       Model |  37.2157633         5  7.44315266   Prob > F        =    0.0000
    Residual |  47.1607323       500  .094321465   R-squared       =    0.4411
-------------+----------------------------------   Adj R-squared   =    0.4355
       Total |  84.3764956       505  .167082169   Root MSE        =    .30712
------------------------------------------------------------------------------
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0142242    .001966    -7.23   0.000     -.018087   -.0103615
          nt |  -.9367258   .2216862    -4.23   0.000    -1.372277   -.5011746
           a |  -.0036124   .0007871    -4.59   0.000    -.0051589   -.0020659
           d |   -.055945   .0112514    -4.97   0.000    -.0780509   -.0338391
         tax |  -.0005764   .0001218    -4.73   0.000    -.0008157   -.0003371
       _cons |   4.300864   .1393845    30.86   0.000     4.027012    4.574715
------------------------------------------------------------------------------
. predict ehat, resid
. gen ehat2 = ehat^2
. * scatter ehat a
```
Here are the results of our LogLin regression. as estimated, all of our coefficients appear negative to a high degree of statistical certainty. In this case our coefficients represent partial elasticities, representing the percentage change in our dependent variable, from a one unit change in each of our independent variables. As such, we can see that one unit higher of crime results in a 1.5% drop in value, one part per million of additional nitric oxides results in an almost 93% decrease, one year older  average houses results in a 0.3% decrease in value, one extra weighted unit of distance results in a 5.5% decrease, and one extra tax dollar per 10000 of value results in a .05% decrease in value.

Our results appear to be decently statistically significant at first glance. Our F-value is quite large at 78, resulting in an f statistic of 0. R squared is 0.4411, fairly low, and adjusted R squared is 0.4355, suggesting our degrees of freedom are slightly influencing the results. Our root MSE is quite low at 0.307, given the values range from about 5000 to 45000. In terms of individual significance, the story appears even more positive. All of the variables have high individual significance, meeting any reasonable test of significance. This is actually a suggestion that we may be experiencing some heterosketasticity, and thus overestimating some of our t values. It is unlikely that the coefficient estimates are biased, however.

# b)
### Model 2: Robust Regression
```
. * Model 2 - Robust Regression
. reg lnv c nt a d tax, robust
Linear regression                               Number of obs     =        506
                                                F(5, 500)         =      74.82
                                                Prob > F          =     0.0000
                                                R-squared         =     0.4411
                                                Root MSE          =     .30712
------------------------------------------------------------------------------
             |               Robust
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0142242   .0031181    -4.56   0.000    -.0203505    -.008098
          nt |  -.9367258   .1986804    -4.71   0.000    -1.327077   -.5463745
           a |  -.0036124   .0006766    -5.34   0.000    -.0049417   -.0022832
           d |   -.055945   .0097236    -5.75   0.000    -.0750491   -.0368409
         tax |  -.0005764   .0001478    -3.90   0.000    -.0008668   -.0002861
       _cons |   4.300864   .1182364    36.38   0.000     4.068562    4.533165
------------------------------------------------------------------------------
```
Above are the results of a robust regression, using the same model as specified above. The overall significance decreases slightly, while R squared and root MSE stay the same. As expected, coefficient estimates remain the same.

The results are very different when it comes to the robust standard errors, and by extension t values and resulting P-statistics. 
Crime rate gets a twofold increase in standard error, while nitric oxides standard error decreases slightly from .221 to .198. Age also becomes more significant, as well as distance. Tax’s standard error decreases from 0.00012 to 0.00014. Overall, we see adjusted standard errors, but not always larger than the naive model. This suggests that there may be some intrinsic issues that are not fixed with the robust regression.
# c)
### Goldfeld-Quandt test
```
. * Goldfeld-Quandt test
. gen n = _n
. sort a, stable
. reg lnv c nt d a tax if _n <= 202
      Source |       SS           df       MS      Number of obs   =       202
-------------+----------------------------------   F(5, 196)       =     10.71
       Model |   2.8206579         5   .56413158   Prob > F        =    0.0000
    Residual |  10.3247587       196   .05267734   R-squared       =    0.2146
-------------+----------------------------------   Adj R-squared   =    0.1945
       Total |  13.1454166       201  .065400082   Root MSE        =    .22952
------------------------------------------------------------------------------
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0089711   .0156727    -0.57   0.568    -.0398798    .0219376
          nt |  -1.757228   .4835086    -3.63   0.000    -2.710775   -.8036808
           d |  -.0452257   .0119249    -3.79   0.000    -.0687433   -.0217081
           a |  -.0020217   .0011416    -1.77   0.078    -.0042731    .0002297
         tax |  -.0005881   .0002336    -2.52   0.013    -.0010489   -.0001274
       _cons |   4.555397   .2535639    17.97   0.000     4.055333    5.055461
------------------------------------------------------------------------------
. predict ehatL, resid
. gen ehatL2 = ehatL^2
. reg lnv c nt d a tax if _n >= 305
      Source |       SS           df       MS      Number of obs   =       202
-------------+----------------------------------   F(5, 196)       =     22.80
       Model |  15.3180319         5  3.06360638   Prob > F        =    0.0000
    Residual |  26.3402915       196  .134389242   R-squared       =    0.3677
-------------+----------------------------------   Adj R-squared   =    0.3516
       Total |  41.6583234       201   .20725534   Root MSE        =    .36659
----------------------------------------------------------------------------
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0136214   .0025858    -5.27   0.000     -.018721   -.0085217
          nt |  -1.054857   .3334478    -3.16   0.002    -1.712463   -.3972503
           d |  -.1546129   .0352619    -4.38   0.000    -.2241543   -.0850715
           a |  -.0154226   .0066135    -2.33   0.021    -.0284653   -.0023799
         tax |  -.0008228   .0002006    -4.10   0.000    -.0012184   -.0004272
       _cons |   5.841358    .674614     8.66   0.000     4.510924    7.171792
------------------------------------------------------------------------------
. predict ehatH, resid
. gen ehatH2 = ehatH^2
. egen ehatH2sum = sum(ehatH2)
. egen ehatL2sum = sum(ehatL2)
. di(ehatH2sum/ehatL2sum)
1.8787767
```
Our resulting GQ-F ratio, as indicated above, is 1.89. Given our degrees of freedom, our critical F value is 1. Since our observed F value is larger, we can reject our null hypothesis that the errors in the upper portion are the same as the errors in the lower one. This indicates strongly that heteroskedastisticity may be present.

### Breusch-Pagan-Godfrey Test
```
. * Breusch-Pagan-Godfrey test
. * H0: a1 = a2 = a3 = a4 = a5
. * HA: H0 false
. reg lnv c nt a d tax
      Source |       SS           df       MS      Number of obs   =       506
-------------+----------------------------------   F(5, 500)       =     78.91
       Model |  37.2157633         5  7.44315266   Prob > F        =    0.0000
    Residual |  47.1607323       500  .094321465   R-squared       =    0.4411
-------------+----------------------------------   Adj R-squared   =    0.4355
       Total |  84.3764956       505  .167082169   Root MSE        =    .30712
------------------------------------------------------------------------------
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0142242    .001966    -7.23   0.000     -.018087   -.0103615
          nt |  -.9367258   .2216862    -4.23   0.000    -1.372277   -.5011746
           a |  -.0036124   .0007871    -4.59   0.000    -.0051589   -.0020659
           d |   -.055945   .0112514    -4.97   0.000    -.0780509   -.0338391
         tax |  -.0005764   .0001218    -4.73   0.000    -.0008157   -.0003371
       _cons |   4.300864   .1393845    30.86   0.000     4.027012    4.574715
------------------------------------------------------------------------------
. 
. hettest c nt a d tax, iid
Breusch-Pagan / Cook-Weisberg test for heteroskedasticity 
         Ho: Constant variance
         Variables: c nt a d tax
         chi2(5)      =    33.21
         Prob > chi2  =   0.0000
. reg ehat2 c nt a d tax
      Source |       SS           df       MS      Number of obs   =       506
-------------+----------------------------------   F(5, 500)       =      7.03
       Model |  1.02816991         5  .205633982   Prob > F        =    0.0000
    Residual |   14.635371       500  .029270742   R-squared       =    0.0656
-------------+----------------------------------   Adj R-squared   =    0.0563
       Total |  15.6635409       505  .031016913   Root MSE        =    .17109
------------------------------------------------------------------------------
       ehat2 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |   .0011233   .0010952     1.03   0.306    -.0010285    .0032751
          nt |  -.2478027   .1234952    -2.01   0.045    -.4904363   -.0051692
           a |   .0005326   .0004385     1.21   0.225    -.0003289    .0013941
           d |  -.0177944   .0062679    -2.84   0.005     -.030109   -.0054799
         tax |   .0001031   .0000679     1.52   0.129    -.0000302    .0002364
       _cons |    .215522   .0776472     2.78   0.006     .0629669     .368077
------------------------------------------------------------------------------
. di(0.0656 * 506)
33.1936
```
For the BPG test, our degrees of freedom will be 5. Our Chi Observed value is 33, both from the bootleg and the official state implementation. Our Chi critical will be Chi2(5) = 11.07 at 5% significance. Since our Chi observed is much larger than the critical value, we can reject the null hypothesis that all of our coefficients relating our independent variables to our square residuals will be 0. This means that there is a case of heteroskedasticity.

### White Test
```
. * White test
. imtest, white
White's test for Ho: homoskedasticity
         against Ha: unrestricted heteroskedasticity
         chi2(20)     =     98.11
         Prob > chi2  =    0.0000
Cameron & Trivedi's decomposition of IM-test
---------------------------------------------------
              Source |       chi2     df      p
---------------------+-----------------------------
  Heteroskedasticity |      98.11     20    0.0000
            Skewness |      38.41      5    0.0000
            Kurtosis |       7.60      1    0.0058
---------------------+-----------------------------
               Total |     144.12     26    0.0000
---------------------------------------------------
``` 
For the white test, we use state’s built in function to save having to add 20 coefficients. The resulting Chi2(Observed) = 98.11. With our df = 20, our Chi2(20) = 31.4, if testing at 5% significance. Since our Chi observed is much larger than our Chi critical, we can reject our null hypothesis that there is no connection between the residuals and either our base variables, or their squares and interactions. Hence, the white test further supports the initial claim of heteroskedasticity.

# d)
### Model 3: Weighted Least Squares
```
. * Model 3 - Weighted Least Squares Regression
. reg lnv c nt a d tax [weight = 1/a]
(analytic weights assumed)
(sum of wgt is 11.04974902544757)
      Source |       SS           df       MS      Number of obs   =       506
-------------+----------------------------------   F(5, 500)       =     69.70
       Model |  24.9527768         5  4.99055537   Prob > F        =    0.0000
    Residual |  35.7977704       500  .071595541   R-squared       =    0.4107
-------------+----------------------------------   Adj R-squared   =    0.4048
       Total |  60.7505472       505  .120298113   Root MSE        =    .26757
------------------------------------------------------------------------------
         lnv |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           c |  -.0144293   .0024002    -6.01   0.000    -.0191449   -.0097137
          nt |  -.9386902   .2397309    -3.92   0.000    -1.409694   -.4676861
           a |  -.0018765   .0006041    -3.11   0.002    -.0030633   -.0006896
           d |  -.0275054   .0087124    -3.16   0.002    -.0446229   -.0103878
         tax |   -.000497   .0001277    -3.89   0.000    -.0007479   -.0002461
       _cons |   4.043292   .1287973    31.39   0.000     3.790241    4.296342
------------------------------------------------------------------------------
```
Above are the results for our corrected, weighted least squares regression using 1/a as our FOP, as we identified earlier. Our results are still in line with our theoretical expectations. Our overall significance is quite high, with a slightly decreased F value from our naive regression. Our R squared is roughly similar at 0.41. Besides this, most of the conclusions from the prior models hold.

# e) 
Our initial naive regression performed similarly for the most part to our WLS regression, except for several key differences that make a big difference overall. Firstly, our overall significance F value falls from 79 to 70, a significant drop. Our R squared and especially adjusted R squared values also drop relative to the original regression, but are at roughly similar levels overall. Root MSE goes from 0.31 to 0.27, suggesting our estimates have a tighter spread after the correction. These overall statistics seem slightly contradictory, but overall make sense given our expected changes in standard errors, as noted earlier. 

On the individual significance level, our results are more different. While our coefficient signs are all still negative, our standard errors are all larger as a proportion of our initial coefficients. Most coefficients are roughly similar to the original estimates, although our age and distance both get smaller coefficients by a factor of about 2. In terms of standard errors, the story is more consistent, as we notice all standard errors are slightly higher after our WLS regression, with the exception of the constant. These results are consistent with what we expected to be wrong in the first part of this homework, as we can see some shifts in our coefficients and a general increase in the uncertainty of estimates. We also notice that OLS was not the minimum variance estimator, as expected.

<!-- {BearID:3EEF5B56-5D78-4317-8C0A-923EC76CB8CA-21279-0000560D620FBB88} -->
# Regression Project
#school
[[ECON211 Regression]]
White/ethnic minority unemployment differential tends to move counter cyclically (Blackaby), (Jones 1993), (Labour Market Trends 1996).

Blackaby:
- Look at housing market
- educational attainment
- occupational attainment
- Household composition
	- Unemployment highest among unmarried population
	- children
	- family size
- Poor health
- Qualifications

- Discrimination/ Isolation
-

<!-- {BearID:096590A1-1E18-4752-9ED9-BBD060CF4429-530-000020119BF148FC} -->
[[Machine Learning TOC|Machine Learning]]
# Regression
Regression is a type of [[Machine Learning TOC|ML]] problem that involves predicting values in a continuous set based on input [[Features]][[053 Computer Science TOC]] | [[Natural Language Processing TOC|NLP]]
# Regular Expressions
#concepts/cs

[[Chomsky Hierarchy TOC]]

Regular expressions are closed under various operations including:
	* Union
	* Intersection
	* Complement
	* Concatenation
	* Reverse

## Example
Match any of:
- woodchuck
- woodchucks
- Woodchuck
- Woodchucks

### Disjunction
- Letters inside square brackets, match *any*:

 | Pattern      | Matches              |
 | ------------ | -------------------- |
 | [wW]oodchuck | woodchuck, Woodchuck |

### Ranges
- Simplify accessing disjunctions

| Pattern    | Matches                  | Example                             |
| ---------- | ------------------------ | ----------------------------------- |
| [A-Z]      | Any upper case letter    | **D**renched Blossoms               |
| [a-z]      | Any lower case letter    | **m**y beans were impatient         |
| [0-9]      | Single digit             | Chapter **1**: down the rabbit hole |
| [0-9]{2-4} | Any string of 2-4 digits | **1234**5 a h**125**a               |

### Negation in disjunction
- Carat means negation when first char in disjunction

| Pattern    | Matches         |
| ---------- | --------------- |
| [\^A-Z]    | Not capital     |
| [\^A-Za-z] | Non-alphabetics |

### Pipe
- Pipe adds "OR" logic
- `a|b|c = [abc]`

| Pattern                 | Matches                             |
| ----------------------- | ----------------------------------- |
| [Gg]roundhog\|Woodchuck | Groundhog OR groundhog OR Woodchuck |

### More Operators

| Pattern | Matches                    | Example             |
| ------- | -------------------------- | ------------------- |
| colou?r | optional previous char     | color, colour       |
| oo\*h!  | 0 or more previous char    | oh!, ooh!, oooooh!  |
| o+h!    | 1 or more of previous char | oh!, ooh!, oooooh!  |
| beg.n   | Any char                   | begin, begun, beg2n |

### Special

| Pattern | Matches                  | Example       |
| ------- | ------------------------ | ------------- |
| \^[A-Z] | Start of line            | **P**alo Alto |
| \.$     | End of line, dot escaped | The end**.**  |

## Example
Find all occurrence of the word "the" in:
```
The cat in the hat!
The other one there, the blithe one!
```

Start with `the`, but this ignores caps.
So add `[Tt]he`, but this is too general and returns other and blithe too.
Finally augment with `[^a-zA-Z][tT]he[^a-zA-Z]` giving us the right result by excluding non-alphanumerics.

This is actually the process of solving [[Type 1 Errors]] and [[Type 2 Errors]]
[[Machine Learning TOC]] | [[ML Models TOC]]
# Reinforcement Learning
#concepts/cs/ml 
#🌱 

Here the model goes out and collects data itself. Like those things that learn how to walk. Basically self-implements a trial and error method. Very much based on the [[Visual Cortex]].

[[Hassabis - Neuroscience Inspired AI]]: "RL Methods address the problem of how to maximize future reward by mapping states of the environment to actions"

NEED WAY MORE!! Very interesting topic
#todo 
- [x] Learn more about Reinforcement Learning


 ## Further Reading

- Denil, M., Agrawal, P., Kulkarni, T.D., Erez, T., Battaglia, P., and de Freitas, N. (2016). Learning to perform physics experiments via deep reinforcement learning. arXiv, arXiv:161101843.
	- Might be an interesting read on gaining commonsense knowledge through trial and error
	- ---
aliases: ["Relations"]
---
# Relational Thinking
#concepts/mental_models [[Quantum Mechanics TOC]] | [[Quantum Phases]]
# Relative Phases
#concepts/cs/quantum 
See also [[Global Phases]]
#todo
* Relative phase is what you must multiply one item in a quantum state by to get to another. If two different-represented states have the same relative phase, they are the same state, since bases are orthonormal!

### Example
Start with:
$$\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})
$$

Multiply by [[Global Phases]] -1:


![[Pasted image 20210323105626.png]]
Relative phase is the phase between two elements in a vector
![[Pasted image 20210323105731.png]]

This is the same as !!

Some [[Quantum Gates]] are relative phases (note they seem like [[Global Phases]] but are not!) 


## Where they show up:
[[CS333 PS5]]
[[Deutsch's Algorithm]] <- Used to solve![[Cognitive Biases TOC]]
# Representativeness Bias
#concepts/mental_models #concepts/cognitive_biases 

This is the bias that emerges when people judge the likelihood or frequency of an event by how much it fits. Think also of [[Cognitive Ease]], [[Tendency to Stereotype]].

This goes hand in hand with ignorance of [[Base Rates]] and failure to notice [[False Conjunctions]].

[[095 Journals MOC]]
# Request for confirmation of acceptance of studies

Dear Rasheed,

I hope you are well. I am writing because soon I will need to start applying for my student visa. As you might know this is a really tough process. I was hoping you could advise me what you may do to help with the application. I am pretty sure I can’t get a CAS on the 6 month course, but I know I will still need some sort of documentation from you to confirm to the British authorities that I am studying in your course.

Please let me know,

Farjana
[[XCAP Current Positions]]
# Research areas
#projects/old/xcap

* Xilinx
* Intel
* Microsoft
*
[[070 Finance MOC]]
# Residential Mortgages
#concepts/finance #concepts/econ

This note will be a central reference point on how mortgages work, broadly as well as individually. Interesting dynamics of the housing market may also be referenced here.

# Determinants of Mortgage Delinquency & default
Broadly, three core categories of variables affect delinquency and mortgage rates: 1) Borrower characteristics; 2) loan characteristics; and 3) economic conditions and shocks to income
1. *Loan-to-value ratio* has a positive effect on delinquency as well as default.
	* (Herzog & Earley 1970), (von Fürstenberg & Green 1974)

2. *Income stability*, measured by time of occupation significantly decreases delinquency
	* (Herzog & Earley 1970)

3. *Presence of Junior financing* has a positive effect on delinquency
	* (Herzog & Earley 1970), (von Fürstenberg & Green 1974)

4.  *Age pattern to delinquency* - delinquency rates increase after origination, peaking at the fourth year, before falling to about one-third of their initial level by their fifteenth year.
	* (von Fürstenberg & Green 1974)

6.  *Income* is negatively linked to delinquency
	* (von Fürstenberg & Green 1974)

6.  *Interest rates* positively affect delinquency, moreso than home equity 
	* (Campbell & Dietrich 1983)

7.  *Unemployment rate* has a positive impact on delinquency & default rates
	* (Campbell & Dietrich 1983)

9.  *Mortgage payment-to-income ratio* is shown to positively affect delinquency rates. This is used as a proxy for examining the effects of income and expense shocks on borrowers’ decisions to delay mortgage payments.
	* (Webb 1982) 

10. *Expected equity position at default* determines whether defaulters are “ruthless” or “trigger event” types. Delinquency and default profiles differ for these two groups. Subprime borrowers are more likely to be trigger event defaulters, who default for reasons such as job loss, illness, death, divorce, or jumps in payments on adjustable-rate mortgages (ARMs).
	* (Ambrose and Capone 1998)

Loans that are very delinquent prepay while those with negative equity default 
	* (Danis & Pennington-Cross 2005)

12. *Credit scores* are negatively related to delinquency and default
	* (Danis & Pennington-Cross 2005)
13.  
14.  
15.    



# Delinquency vs Default
* While delinquencies matter, it has been shown (Morton 1975) that most delinquencies are cured.
* The factors that most affect default are not the same as those that affect delinquencies - variables capturing home equity and a borrower’s ability to pay are more relevant to defaults than delinquencies.
* Prime mortgages default later and the losses are smaller than for subprime loans.
* Studies after (Campbell & Dietrich 1983) extend the work and consider delinquency and default to be different outcomes.
	* (Ambrose & Capone 1983), (Capozza and Thomson 2005), (Danis and Pennington-Cross 2005), (Capozza and Thomson 2006) in particular extend the work of Campbell & Dietrich


### Backreferences:
[[ECON411 Final Paper]]# Responses to Presentations
[[200 Outputs MOC]]
# Resume updates
- Quick learner -> Work ethic, entrepreneurial/ business oriented, dual nationality background/ lived in 3 countries -> global perspective
- Shohoz -> Look for quantifiable metrics
- Machine learning -> developed graphics tool using machine learning which helped identify fraud cases
- Steam/ cloudcity -> chartered to negotiated
- Capital letters for steam and facebook
- Skills interestss
	- Native russian/ english/ 
	- Delete understanding and employing concepts
	- Add wall street prep modeling modeling course
	- Winner of Middlebury entrepreneurship
	- Experienced programmer with significant experience in python, web/ collaboration
- Paraglider, skier and rock climber instead of adventurer/ explorer
	- 


IB -> industry team covering tech companies, using knowledge of cs to participate in presentations or valuation studies
Corporate finance -> better learning

<!-- {BearID:FF8E9152-9C6B-4F80-84DF-9F2F5AC9D7CC-21279-00005C98F440F0F4} -->
# Rise 2Q20
[[XCAP Writing]]
#projects/old/xcap
Individual performance reviews
- Compile into overall performance rundown, quarterly performance, comparison with past quarters & benchmarks
- One pager reflection of performance, explaining market circumstances and how we adapted to them
- letter on future strategy based on this point
- Trade evaluation
- allocation review
- Per-position positioning review and trade evaluation. Reflection + lay out strategy
- XLE
-

<!-- {BearID:3EFC1F2E-EF09-4BBD-8346-F30566D43FA5-324-0000798D1058B898} -->
[[XCAP Writing]]
# Rise Bio
#projects/old/xcap 

Mike is a double computer science and economics major in his senior year at Middlebury. After co-founding Rise in January 2018, Mike has continued to grow his passion and experience in asset management. At the moment he is pursuing a career at Morgan Stanley Wealth Management, where he hopes to work after graduation.

In the meantime Mike enjoys thinking about ways to integrate technology into the group’s trading strategy, especially focusing on algorithmic trading and performance visualisation.

Outside of finance and coding, Mike is an adventurer and loves climbing, skiing and paragliding. He also plays in a part-time, super-secret band with Rise’s CFO Carl Langaker.

<!-- {BearID:16A11D0B-29D8-4419-A50C-78F680AC9066-324-00001B0FA2890AD4} -->
# Rise Call Agenda
#projects/old/xcap
[[XCAP Meeting Minutes]]
- Positioning - now fairly conservative, want no more than 25% cash
- $10570 cash rn 
- General plans
- Timeline for investing


New investments
+ 10% EU
- 5-10% UltraEM
- 20% Industrials/ Materials
- 20% video games
- 15-20% small 
- 25% PureAlpha fund
-

<!-- {BearID:43C5B787-781B-4AC8-A079-96C03345920F-4760-000349BD0A184BA5} -->
[[071 XCAP MOC]]
# Rise Deliverables#
#projects/old/xcap

1. Weekly watchlist report:
	1. Update Excel spreadsheet with basic stats on each company (according to template)
		1. Ticker
		2. Company
		3. Sector
		4. Sub sector
		5. Strategy/ style (eg growth, value, SMID)
		6. Mkt cap
		7. Week high price, week low price and week end price
	2. Mention any earnings coming that week
2. Weekly earnings report:
	1. Advanced notice for all earnings coming that week
	2. BRIEF summary of all earnings that came out over the past week ( 2-3 bullet points and key takeaway moving forward per position, can be taken verbatim from earnings summaries)
3. Earnings summary for top 6 positions:
	1. Written as bullet points
	2. Key takeaways on performance
	3. Main things to watch moving forward - what are the big catalysts? 
	4. Publish pdf version in slack day of earnings if released in morning or by the next morning if released in the evening
4. Biweekly positioning review:
	1. Published every other Sunday in PDF form
	2. Up to date graph with sectoral and sub sectoral breakdowns
	3. Quick macro recap, reviewing positioning based on our macro view. 
	4

SLACKBOT here, quick summary of our deliverables and who is responsible: \n\n *1) Main Holding Earning Summaries*: Few bullet points with key numbers, bit of explanation for why they are as they are and few key things to watch moving forward. Everyone should have 2 companies they keep an eye on for this. So far we have:\nMike: Trulieve & NMIH \n Carl: VRRM & (OTHER) \n Joe: (Other), (Other) \n Dan: (Other), (Other) \n Xav? \n \n *2) Weekly Position Performance Report* This will be a single spreadsheet we can split responsibility for over time that will need to be updated regularly with a few numbers. It won’t be hard but we need to be consistent. There’ll be some automated charts here too. \n \n *3)Weekly Report on our Watchlist Companies*: a spreadsheet to keep track of prices over time with some automated charts. This way we can keep track of how $$$ things are and know when to buy \n \n *4) Positioning Review*: A review of portfolio allocation & main macro positioning every two weeks \n \n *MORE TO COME (ALGO TRADING, INTERNATIONAL REVIEWS & MORE)*

<!-- {BearID:2134C1A4-4297-4C27-B5EB-83B4398E58A5-4760-0003ED83BF74EF19} -->
# Rise Notes
[[Deprecated]]

[[071 XCAP MOC]]
# Rise meeting minutes

- Investment Board
	- Sergio $350
	- Guo $1000
	- Sam $400
	- Daniyal $0
	- Josh $300
- Analyst positions
- Nvidia pullback 50%
- Electronic Medical Records
- Portfolio Balancing
- New plays
- Mining/ raw materials
- Xylem
-[[XCAP Meeting Minutes]]
# Rise Meeting


1. Company coverage
	1. Me: TCNNF, NMIH, CURLF
	2. CARL: VRRM, TCEHY
	3. JOE: PYPL, JPM, MA
	4. DAN:  SNAP, FB
	5. XAV: MSFT
	6. SERGIO: 

Watchlist addition:
- mercadolibre


# Deliverables
- Watchlist: serg/ xav
- Weekly Performance Report: Carl
- Bi-weekly review: Mike
- Site: ALL

<!-- {BearID:8F942E7B-9706-48CF-AB5A-21933503D9F7-75570-0004759AF260E988} -->
# Rise summer meeting 1
[[XCAP Meeting Minutes]]
19 June 2019

Minutes 
* Venmo being in tougher competition with Zelle than we appreciated
	* keep an eye on competition
	* Facebook insulates us from this
* Trimming nvidia- selling up to two shares to make room for infrastructure 
* Looking into companies that would benefit from increased infrastructure spending- gonna happen anyway and will be even higher in a recession 
	* us infrastructure sucks 
	* Bipartisan agreement on that
	* Massive spending bill is the only thing congress may do
	* Aecom
	* Look at state money flows
	* Akhil and Xavier will lead search
* Emerging markets having different needs from tech and services
	* scary
	* Silicon Valley not well positioned for the global stage
*

<!-- {BearID:E7E1AE95-5669-4535-8025-6BB5C5A05EF9-1710-0000015F2FB09F4B} -->
# Risecap meeting thots
[[XCAP Meeting Minutes]]
- Maintaining strong hold for paypal
	- offline opportunity 
	- By far the best brand recognition- better than all other processors combined
- Investment theory: the Asian Phone-wielding middle class 
- Proposals:
	- The case for prudential plc
	- Adobe inc
	- Microsoft
	- Single family reit pending arrival of gen y+z

<!-- {BearID:FE592C4B-1345-4FF3-9CD8-A33BBBFCF235-6084-0000036BC2792E88} -->
[[Reading List TOC]]
# Robert Carver - Systematic Trading
#inputs/books 

Notes from the book /Systematic Trading/ by Robert Carver
For my actual attempt at planning see [[076 Algo MOC]]
***Always be ideas first!***



## Premia to collect
	* Liquidity
	* Volatility
	* Sentiment bias
	* Predictable vs unpredictable risk

## Skew
* Skew is a crucial consideration
	* Positive skew = good (lots of small losses, occasional big wins)
	* Negative skew = dangerous, leads to blowups: lots of small gains (usually increased via leverage) occasional huge losses, magnified by leverage
* Consider whether strategy has positive or negative skew! Rarely symmetrical
	* Stocks generally have negative skew
* Consider also combining negative and positive skew strategies!

## Model Types
* Contrarian - mean reversion or relative value
* Technical - based solely on price/ volume data
* Fundamental - based on real world data:
	* Micro: company level
	* Macro: country level
* Trend following
* Volatility targeting - often by combining assets/ rules. Can help decide allocation levels 
* Cognitive bias advantage


## Fitting Models
* Various ways to break up data to fit
	1. Worst - Use all data to fit, all data to test
	2. Ok - Use half of data to fit, other half to test
	3. Better - Expand out of sample, fit with only first year, test on second year, then fit using first two years etc.
	4. Best - Rolling variant of above, using only last X periods to fit
* Consider that you can and *should* combine multiple models! This helps avoid mistakes from overfitting.
* Pooling data may be useful, and consider t-tests for selecting profitable strategies. NEED BIG SAMPLES!
* One way to select rules:
	1. Come up with trading rules to exploit each identified idea
	2. Create several variations of each rule, keeping ones with optimal trading speed and correlations
	3. Assign weights st/ poorer rules have lower weight
* Always consider the marginal benefit of adding a rule - if highly correlated, drop it!


## Backtesting
* Fitting/ overfitting - see above (Do all possible to avoid)
* Honestly avoid selecting rules based on predicted returns from backtest! they don’t hold up!
* Consider the role of randomness!!!


## Portfolio Allocation
* Instrument weights
	* What proportions of each asset type or instrument to trade
* Forecast weights
	* How to weight combinations or variations of rules
* Weighting strategies:
	1. Same SR and correlation (Esp if volatility is standardized): equal weighting works
	2. Significantly different SR: larger weights for higher SR strats
	3. Different correlations: Larger weights for highly diversifying assets which have lower correlation to other assets

## Forecasts
* Forecasts should be weighted by volatility `[conviction] / volatility`
* Forecasts should change continuously based on new data, either due to conviction or volatility changes, independently of what the position is, throughout the life of the trade.
* Scale forecasts so that the expected average forecast is 10 (or for shortable securities [-10 : 10] , should be bounded around +/-20
* If operating multiple rules for an instrument, must *combine* all forecasts into single forecast, still bounded
	* This will necessarily scale down all forecasts, so they must in turn be multiplied by a forecast *Diversification multiplier:*
		* Ex if target volatility is 10%, and combined volatility is 8%, would scale by `10%/8%` 
		* If using this method, recommended to floor corr at 0 to avoid whackiness\

# Strategies
## EPS Growers + small mkt cap

 screening on entire NASDAQ

### Ideas
EPS growth
ROE
LOOK AT POLEN CRITERIA!!!
[[030 People MOC]]
# Robert Rubin
#people 

Person I know. Works at MS at work, ED. Really great guy, one of my [[Mentors]]. 

Favorite quote - "Remove yourself from what is perceived as comfortable, because that's where the magic happens"# Romance
<!-- #_reference/literature -->
[[Liberation of Jerusalem]]
* A twisting, winding story.
* Etymology comes from Romance languages
* The dangers of desire

<!-- {BearID:F812CADD-96AC-489B-9633-30B4302A5735-37181-00039BFEF80F62CD} -->
[[070 Finance MOC]] [[072 MS TOC]]
# Ron Basu Apple Recommendation
Mike is an exceptionally motivated and competent young man who would excel at any technical position he took on. I was his supervisor this summer at Morgan Stanley, and I was particularly impressed with the breadth of his skills and knowledge, both soft skills like working with others and being goal-oriented as well as his strengths technically in coding and finance. 

For the first half of his experience with us this summer he worked for me in a branch, helping us deal with clients and manage their funds. After getting that down, he moved onto our corporate digital team, who design all of our internal software tools, where he not only engaged with his assigned team, but also befriended several senior software engineers and started understanding how our internal systems work. I am most impressed with Mike’s ability to grapple with complex new concepts and ideas that may be very different from his past experiences, and to do so with vigor and excitement. Mike’s background is in coding and technology, but this didn’t stop him from excelling at my firm. When we told him to go home at 5, he would often stay as late as 10 or 11, learning around the topics we worked with and deepening his understanding of highly technical concepts like valuations, corporate structure and risk management.

Finance and technology may seem very different fields, but in fact many of the key skills are highly transferable - Mike is excellent at working in groups, he learns quickly and is never afraid of a challenge, and most importantly he is ready to do whatever it takes to get the job done. His work ethic will definitely take him far in life, and when paired with his general level of curiosity for a broad range of subjects, will make him successful in his future career, regardless of whether that is in finance or technology.

I hope you take this recommendation to employ Mike with the utmost seriousness. 

Regards,

Ron Basu
Managing Director
Private Wealth Management
Morgan Stanley

<!-- {BearID:1A779B2A-BFE5-46A1-ADB4-0DDA82D12CBC-1833-00000269C65AA93A} -->
# SPACs
#projects/old/xcap 
#🌱

[[Aerofarms]]---
aliases: ["Whorfian"]
---
[[Linguistics MOC]]
# Sapir-Whorf Hypothesis
#concepts/linguistics

This hypothesis is one of the existing explanations for the connection between [[Linguistics MOC|Language]] and [[Thought]], and the two main schools of thought in this hypothesis argue:
1. [[Linguistic Determinism]] Argues that without without [[Linguistics MOC|Language]] there can in fact be no thought at all.
2. [[Linguistic Relativity]] Posits that there are significant influences from language on thought.

The main group of theories opposing the Sapir-Whorf view that language influences cognition are termed [[Language of Thought Theory]][[062 Religion TOC]]
# Secularism
[[files/RELI254 Week 5]]
Secularism does not mean no religion in politics, it means ALL religions can be brought into politics - protected to allow equally.

It is a relatively recent idea. Interesting links to the concept of [[Humanism]].[[001 Meta MOC]]
# Seedling Notes
#concepts #🌱 #🌲 

Seedling notes are early stage notes I create that I need to expand before they truly become [[Evergreen Notes]]. This category represents the first stage of my [[003 Process MOC|Process.]]
[[Linguistics MOC]]
# Semantics
#concepts/linguistics 

The study of meaning as it relates to [[Linguistics MOC|Language]].[[Natural Language Processing TOC]] | [[Linguistics MOC]]
# Sentence Segmentation
#concepts/linguistics #concepts/cs/ml/nlp 

- Some sentence segmentation problems are pretty unambiguous, like for example !, ?
- Period "." is ambiguous though: can be a sentence boundary, abbreviation like Dr., or a number like .023%
	- To solve this we build a [[Binary Classification]] that looks at "." and decides whether we're at the end of a sentence or not.
	- [[Decision Trees]] are one of the easier ways of going about this.
		- Can get arbitrarily complex with [[Features]].  
[[]]
# Sergio letter
Dear General Stafford,

Honor to meet you. Please consider my resume. Thank you for considering me, I would appreciate any advice you can provide. I am applying for a final summer internship in ee. 

Cc mom
Confirming that he is honored excited and ready to work. V grateful for any advice or help. And reiterate which jobs you’re interested for summer as well as long term career.

<!-- {BearID:DF984778-529C-4357-BB7F-9FA42E6D6B6C-20883-00000A2658CCCDE1} -->
# Shareholder Meeting
<!-- #finance/xcap/Presentations #finance/xcap/meetings -->
- [[Presentation]]
	- New vision
	- Division of responsibility
	- Past performance
	- Current equity
	- Campus business ideas and cash flow
	- Midd entrepreneurship pitch
	- Sharing ideas
- Discussion of new direction
- Discussion of business ideas
- Discussion of new roles
- Shareholder questions

<!-- {BearID:6819FD21-DCDB-4DD5-9B0C-71E4F9A6B52D-7124-0000160D5EDC391D} -->
[[070 Finance MOC]] | [[075 Quantitative Finance MOC]]
# Short Interest Theory
#concepts/finance 

Short interest theory sort of works in reverse to my prior ideas about short interest. It states that higher short interest is a bullish [[Trading Signal]] because the shorts will need to cover their position. Think here of Gamestop and the retail/ roaring kitty bs# Sigma squared! 
[[ECON411 Applied Econometrics]]
[[Statistics TOC]]
#school

T0: (Bhat0 - H0 B0)/SE(Bhat0)

<!-- {BearID:954632B5-2937-4F51-9A9B-BEAAA5CFFF0D-87251-000573F7AD9D02B5} -->
[[100 Ideas MOC]] [[040 Interests MOC]]
# Sketches
#outputs/skits
- Shock Collar - Office
- N**** News @ nine
- Mike’s Money Corner
- Shock Collar - Family Dinner Table
- News Correspondent
- Shock Collar - Seminar 
- Pub Safe Cam


X M G O S S N

[[010 Mind MOC]] [[095 Journals MOC]]
# Smiling Exercise

Whenever you feel down, use your fingers to make yourself smile 😃
# Social Contract
<!-- #_reference/philosophy -->

* This is a concept from moral and political philosophy that describes the agreement between citizen and state, whereby citizens willfully give up some of their freedoms and submit to a ruler’s authority, receiving protection and social order in return.
* Term itself originates from French: /Du contract social ou Principes du droit politique/ - Jean-Jacques Rousseau, although original roots stem from Greek & Stoic philosophy & Roman and Canon Law.

<!-- {BearID:679EB2E4-3751-4230-BD57-C0F0D75C8CFD-37181-00038D15810EE45B} -->
[[061 Philosophy TOC]] | [[063 Economics TOC]]
# Socialism
#concepts 

	"Socialism may mean, and is often used to describe, merely the ideals of social justice, greater equality, and security, which are the ultimate aims of socialism. But it means also the particular method by which most socialists hope to attain these ends and which many competent people regard as the only methods by which they can be fully and quickly attained. In this sense socialism means the abolition of private enterprise, of private ownership of the means of production, and the creation of a system of “planned economy” in which the entrepreneur working for profit is replaced by a central planning body.”

	“There are many people who call themselves socialists, although they care only about the first, who fervently believe in those ultimate aims of socialism but neither care nor understand how they can be achieved, and who are merely certain that they must be achieved, whatever the cost.”

God damn I agree with both of these so much. It's insane how closely this resonates with my Middlebury experience. I wish I'd studied all this a little more closely earlier in my life so I could have repudiated the ideology more succinctly. 

	“Whether we should wish that more of the good things of this world should go to some racial élite, the Nordic men, or the members of a party or an aristocracy, the methods which we shall have to employ are the same as those which could insure an equalitarian distribution.”
 
 This resonates hardcore with my view that the fight between [[Socialism]] and [[Capitalism]] is really a fight of process vs outcome. Socialism extolls that the best outcome necessitates an unjust process, whereas capitalism flips this and argues that the justness of the process is first and foremost, to be aimed for at the expense of a just outcome. 
 

Exerpts from [[Hayek - The Road To Serfdom]] by [[Friedrich Hayek]][[050 Concepts MOC]] | [[061 Philosophy TOC]] | [[Greek Philosophy]]

# Socrates

Most prominent Athenian moral philosopher (see [[Greek Philosophy]]). Life was mostly documented by his student [[Plato]], he never wrote anything of his own.

Worked almost exclusively through the [[Socratic Method]], where he would start with a question that had a relatively easy answer, and then continue questioning until the subject reached a contradiction. His goal in all of this was not to demonstrate stupidity or his own wisdom (he claimed to have none), but rather to teach others to think independently and "reject the common wisdom." Really quite powerful concepts. Questions are super powerful, and can be dangerous.

He also pioneered a very ironic style. This demonstrates his cleverness and rhetorical skill.

## The Socratic Problem

## Paradoxes
	* No one desires evil
	* No one errs or does wrong willingly or knowingly
	* Virtue - all virtue - is knowledge
	* Virtue is sufficient for happiness
	
	
## [[Quotes]]
	"An unexamined life is not worth living"
	
	
## Related People
[[Plato]]
[[Pericles]]
[[Aristotle]]


* *Erotan* - the art of love, which Socrates claimed to have knowledge of, manifested as “erotan,” or asking questions.
* Paradox of wisdom - those who know then are not wise must be more wise than those who think they are, for at least they are aware of their own ignorance. First happened upon this thought when the oracle of Delphi told his friend Anaximander that Socrates was the most wise in all the land. Socrates concluded that the only advantage he had over the other people who could be wise is that he recognized his own ignorance.
* Socrates claims he himself is “barren of theories” (*Theaetetus*) however he helps others give birth to them. He compared himself to a midwife rather than a teacher. This was probably inspired by his mother, who may have been a midwife.
* Socratic Method (questioning) - After having the oracle of Delphi tell him he was the wisest in the land, he went around questioning prominent other wise people to determine if the oracle was really correct. (Eh don't agree w/ this so much anymore, but I'll allow it for now)
* Socrates was tried and killed for “corrupting the minds of the young of Athens (allowing people to question what was good, what was right, and whether things were improving or worsening).
	* Some suggest he perhaps killed himself, or at least did not fight or try to flee death. Some explanations have been suggested, including not showing fear of death, not leaving Athens where his teachings were relevant, not endangering his friends and breaking the [[Social Contract]] with the state.

* Opposition to democracy - never proven but often suggested and hotly debated. In especially some Platonic depictions, Socrates appears to reject the Athenian democratic system (which was granted not functioning perfectly during his adult years) in favor of a system run only by the wise - philosophers. He did also say that no humans can truly be wise, and apparently did not think himself wise either. It has been claimed that much of the un-democratic leanings in writings like /Republic/ come from [[Plato]] would have been disgusted at the needless killing of his teacher.
	* Paul Johnson supports this in [[Johnson - Socrates, A Man For Our Times]], where he says that while Socrates critiqued the state heavily, he loved Athens and felt a huge duty to the state. He also probably thought democracy was better than surrounding alternatives.


# Special Characters
#concepts/reference 
[[Todo]] #todo 
* Delta: Δ
# Standard Model TOC
[[000 Life MOC]] -> [[Quantum Mechanics TOC]]
#TOC
#concepts

#🌱 

[[Atoms]] are core units of matter, made up of [[Electrons]], [[Neutrons]] and [[Protons]].

## Photon
[[Photons]]
Functions as a wave and particle
* *Polarizer* - only allows photons polarized in same orientation through - say vertically
	* Stacking: allows light if 3 off axis! (with two perpendicular)
	* If you send a right diagonally polarized photon through a vertical filter, prob 1/2 a vertical photon comes out prob 1/2 nothing comes out. This is a key aspect of quantum! Taking a [[Quantum Measurement]]!

All particles with mass attract each other via [[Gravity]]


[[051 Math TOC]] 
# Statistics TOC
#concepts/cs/ml #TOC

This shall be my central reference document for statistics.
Statistics are often in reality built around [[Linear Algebra TOC]]
## Regression
See [[Linear Regression]]
[[Random Effects Model]]
[[Fixed Effects Model]]
[[Panel Data]]

## Significance
Significance is a key issue when thinking about statistics. Determining whether an outcome is significant enough to believe depends on context, but there are tools to help:
* [[Granger Causality Test]]
* [[Hausman Test]]
* 
[[051 Math TOC]]
# Stats Simultaneous Equations
#concepts/math 
[[ECON411 HW6]]
[[Statistics TOC]] [[Linear Regression]]

[[095 Journals MOC]]
# Status
#outputs/journal 

My whole life I have been afraid to speak frankly with people.  I never stood up for my rights out of respect. I just want people to know that unless they respect me, I will no longer give out my respect and love blindly. From now on, I will be much more careful about these things. To anyone that wronged me, you probably don’t even realize what you have lost. I don’t harbor ill will for anyone, but this is the end of my wasted respect.
[[030 People MOC]]
# Steven Pinker
#people 
#concepts/linguistics 

Acclaimed Harvard [[Linguistics MOC|Linguistics]] Prof. I watched an interesting lecture of his [^https://www.youtube.com/watch?v=Q-B_ONJIEcE] here, which significantly influenced my thinking on the topic generally and has acted as a good starting off point for thinking about the topic more generally. I'm trying to relate this a bit more closely now to my own study of [[054 Neuroscience MOC|Neuroscience]], and in some ways to my research on [[Natural Language Processing TOC|NLP]].[[Machine Learning TOC|Machine Learning]]
# Stochastic Gradient Descent
#concepts/cs/ml #🌱

Can be computed using multiplication of [[Matrices]], very similar to a [[Linear Regression]]. In fact, [[Logistic Regressions]] are a good way of computing the boundary.

Uses a bit of randomness to converge faster to the optimal point. # Style Guide
#### HEAD
This document lays out how to structure new notes

#### REF
[[000 Life MOC]]
#todo
#### PLAN
	- Outline idea
	- 

## Colors
* **Light text: **\#F1F1F1
*this is the italic way*
**This is bold**


# Title
THen some text
## Heading
Then some text
### Subheading 1
Then some text
#### Subheading 2
Then some text
##### Subheading 3
Then some text
###### Subheading 4
Then some text
[[005 Active MOC]] [[080 Personal Finance MOC]] 

# Subscriptions



| Item             | Monthly Cost |
| ---------------- | ------------:|
| Apple Music      |           $8 |
| Google Drive     |           $5 |
| ICloud Storage   |          $20 |
|                  |              |
| **Tech**         |              |
| Alphavantage     |         $100 |
| Polygon          |         $200 |
| Bear             |            ? |
| Obsidian Sync    |           $4 | 
| Obsidian Publish |           $8 |
|                  |              |
| **Total**        |         $333 |



[[Cognitive Biases TOC]]
# Substitution Bias
#concepts/cognitive_biases 

This bias occurs when people tend to substitute easier questions when seeking answers for harder questions. There are elements of [[Cognitive Switching Penalty]] here, as well as [[Inherent Laziness Instinct]].[[PHYS106 Physics for Educated Citizens]]
# PHYS106 PS6b - Superconductivity

Essay question #3: Discuss superconductivity.  What is its value, and what are its limitations? How might it prove important in the future?

Superconductivity is a fascinating physical phenomenon that has the potential to change the way our electronics work on a fundamental level. Superconducting materials are those with near-zero resistance to electric currents. In practice, this means that nearly no electrons are stopped as the current flows through the material. This creates way less heat and allows for much more efficient long-distance circuits. So far, humans have succeeded in making superconducting materials work at near-absolute 0, and "high temperature superconductors" have been successfully used at temperatures of 150K.(Muller 2008, pg 206) What is so special about superconductors, and what would it change if we created true room-temperature superconductors?

What makes superconductors so alluring? All of the benefits of superconductors stem from their ability to conduct electric currents with nearly no resistance. This allows applications like vastly more efficient power transport, over 99% efficiency electricity generation (Eck 2020) and the potential for computers orders of magnitude faster than current classical computers. Levitating trains use superconducting electromagnets to induce levitation. This allows them to accelerate to very high speed with little excess heat creation through less energy being lost to friction. Because superconducting materials do not have much resistance, they create little waste heat, allowing all kinds of electronics to function far more quickly and efficiently. 

Although named "high-temperature", even these superconductors require robust cooling systems to operate. Much excitement has been generated in recent years by advances that claim to create stable above-room-temperature superconductors. Unfortunately, some downside persists in all cases: the recent New York discovery (Fenster 2020) for example requires pressures equivalent to 75% of the pressure at the earth's core. If we are able to discover a true room-temperature superconductor that does not require high pressure or careful state control the impact on technology will be revolutionary. 


## Works Cited
Muller, R. (2008). _Physics for future presidents_.

_Physicists Discover First Room-Temperature Superconductor | Quanta Magazine_. (n.d.). Retrieved April 2, 2021, from [https://www.quantamagazine.org/physicists-discover-first-room-temperature-superconductor-20201014/](https://www.quantamagazine.org/physicists-discover-first-room-temperature-superconductor-20201014/)

Eck, Joe. _Superconductor Uses_. Retrieved April 2, 2021, from [http://www.superconductors.org/Uses.htm](http://www.superconductors.org/Uses.htm)


















I have neither given nor received unauthorized aid on this exam
Michael Calvey# Superdeterminism
#concepts
[[Interpretations of Quantum Mechanics]]  [[Quantum Mechanics TOC]]
## Summary
This is one of the potential interpretations of quantum mechanics that helps fix Bell’s paradox. The core idea is that the entire universe is fundamentally deterministic down to the quantum level, and that in fact all of the past and all of the future are theoretically already known. Bell wrote:

	* /There is a way to escape the inference of  superluminal speeds and spooky action at a distance. But it involves absolute  determinism  in the universe, the complete absence of  free will. Suppose the world is super-deterministic, with not just inanimate nature running on behind-the-scenes clockwork, but with our behavior, including our belief that we are free to choose to do one experiment rather than another, absolutely predetermined, including the 'decision' by the experimenter to carry out one set of measurements rather than another, the difficulty disappears. There is no need for a faster-than-light signal to tell particle A what measurement has been carried out on particle B, because the universe, including particle A, already 'knows' what that measurement, and its outcome, will be./

The implications of this are obviously quite enormous, and speak volumes to our potential to understand more about this universe.

<!-- {BearID:EB0E6971-3285-4B1F-AF83-FC0E7893F9AA-37181-0002BEEA9FF7F2A1} -->
[[Quantum Mechanics TOC]]
# Superposition
#concepts

You could say a particle is “in a superposition of standard basis states”, roughly equating to a linear combination. Superposition is purely relative to the basis states with which a quantum system is measured. All quantum systems are superpositions with respect to some states, but not to others.

This allows algorithms to explore multiple paths at once- think about the power of [[Deutsch's Algorithm]]!

A basic interpretation of superposition states is that [[Qubits]] behave as though they are in multiple states "at the same time". This is a simplistic analogy when you think of the fact that this is only with respect to certain bases of measurement. ^[Rieffel - A gentle introduction to quantum computing, CH3] This may be more an inability of our mathematics to truly encapsulate [[Quantum States]] than anything else.

This applies equally to [[Multi Qubit Systems]].

All of this stuff starts to get pretty weird when [[Quantum Phases]] get involved, jeez.[[Cognitive Biases TOC]]
# Survivorship Bias
#concepts/cognitive_biases #🌲 

Survivorship bias is a very important concept whereby people do not appropriately appreciate the causes of outcomes. Specifically, when examining a set of abstract "winners," we do not know about the losers who acted in the same ways yet failed to reach the same outcome, thus skewing our perception of what actions result in which outcomes. This is an example of a [[Selection Bias]] that emerges due to the lack of visibility among failures.

There's a link to be made here with the [[Long Tail]], where we can attribute a majority of extreme outcomes to long tail events, yet do not adequately treat them as such. 

An example of this is hedge fund managers, where even pretty good track records can randomly emerge.

A major result of this bias is closely linked to overoptimism (Not the same as although tangentially related to [[Overoptimism Bias]], which concerns ignorance of base rates)[[053 Computer Science TOC]]
# Symbolic AI
#🌱 #concepts

[[Decision Machines]] [[AI]]

Sort of like what I've been thinking about for my decision engines, but low key missing a few core components.

Look at https://www.cyc.com/resources/white-papers

Very interesting concept

Often called *Expert System*

Built on a semantic or knowledge type graph

[[Decision Machines]][[Linguistics MOC]]
# Syntax
#concepts/linguistics 

Syntax doesn't consist of linear, [[Words]] by word [[Associative Machine|Associations]].
`Colorless green ideas sleep furiously` - sounds like well formed english, yet very little linear probability.

## Structure of a sentence
We can break down the structure of a sentence using [[Phrase Structure Parse]], in fact this is argued to be how we understand phrases. The two key components and their expantions are:
1. *Noun phrase* - subject
2. *Verb phrase* - predicate
	1. Verb
	2. Noun Phrase (object)
	3. Sentence (complement)

E.g: (I) told him (it was sunny outside)

This geometry is critical in understanding who did what to whom.

These phrase structure rules help us understand open ended creativity in language as well as the expression and understanding of unfamiliar sentences.

We still have [[Ambiguous Sentences]]

[[011 Mental Models MOC]] | [[Systems 1 and 2]]
# System 1
[[Kahneman - Thinking Fast and Slow]][[011 Mental Models MOC]] | [[Systems 1 and 2]]
# System 2
[[Kahneman - Thinking Fast and Slow]][[011 Mental Models MOC]]
# Systems 1 and 2
#concepts #🌲 

[[System 1]]
[[System 2]]
---
aliases: ["Systems"]
---
[[011 Mental Models MOC]] | [[051 Math TOC]]
# Systems Theory TOC
* [[Decentralization]]
* The idea of autopoiesis - A system capable of maintaining and reproducing itself [[Decision Machines]]
* Definitely need to learn/think a lot more about this, might be a good starting point on wiki: [Systems theory - Wikipedia](https://en.wikipedia.org/wiki/Systems_theory)
# TCNNF
#inputs/companies
*Trulieve is a vertically integrated cannabis retailer based in Fl, USA. Trulieve has the most geographically concentrated footprint of all the big US MSOs, and is the only big player that has managed to maintain profitability for the past several years.*

Trulieve! I will slowly start aggregating all the work/ thinking I do on TCNNF here.

I’ve also thought about this from the perspective of [[Quantitative Finance]]

* [[XCAP 3Q20 TCNNF]]

---
aliases: ["TOC", "TOCs"]
---
[[001 Meta MOC]]
# Table of Content
#concepts #concepts/definition

## TOC Definition
TOC, designed to connect [[Concept Notes]] to each other and [[Map of Content|MOCs]]. I think of TOCs as sort of *constellations*. These are little clusters of knowledge and wisdom, all connected by some underlying theme, project or discipline.[[052 Physics TOC]]
# Take a physics course!
We need to find a good course to learn basic physics. 

Will ask serg and omie

Thinking:
* maxwell 
* Matrix physics

<!-- {BearID:824F25A5-322D-4FAD-BC36-821E54044BC3-499-0000002B4A83E08A} -->
[[070 Finance MOC]]
# Tancap
[[000 Life MOC|Life]] | [[010 Me MOC|Me]]
# Technology
#🌲 

I use technology in my [[000 Life MOC|Life]] to assist me in accomplishing my [[015 Goals MOC|Goals]] thanks to my awareness of the [[001 Meta MOC|Meta]]. Technology auments [[010 Me MOC|Me]], and is thus an intrinsic extension of [[010 Me MOC|Me]].

This note is undergoing an overhaul. This is now my touchpoint for everything to do with technology, including my personal interactions with it. 

It should also be a launching point for my broader [[061 Philosophy TOC|Philosophy]], since I really believe tech is an absolute gamechanger for humanity. Part of the power of tech is that it works as an enabler for continuously more advanced applications and ideas. It empowers people to be smarter and more in control of themselves and their actions.
- [[Systems Theory TOC]]
- [[Emergence]] and [[Decentralization]]

There is also a huge, deep dark side to tech. I need to be brutally aware of this, because it is one of the biggest dangers facing people. When applied to warfare, espionage or government in unscrupulous ways, tech results in [[Totalitarianism]], a la 1984

# My Technology
- Phone
- Laptop
- Watch
- Rig





[[Aerospace]]

# Actual Technology
* Internet - this is obviously the biggest one
* SOCs - custom silicon? Paving a revolution, hyper specialized chips. What can huge increases in efficiency allow?
	* Can’t manufacture them in the US?! might be a huge strategic error
* Augmented Reality - hitting by 2025
* Cryptocurrencies - Need mainstream adoption/ usability improvements
* Vertical farming - huge advances still to be made
	* Main things to think about are dropping costs of electricity, increasing costs of water, AI nutrient distribution, ideal lighting, direct-use geothermal heating
* Housing construction tech - how to automate and improve? We are currently living in a 1950s paradigm
*
# Tencent Post-Mortem
<!-- #finance/xcap/Post-Mortem -->

<!-- {BearID:7B2C4F63-DC5B-4339-B850-DDC588CEEE61-406-0000023B3553CFC0} -->
[[051 Math TOC]] | [[052 Physics TOC]] | [[Linear Algebra TOC]] | [[Quantum Mechanics TOC]]
# Tensor Products of [[Vector Spaces]]
#concepts 
The tensor product is an operation on two or more [[Complex Numbers|Complex]] vectors that describes their overall state space. Can be visualized as:

![[Untitled Notebook (2)-1.jpg|250]]

Tensor products are used to describe systems consisting of multiple subsystems. Each subsystem is described by a vector in a complex [[Vector Spaces|Vector Space]], which in quantum would be called a Hilbert Space.

If we have two systems $I$ and $II$, vectors $\ket{\psi}_I$ and $\ket{\psi}_{II}$ describe the states of systems $I$ and $II$, with the state of the total system given by $\ket{\psi}_{I}\otimes\ket{\psi}_{II}$

#### Dimensions
If the dimensions of system $I$ are $m\times n$ and the dimensions of system $II$ are $p\times q$, the dimensions of any tensor product of the two systems will be $mp\times nq$

### Matrix Example
$$
\begin{pmatrix}1 & 2 & 3 \\ -1 & -2 & -3 \end{pmatrix} \otimes
\begin{pmatrix}0 \\ 1 \\ i \end{pmatrix}
=\begin{pmatrix}1\begin{pmatrix}0\\1\\i\end{pmatrix} & 2\begin{pmatrix}0\\1\\i\end{pmatrix} & 3\begin{pmatrix}0\\1\\i\end{pmatrix}\\
-1\begin{pmatrix}0\\1\\i\end{pmatrix} & -2\begin{pmatrix}0\\1\\i\end{pmatrix} & -3\begin{pmatrix}0\\1\\i\end{pmatrix}
\end{pmatrix}
=\begin{pmatrix}0&0&0\\1&2&3\\1i&2i&3i\\0&0&0\\-1&-2&-3\\-1i&-2i&-3i\end{pmatrix}
$$
In this case we have dimensions: $2\times3\otimes3\times1=6\times3$

## Application to Entanglement
Most elements $\ket{w}\in V\otimes W$ **cannot** be written as the tensor product of some $v\in V$ and $w\in W$, though they are all linear combos of such elements. This is of crucial importance to [[Quantum Computing TOC|Quantum Computing]].

States of $V\otimes W$ that cannot be written as the tensor product of a vector in V and a vector in W are called [[Entanglement]].

For more on this topic see [[Multi Qubit Systems]]

## *n*-Qubit Systems
The state space of *n*-Qubit systems with states represented by unit vectors $V$ and $W$ can be represented by the unit vectors in the space $V\otimes W$. This would take forever to write, so we have compact notation to represent this:

![[Quantum States#Multi-Qubit States]][[010 Mind MOC]] | [[011 Mental Models MOC]]
# The Map is not the Territory
#concepts #concepts/mental_models #🌱 

This mental model is also an example of [[Heuristics and Biases]]. The idea states that people remember places, concepts and relationships as generalized, abstracted versions of the ideas' true selves, creating a dissonance between expectation and reality that can cause problems. Even the best map is imperfect as it is just a reduction of what it represents. Time also constantly erodes the "truth" of maps.

Think of an explorer entering a new area with a purchased map. No matter the detail of the map, it is not perfectly able to encapsulate the true nature of the area. When the explorer is searching for something, she will by definition make errors of judgement if she only uses the map to make predictions.

Interesting thoughts on this at [this blog post](https://fs.blog/2015/11/map-and-territory/)

# The dude abides
[[Deprecated]]
[[040 Interests MOC]]
# The equine indifference society
#outputs/skits 

At the entrance of a society.
On attempted entry, a bouncer stops you to ask a question. 

First respondent starts to answer thoughtfully, rejected before he even starts. Gets upset. Bouncer becomes aggressive. Heartily tells man to leave. As he is leaving bouncer says “see you tomorrow carl”. On exit man turns around and says “like i told you yesterday, my name is jeffrey!”

Second respondent begins to answer aggressively, immediately accepted.

<!-- {BearID:9E496F8F-178A-4716-BA3B-CDC01F7F6902-13748-00000BD6C1C2B45D} -->
[[Sketches]]
# The perfect crime

At the scene of a crime with a reporter. 
He reports about the audacity of a crime, without ever saying what happened. 
All the perpetrators stand behind him 
Clearly a heinous crime
Abhorrent, in fact

Is whatever comes to mind the answer? The story could even be unique- each person individually comes to their own conclusions about what occurred, while noone actually knows!

[[010 Mind MOC]] | [[011 Mental Models MOC|Mental Models]]
# Thought
#concepts/philosophy #concepts/psychology # Thoughts on note taking
[[010 Mind MOC]] | [[050 Concepts MOC]] | [[095 Journals MOC]]
[[Todo]]
#outputs/thoughts 

My notes are like [[My Forest]]. They must be tended to and traversed regularly for the concept to be effective.

By using  [Evergreen notes should be concept-oriented](https://notes.andymatuschak.org/z6bci25mVUBNFdVWSrQNKr6u7AZ1jFzfTVbMF) , you can build an organizational structure which reflects the future contexts in which you’d likely want to see the ideas you’ve distilled.

* More ideas in [[Zettelkasten]] [Niklas Luhmann - Wikipedia](https://en.wikipedia.org/wiki/Niklas_Luhmann)

- The way people choose their keywords shows clearly if they think like an archivist or a writer. Do they wonder where to store a note or how to retrieve it? The archivist asks: Which keyword is the most fitting? A writer asks: In which circumstances will I want to stumble upon this note, even if I forget about it? It is a crucial difference.
-	Andy Matuschak~!!!! We're back to this guy!!
[Taxonomy of Notes](https://notes.andymatuschak.org/Taxonomy_of_note_types)

* **Note Types:** I'm now coming up with a bunch of new discrete classifications for a note, that defines what it should try to tend towards. The core idea is centered around the gradual development of [[Evergreen Notes]], to which I contribute constantly and gradually over time.
* [[2021-03-23]] This is where I develop these kinds of ideas a bit more
[[Daily Notes]] -> Great places to put stuff that has no other home.
[[Map of Content|MOCs]] -> Highest level of organizational meta-category, these files act as "galaxy" connecting "constellations" - the TOCs, which actually link you to the "planets" or [[Evergreen Notes]] themselves. 
[[Table of Content|TOC]] -> 
[[Evergreen Notes]]
I don't have a super# Time Series Data
#concepts #concepts/math/stats 
[[ECON411 HW8 Notes]]
[[Statistics TOC]]
[[Linear Regression]]

## Level Space vs First Difference space
* Level Space: Yt, Yt-1 -> directly comparing total level
* First Difference Space: Yt - Yt-1 -> looking at diffs vs vals

Get weird effects in level space when data is not stationary!

## Stationary Data
A stationary time series’ observations do not depend on the date of observation - there is no trend (for example seasonality or overall trend). Really this means that older values don’t determine newer values, looking for something like a zig zag vs trend line.
Non-stationary data suffers from /unit roots/
Stationary data has an Integral of 0
* I(0) - stationary data (0 order integrated)
* I(1) - stationary in the first difference space (first order integrated)
* I(2) - stationary in the second difference space (second order integrated)

### Detecting Stationary Data:

#### Informal
Will see “systematic” level lag correlations, starting v high and getting smaller as the lag increases.
Want to see no memory between different absolute levels of observations in a series!

If your R-squared is larger than DW-D [[Durbin Watson Statistic]], issue might be more severe than autocorrelation - a spurious regression

#### DF Test
See [[Dickey Fuller Test]] for formal derivation

Augmented Dickey-Fuller test
Read textbook Eq 13.5 - 13.7 -> DF test (Dickey-Fuller Test), can be expanded to ADF (Augmented Dickey-Fuller test)
Also section 14.2 -> spurious regressions

`Yt = Yt-1 + Et`
Can backsub! `Y2 = Y0 + E2 + E1`
`Yt = [Y0 + E1 + E2...]`

### Correcting for non-stationary data:
Can use the first difference space!
`deltaYt = (Yt - Yt-1)`

# Cointegration
Cointegration tells us that even if we’re dealing with I(1) series, we might still be able to get away with running the regression in the level space. We essentially test to see whether the residuals are stationary! If so, series are cointegrated, but we might still be able to run the regression!

*Angel-Granger test* can be used for this!

`H0: No cointegration`
`H1: Have cointegration`

See table 14.6 in the textbook!

<!-- {BearID:7755821C-0CA2-4455-AC01-747D145DC7A6-338-000004C65BD1E208} -->
[[000 Reality MOC]]
# Time

#🌱 

# What?

[[Future]]

# Why?
[[Quantum Mechanics TOC|Quantum Mechanics]]

# How?[[Linguistics MOC]] | [[Natural Language Processing TOC|NLP]]
# Token Normalization
#concepts/cs/ml/nlp 

This is the next step in the processing of [[Words]] with the end goal of conducting NLP activities. Normalization is the process of equating strings like `U.S.A.` to `USA`, and ensuring a consistent scheme for accurate processing.

## Case Folding
For information retrieval, we can reduce all letters to lower case.

For sentiment analysis, we often want to keep case. Things like `Fed` and `fed` are different!

## [[Lemma]]tization
Reduce inflections or variant forms to base form, eg:
- `am, are, is` -> `be`
- Have to find correct dictionary headword form.

The next step of the process is digging into [[Morphology]].[[ECON431 Economics of the EU]]
# Topic Proposal
#school 
Michael Calvey


For my research project I would like to explore the impacts of converging financial and monetary institutions on FDI across Europe. I’m pretty interested in figuring out what it takes to create a good business environment that fosters investment internationally - last year I wrote a paper on FDI in Russia after the fall of the Soviet Union, looking specifically on the impact of sanctions on agricultural output prices in the country, and I found the topic immensely interesting. I think I need a little more meat/ an actual direction to take the paper in, however. The way it would be now would be pretty broad and difficult to draw any reasonable conclusion on.

I would like to incorporate some element of econometric analysis into my paper. I pride myself on my practical statistical ability, and like the challenge of figuring out how to test ideas. The core idea I would want to test in this case would be to compare FDI before and after countries joined the EMU, for example. 

As a backup topic (in case you think my main FDI idea doesn’t hold up/ is not that relevant to this class), I’ve noticed that I’ve been quite interested in deciphering the impacts of restrictions on monetary policy on governments’ abilities . I think a lot of the questions I would like to approach haven’t been answered even in the literature on this, with the advent of modern monetary theory and such, but I would still love to find a way to approach the issue. I have several angles in mind with this, one being the thesis that “countries with restricted monetary policy are not able to weather economic shocks as well as those with the ability to nationally determine (and carry out) monetary policy.”

<!-- {BearID:DDDB2DBA-976B-4A6C-AFA7-285772DABBFC-1286-00006210950D61D5} -->
[[053 Computer Science TOC]]
# Turing Machines
#concepts/cs #TOC

Turing machines are the most powerful general models of computation described in the [[Chomsky Hierarchy TOC]], and they generally capture what is meant by “computable”. If a decision function is computable by a computer, it is also computable by a TM. 

## Intuition
We can think of TMs as pushdown automata that have access to an infinite tape, which is really an abstraction for RAM, allowing infinite read/ write. 

The point of a TM is to accept, reject or loop on some input x.

A TM that halts on all inputs is called a total TM.


## Formal Definition
The set of all strings x that a TM M accepts is called L(M)

TMs can be completely described in a 9-tuple:

`(Q, Sigma, Gamma, |-, _, delta, s, t, r)` where:
		* Q: the set of possible states
		* Sigma: Input alphabet
		* Gamma: Tape alphabet
		* |-: Left endmarker
		* _: Blank symbol
		* delta: Transition function
		* s: Start state
		* t: Accept state
		* r: Reject state
 
	* We say set A is *Recursively Enumerable* if A = L(M) for some TM M
		[[Recursively Enumerable Sets]]
	* We say set A is *Recursive* if A = L(M) for some total TM M
		[[Recursive Sets]][[Reference Homepage]]

*Transitions:* a transition such as:
`delta(p, a) = (q, b, d)` means “when in state p calling symbol a, transition to state q, write a b on the initial tape cell and move the head in direction d”

*Configurations:* a TM configuration `(p, z, n)` specifies a current state p in Q, current tape contents z in Gamma*, and current position of the read/ write head n >= 0.

TM M *accepts* input x in Sigma* if it ever enters its accept state t on input x.

TM M *rejects* input x in Sigma* if it ever enters its reject state r on input x.

/Note:/ one a TM enters an accept or reject state, it may never leave it. Furthermore, if M never enters its accept state on input x, it is said to *loop* or *diverge* on that input. M will then run infinitely long on that input. A TM only *halts* if it enters state t or r. If a TM halts on every input, it is a *total TM*

## Examples
See [[Chomsky Hierarchy TOC]] for explicit examples of TMs

## TM Variants
Variants generally add convenience by creating additional abstractions that enhance how we can think of solving problems. Interestingly, all variants are still just as powerful as a regular TM, and can be simulated by regular TMs.

	* Multi tape machine
	* Two-way infinite tape
	* A finite-state machine with two stacks is equivalent to a single infinite tape TM

### Enumeration Machines
[[Enumeration Machines]] are another interesting abstraction of TMs. These machines generate the [[Recursively Enumerable Sets]]. This is actually the original type of computational machine proposed by Alan Turing, when he used enumeration machines to prove that the [Entscheidungsproblem - Wikipedia](https://en.wikipedia.org/wiki/Entscheidungsproblem) was impossible. This problem posited whether it was possible to determine whether mechanical symbol manipulation could be used to determine the truth of mathematical statements.

The EM is a generator of strings, so has no input. We can think of this as similar to the way in which [[Regular Expressions]] generates strings.  As such, there is no input alphabet, although there is still an output alphabet.

EMs have two tapes, both starting blank:
	1. Work tape: read/ write, used during computation
	2. Output tape: write only, used to store outputs while in progress.

EMs have no start, accept, reject states, but rather a single enumerate state.

*Turing machines = Enumeration Machines*
Given EM E, construct equivalent TM M
What M does on input x:
	1. Simulate EM E
	2. Whenever E enumerates a string, compare it to x
	3. If the string matches x, x accepted
	4. Go to step 1, repeating indefinitely

Can also go the other way:
Given TM M, construct equivalent EM E
What E does: run M on all strings in Sigma* using *timesharing* - this allows us to do a number of simulations in parallel and not getting stuck on semi-decidable problems that do not accept (loop forever)
	1. Initialize i = 1
	2. Simulate M for i steps on all strings x where |x| < i
	3. If M accepts a string x, enumerate x
	4. Increment i and go to step 2

### Universal Turing Machine
This is the basis for our study of [[Decidability TOC]]

A TM U that takes another TM M encoded as a bit string input. The universal TM then reconstructs TM M and simulates it on x, such that TM U will:
	* Accept M#x if M accepts x,
	* Reject M#x if M rejects x,
	* Loop on M#x if M loops on x

We formally define Universal TM U as:
`L(U) = {M#x | M accepts x}` = MP, Membership Problem
`L(U) = {M#x | M halts on x}` = HP, Halting Problem

Using [[Reductions]], we can show that other properties are also undecidable.
Both HP and MP are [[Recursively Enumerable Sets]] but are not [[Recursive Sets]]


## TMs and lower members of the Chomsky hierarchy
We can show via reduction that there does not exist a total UTM that determines whether the language L(M) accepted by a TM M is either:
	* [[Regular Expressions]]
	* [[Context-Free Language]]
	* [[Recursive Sets]]

## Time bounded computation
When we bound resources (limit the length of the tape) the set of problems we solve becomes constricted. Here, deterministic and non-deterministic TMs can differ. We can think of problems as being solvable in a length of time or space that is a function of the input length.

### Definitions
See [[Complexity Classes]]
	* DTIME(T(n)) = problems solvable by a Deterministic TM (DTM) in time T(n)
	* NTIME(T(n)) = problems solvable by a Nondeterministic TM (NTM) in time T(n)

	* DSPACE(S(n)) = problems solvable by DTM in space S(n)
	* NSPACE(S(n)) = problems solvable by NTM in space S(n)
and thus:
	* P = Union(DTIME(n^i)) = {problems solvable by a DTM in O(|x|^k) time} [[Deterministic Polynomial Time]]
	* NP = Union(DTIME(n^i)) = {problems solvable by a NTM in O(|x|^k) time} [[Nondeterministic Polynomial Time]]

The classic question is whether P = NP? There is no current proof that these sets are not equivalent. We likely need /new/ proof techniques if we can resolve these questions at all.

### Nondeterministic TM
	* Has an extra oracle/ guess tape, which indicates whether a series of strings (determined in a canonical, methodical way) are accepted by a TM.
	* Does not add any computational power

<!-- {BearID:148A123B-B831-4683-A9B7-B902DDE5742B-5857-00012BA642B359C9} -->
[[Machine Learning TOC]]
# Type 1 errors
#concepts/cs/ml 

Type 1 errors are **false positives**, in other words where we match things that shouldn't be matched. These are the inverse of [[Type 2 Errors]].
This is **accuracy** or **precision**.

These are used in [[Machine Learning TOC|ML]] and [[Statistics TOC]], with applications in [[Natural Language Processing TOC|NLP]] and [[Regular Expressions]].
[[Machine Learning TOC]]
# Type 2 Errors
#concepts/cs/ml 

Type 2 errors are **false negative** errors, where we should have matched something but don't. The opposite error type of [[Type 1 Errors]].
The rate of success in this is **coverage** or **recall**.

These are used in [[Machine Learning TOC|ML]] and [[Statistics TOC]], with applications in [[Natural Language Processing TOC|NLP]] and [[Regular Expressions]].# UK News
#school
[[ECON431 Economics of the EU]]

1. Major covid resurgence
	1. Nearing being as bad as things were before
	2. High schools (secondary) have begun closing down, approx 5% closed in last 2 weeks
	3. Bojo unveiled new 3-tier restriction plan for regional lockdowns within UK
2.

<!-- {BearID:B228E199-1309-4732-BC78-C335E061C795-37181-000217EF32563843} -->
[[095 Journals MOC]]
# Unconference Notes


Connect with Michael Azlen!
# Unknown date Meeting Minutes
- Losing equity majority 
	- Losing board seats?
	- Cutoff for establishing equity majority a week before next fiscal half year
- Dates for quarters and Fiscal year
- Proposed quarters:
	- January-March
	- April-June
	- July-September
	- October-December
- Looking closer at medtech
- Bangladesh
- Nigeria
- Costa Rica
- Other emerging markets
- Rising water levels
	- Levee materials?
- Sustainable
- Coffee?????
	- Dunkin
	-

<!-- {BearID:C33B06CB-C82D-4A49-BCC7-40088892CCD3-312-0000FFAC261BDC7D} -->
[[Machine Learning TOC]] | [[ML Models TOC]]
# Unsupervised Learning
#concepts/cs/ml

Unsupervised learning happens when you don't provide [[Labels]] for [[Features]], but rather allow the model to try to extract meaning. [[Neural Networks]] can be set up to do this.

## Word Embeddings
Word embeddings attempt to predict whether two words are closely related, and rank them. [[Bert Models]] wrecked this.
Really these are Singular Value Decompositions. 

## Projection
Lower the number of dimensions (remove features?!) Can help with 2d/ 3d visualization
[[Dimensionality Reduction]]

## Clustering
Find logical groups in ___ space


## Further Reading
[[Visual Cortex]]
[[Photon Project]]
# Photon Person
#projects/photon 

Richard, 26
Richard is successful and popular. He’s been living in the city for 5 years and loves the city life, but he still likes to spend time outdoors doing things like camping and going to beach parties with his friends. Richard uses his lumens at home for day-to-day lighting, but he also takes them with him for his adventures - last weekend, he threw a nighttime barbecue at his friends house in the country, and brought a lumen for each of his friends. He used them to set the scene throughout the night, starting with an exciting color scheme that reacted to their music in the early hours of the evening, followed by deep purple and red mood lighting as the night wore on. The lumen was so effective at setting the ambiance, that all of his friends want them too now.

<!-- {BearID:B9B12401-3A17-483F-B581-F3BC9B017042-11122-000020756A052E93} -->
[[011 Mental Models MOC|Mental Models]] | [[063 Economics TOC]]
# Utility Theory
#concepts #🌱 

Classic Microeconomics. Assumes people are rational agents who make decisions based on their utility for each decision. Slightly debunked/ expanded by [[Prospect Theory]].# VRBO Fight
[[Deprecated]]

# VT Tax Payment
[[080 Personal Finance MOC]]
[[Deprecated]]

[[051 Math TOC]] | [[Linear Algebra TOC]]
# Vector Spaces
A vector space $\mathbb{C}^d$ is the set of all column vectors with dimension $d$ whose elements are complex numbers.

See [[Linear Algebra TOC]]

## Conjugate Transpose
If $x\in\mathbb{C}^d$, then the conjugate transpose, denoted by $x\top$, is a d-dimensional ***row vector*** where the jth element of $x\top$ is the jth element of $x$

### Example
If:
$$x=\begin{pmatrix}1 \\ i \\ e^{2i} \end{pmatrix}\in\mathbb{C}^3$$
Then:
$$x\top=\begin{pmatrix}1 & -i & e^{-2i} \end{pmatrix}$$

## Direct Sums
$\begin{pmatrix}x_1\\p_1\end{pmatrix}\oplus\begin{pmatrix}x_2\\p_2\end{pmatrix}\oplus\begin{pmatrix}x_3\\p_3\end{pmatrix}=\begin{pmatrix}x_1\\p_1\\x_2\\p_2\\x_3\\p_3\end{pmatrix}$
## Tensor Product
See [[Tensor Products]]
Note that the tensor product behaves *slightly* differently with vector spaces as opposed to [[Matrices]].

# Verbier Curling
Need minimum of 3 dice per person plus one for cochonet
Min 2 ppl
* Start by rolling cochonet
	* cochonet defines winning sips given
* Each player rolls until they have a unique number
	* this is their attack power
* Player who rolled cochonet has first play. Petanque rules apply
* If you touch another players die, they drink the number of sips on your attack value
* If during your throw you knock another players die off you get to play with that die
	* they si- the number on the floor
* If your die falls off without touching anything, you drink it all
	* if it hits something drink the number on the die
* The number of dice from the winner closest to cochonet decides multiplier for everyone else. Everyone but winner drinks cochonet value x multiplier
* If you knock the cochonet, everyone drinks it’s value
* If you knock the cochonet off, you win and everyone else drinks its value x your attack power
* When you finish a drink it goes on the table on your half
	* must be fist distance from other drinks
* Alternate throwing cochonet
* Player who rolls cochonet picks last but shoots first
* Winner throws cochonet next round. Losers may trade off iff queue

<!-- {BearID:7E2400FD-6AB8-43EC-832B-D02E0E8FA71C-964-000001D62F9EAF77} -->
[[053 Computer Science TOC]]
# Virtual Env
#outputs/code

## Creating a new virtualenv in a python project directory
`python3 -m venv env`
	* This creates a new directory `env` that contains env info.
	* Must be activated before use.

## Activate a virtual env
`source env/bin/activate`
	* This now uses the local python env installation instead of the main interpreter

## Deactivate a virtual env
`deactivate`

<!-- {BearID:5124F60B-4F17-49DE-89D3-D8356932C392-87251-00048AED97C8B850} -->
[[054 Biology MOC]] | [[054 Neuroscience MOC]]
# Visual Cortex
#concepts/biology 

The part of the nervous system that processes and interprets visual inputs. [[Neural Networks]] were designed to mimic this system's function (back when we thought human intelligence was entirely based on it), which may help explain why NN's are so successful at parsing complex visual inputs. I should study the way this system works. I remember a good amount from school, but not much about the brain itself besides VERY rudimentary stuff.


 
## Further Reading
- Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. (2016). Early visual concept learning with unsupervised deep learning. arXiv, arXiv:160605579.
	- [[Unsupervised Learning]][[Cognitive Biases TOC]]
# WYSIATI Bias
#concepts/cognitive_biases 
Source: [[Kahneman - Thinking Fast and Slow]]

Caused in part by the [[Energy Saving Brain]]
This is a great bias I got from [[Daniel Kahneman]]. It basically states that people tend to assume they have a complete picture of existing information when making decisions, and generally ignore clearly necessary questions like "What don't I know?" This is pretty reminiscent of [[Donald Rumsfeld]]'s idea of [[Known Unknowns]].#concepts #🌱
[[052 Physics TOC]]
# Waves
#concepts 
Waves transmit energy. Then can be either transverse, or longitudinal.
[[Quantum Mechanics TOC]] Depends mostly on the idea that all particles are waves. See [[Light]] for more

[[Doppler Shift]]
[[Huygens Principle]]
[[Diffraction]]
[[Refraction]]

`v=L/T` - speed eqn
* Wavelength L
* Velocity v
* Time T (period)

`v=fL` - Speed to Wavelength+frequency eqn
* Wavelength L
* Speed v
* Frequency f


Waves tend to change their direction by bending their motion toward the side that has a slower wave velocity.

Material density increases wave velocity since more molecules can hit each other. Shorter local motion needed to transmit energy.
Temperature does the same, hotter temperature = more molecule movement.

## Sound Waves
Pressure conducted through air, liquid or solid.
Temperature inversion at different times of day:
![[Pasted image 20210407113315.png]]
![[Pasted image 20210407113327.png]]
Explains why you can hear more in the evening!

Local motion - where individual molecules are moving (ie up down/ transverse). They don't move all the way!

Restoring force - makes longitudinal waves "bounce back"

Note that these properties can lead to the creation of sound channels, where sound stays at a certain altitude/ depth because that area is slower than those above and below. This exists both up in the atmosphere and down in the ocean. See [[Huygens Principle]]

## Properties
* Direction
* Amplitude
* Period
* Frequency (1/period)



## Shallow Waves
Velocity is dependent on water depth


Sound channel! V cool ocean effect at 1km. Happens bc of fighting temperature and density properties.

## Seismological Waves
* Primary (P) waves - 6km/s
Fastest, compression/ longitudinal

* Secondary (S) waves - 3.5km/s
Slower, transverse, shear

* Love (L) waves

[[Seedling Notes]]
#🌱 #concepts # Week Priorities
[[095 Journals MOC]]
+ Reach out to JAR
+ Follow up with Robert Rubin

<!-- {BearID:0C93326E-B455-4CC0-A190-7F8972AF5B65-59135-0000F80786C19F37} -->
[[072 MS TOC]]
# What are you doing on big data with jedd finn?

<!-- {BearID:6133D9B5-3D59-4935-9635-5EE8219F4758-7658-000006D56E3D44F8} -->
[[095 Journals MOC]]
# What we are
#outputs/journal 

We are quantum simulations. The glue in English sucks. The language that is. Things to link ideas. That’s where all the problems come from. 

Our reality is a matrix and we individually perceive the outcome. What I mean to say, and I won’t subtract from now, is that our reality and existence is really just a big matrix. Our brain builds reality as a representation of the signals it receives to make sense of them. The signals don’t make sense. We are actually a quantum circuit in another dimensions computation. Reality is really just a construct of our minds to make sense of it. We are machines that exist to minimize stimuli idk bro fuck lol

Cloud existence be recursively self evident? 

Life IS actually a simulation. There is really no way to prove that it isn’t? 
But in the sense that our life is the outcome of a 

We exist as quantum circuits. There’s probably a way to abstr

Time is a variable - so the whole thing is known ahead of time

Maybe 

The whole universe is a quantum circuit. Maybe the circuit was to find the solution to reality/ existence, and thus the process became self referential. We defined ourselves into existence bitch 
We perceive the passage of time as what actually constitutes the quantum calculation 

I feel like I’ve seen the matrix pulled back

We are the algorithm. Reality is the solution 

Also the concept of eroticism entirely. 

Also trees. As an abstraction and in nature. They raise scary questions

It took a universes worth of randomness to get us where we are. All that uncertainty played out at once.

<!-- {BearID:C286AF38-3D37-4663-A9DD-E92B706FB73B-8501-000004D393BE680D} -->
# Why ER?
[[MSWM Capital Markets Goals]]

Emphasize writing experience
- paid writing tutor

Rise cap?!
- conducting broad due diligence on a range of exciting industries
	- Recent acquisitions:
		- MA
		- TERP
		- MSFT
		- BA
		- 


1. Enjoy looking at value of companies
2. Broad - see the whole economy
3. Research is interesting

1. Valuing companies is really cool
2. Love writing
3. Understand the US economy and what drives the world



PYPL
1. Enables internet
[[072 MS TOC]]
# Why IBD?
- Goal-oriented
	- Break things down into step-by-step processes
- Work well in teams/ as part of a group — like to lead and have ownership over projects.
	- Have an intuitive sense for how to break down workloads
- Good writer and good at putting out quality material under pressure

# 3 Adjectives:
- Curious 
	- Breadth of knowledge
	- interested in the idea of process - applicable in lots of situations
	- Focus on acquiring skills
- Driven
	- Get-things-done mentality
	- Working backwards
- Entrepreneurial
	- Seeing the bigger picture
	- How to link ideas back to the concept of business
	- Creative thinking/ outside the box - love finding solutions


# Recent transaction


Solium
Why?
- Adding value to core services without detracting value prop
- Long term planning of how customers will be acquired
- Servicing new clients

<!-- {BearID:C14AB710-501D-45AC-8E55-BEBAED3BEA0D-26685-0000521C0FABA635} -->
[[Machine Learning TOC]] | [[Linguistics MOC]] | [[Natural Language Processing TOC]] | [[055 Coding TOC]]
# Word Tokenization
#outputs/code #concepts/cs/ml/nlp #concepts/linguistics 

## Helpful Unix Commands
Break up the words in a file into newlines, so every piece of punctuation and space is converted into a return char. This is then sorted and parsed for uniques. From here, the list is resorted by the number of occurrences. This is so cool! And simple! This gives us all of the tokens, or [[Wordform]] instances in the text, sorted by number of occurrences.
```
tr -sc 'A-Za-z' '\n' < shakes.txt | sort | uniq -c | sort -n -r | le
ss
```

The next step after tokenization is typically [[Token Normalization]].[[Linguistics MOC]]
# Words
#concepts/linguistics 

Words are the fundamental building blocks of [[Linguistics MOC|Language]], associating [[Phonology|Sounds]] with [[Semantics|Meaning]].

## Definitions
- [[Lemma]]: Same step, part of speech, in a rough word sense, ie `Cat`, `Cats` are same lemma.
- [[Wordform]]: The full inflected surface form, `cat` and `cats` different wordforms.

- Type: An element of the vocabulary
- Token: An instance of that type in running text

Generally, for [[Natural Language Processing TOC|NLP]]:
- N = number of tokens
- V = vocabulary = set of types
	- |V| for size of vocab **


Lots of our thinking on this topic come from [[Ferdinand de Saussure]].

High school graduates typically learn about 60,000 words by the time of their graduation.[[005 Active MOC]] | [[001 Meta MOC]] | [[010 Mind MOC]]
# Workflow
This ended up being my [[003 Process MOC]]

## Knowledge Seeking

## Knowledge Consumption

## Knowledge Digestion/ Discovery

## Knowledge Creation# Workouts
[[020 Body MOC]]

I’m gonna use this page to help plan out some cool workouts I can do. I’d like to develop a nice system so I can still get 6 days a week in, even if I don’t have a gym. I also need to track PRs and progress if I’m gonna improve, so I’ll use this as a homepage for that. 


## Modded RR
#### Warmup
* Spin roundies
* Squat jumps x 20
* Push ups x 20
* Run OTS 

#### SS1 - Legs
/Repeat twice/
* Shrimpies x8
* One legger humps x8 each
* Squat jumps x10

#### SS2 - Push
/Repeat thrice/
* Dilapidating dynamic push ups x20
	* Start w/ clappies, go into reggies
* Tri dips - w/ backpack! x15
* Pike push ups slow x8

#### SS3 - Pull
/Repeat thrice/
* Table pull ups x20
* Towel curls

#### Cool down

<!-- {BearID:6563FDC2-A7D7-49A3-B17D-EA775AC620C8-2897-0000190AF17D9C05} -->
# Wunnava meeting
<!-- #school/Economics/ec411 -->

[[ECON411 Final Paper]]
Define model clearly! Explain /why/ my model is as it is! Use the literature for this!

Start simple….

<!-- {BearID:BBA34A66-CF33-4309-A3CF-257A90AD0389-16983-0001233BAE0D66C4} -->
# XCAP 2020 Annual Report
#### HEAD
Plan for review of annual performance, main strategies employed and their performance

#### REF
[[071 XCAP MOC]]
<!-- #finance/xcap  -->


# Final Stretch
* 

# Strategies
* How to present strategies?


* Performance pages
* Outline:
* Start with an overall letter
* Meet the team section
* Performance Section (nav, position, macros)
* one page for NAV through out the year (maybe table for month by month vs. s&p, russell, nasdaq. Perhaps show cash and nav as a percentage of portfolio for every month. Discuss inflows. Maybe amount of positions for every day of the year and deployed cash. (Mike) 
* one page for individual company analysis. Include general evaluation of each company, perhaps include total return, final allocation, add length of holding, turnover, etc. Include percentages, easily read statistics in general. Maybe indicate what was sold vs. what we still have. Save majority of graphs for sector breakdown in 21 strategy. (Joe)
* one page of sector breakdown, comparing sector performance. be generous with graphs this could lead into our strategy. Similar to Mike’s email, recounting sector by sector. Maybe include some of the graphs we used in Q2 report. Eg. start vs. end of year sectors we are invested in. (Carl) 
- Strategy section
	* one strategy per page, be generous with facts/graphs
	* prioritize using percentages rather than net dollar terms
- at the end of the report include a page with more accounting facts, ala beginning vs. end of quarter, more net results vibes


- Long cannabis (TCNNF CURLF) [[XCAP Cannabis Strategy Review]]
- Long cyclicals/ consumer - during deep COVID recession (NMIH VRRM JPM MA PYPL) [[XCAP Cyclical Strategy Review]]
- Big Tech [[XCAP Big Tech Strategy Review]]
- Short Strategies (SH, options, VXX)  [[XCAP Short Strategy Review]]
- Pandemic “scrap hunting” - me carl [[XCAP Scrap Hunting Strategy Review]]
- COVID/ WFH winners (ATVI MTCH DIS) [[XCAP COVID Winner Strategy Review]]
- Chinese hedge [[XCAP China Hedge Strategy Review]]	
- XCAP.QUANT [[XCAP Quant Update]]

# Performance
- NAV
-

<!-- {BearID:B1A7C0D5-69EE-4F23-8E90-A9C74D37B248-396-00002838992BF3E3} -->
# XCAP 2020 EOY Letter
#projects/old/xcap #outputs/letter
# XCAP 3Q20 CURLF
[[CURLF]] [[XCAP Current Positions]]

<!-- #finance/xcap/3q20 -->
*Y we like Cura-dawg:*
* *Positioning goals:* Intend to increase weighting as election nears, gradually seek to increase weighting as cannabis fundamentals generally improve. LT aim: ~20-25%.
1. Good fundamentals, most aggressive growth
	1. Almost at a stage of profitability while growing 100%+ annually
	2. National footprint - most ready to dominate nationally
2. Big beneficiary of dem victory b/c of exposure to recreational
3. Strong leadership, great dealmaking

We now increasingly see Curaleaf as one of the best positioned large-scale US MSOs due to their national scale and top-class leadership. Throughout this quarter we have significantly increased our allocation to Curaleaf from *7.6% to 14.9%*, which includes a *19.2%* increase in the value of our existing position. 

Moving forward, we expect to see CURLF be a big beneficiary of a democratic victory, given the fact they are the current leading US MSO in the recreational space and a democratic victory is likely to come with sweeping pro-recreational legislative changes. This partially explains some of the excessive volumes the stock has experienced throughout the quarter, likely in conjunction with a number of weaker investors exiting at a price 3x above that in March. 

Coronavirus continues to be a major tailwind for CURLF’s business nationally. While we still do not expect to see current levels of customer acquisition to continue if lockdowns ease and life resumes, we still see the ongoing surge in interest as a positive long term for the existing secular trends surrounding cannabis use.

Finally, a big part of our current outlook on CURLF is the strength and vision of the management team, as well as their ongoing ability to make deals at the highest level of the cannabis world today. All of this without sacrificing organic growth means the company is in probably the best position nationally to take advantage of the increasingly positive landscape among US MSOs. Beyond democratic benefits mentioned above, it is now also looking increasingly likely that a win on either side will still result in the passage of banking laws that will given cannabis companies better access to the financial industry. We see CURLF with their position as the leading US dealmaker to be in the best spot to take advantage of this change, regardless of how it comes about.

We have been continuously adding to our existing CURLF position, and intend to do so as long as we can get the stock for a reasonable price. Our long term target allocation is in the 20% range, especially as the election draws closer.

<!-- {BearID:8E2EE24C-2FDB-495E-AEB7-E0D514929E6D-37181-0003D32DC1C267F3} -->
# XCAP 3Q20 Cannabis & the election
<!-- #finance/xcap/3q20 -->
[[071 XCAP MOC]]

The cannabis industry has hit a watershed moment in 2020. With the election outcome of a blue wave increasingly likely, we are predicting a fundamentally different year for companies in this nascent sector. First off, we see concrete regulatory tailwinds from a Democrat-run government. A blue win would allow everything from the minor but significant improvements in capital market and institutional access afforded by the SAFE act, through to more sweeping changes proposed in the MORE act to pass quickly into law. This would have several major effects into 2021 that become clear catalysts for some of our favorite holdings.

First and most importantly, since cannabis is a federally restricted substance, financial institutions and institutional capital generally are barred from investing in or facilitating legitimate cannabis businesses. In practice, this makes it difficult for firms to raise capital (aside from private placements) and operate effectively within the economy. Most businesses are forced to use only cash for all transactions, significantly increasing the risks and cost of doing business. SAFE will address all of this and will for the first time enable cannabis companies to open bank accounts, access capital markets and tap the significant amounts of capital controlled by institutional investors. We see this as one of the biggest catalysts for the industry, since the cannabis industry still on average offers attractive returns and significant growth prospects.

Second, the MORE act is set to decriminalize cannabis federally and bring federal legislation more in line with what states have. Some 37 US states have passed laws easing access to cannabis, and MORE should help connect currently disjoint markets and create a more cohesive national opportunity. This is a major catalyst for the largest cannabis companies, since they will be able to use their scale and existing executional expertise to rapidly consolidate nationally and build true national brands. This will likely be the key to unlocking valuations in excess of ~$10b. Companies that continue to build scale and expand (in a fiscally responsible way) will be the ones to reap the biggest rewards.

Finally, regardless of the political party of the upcoming administration and house we continue to see improvement and solidification in the secular story supporting the industry at large. The COVID pandemic accelerated customer acquisition by almost half a year for some companies and has set the stage for continuing mainstream adoption and normalization for the industry broadly.

<!-- {BearID:C3DAD76E-6F06-41E8-BB1B-DCB3B9728B22-37181-0003EC04C2FF2325} -->
# XCAP 3Q20 Macro Note
<!-- #finance/xcap/3q20 -->
[[071 XCAP MOC]]

	Our Macro thesis has changed gradually but consequentially throughout this quarter as COVID uncertainty has gradually become more clarified. Our main focus throughout the quarter has been to implement a barbell strategy focusing on the two extremes of work from home (WFH) winners and cyclical stocks. To do this we have added several high-quality WFH winners to our portfolio such as ATVI and MTCH, which we believe both still have more to gain from the ongoing pandemic. While doing this, we have continued to gradually increase our allocation to cyclical names like NMIH and VRRM, which we believe are beginning to have more upside as the economy adapts and recovers. We see cyclical companies as being likelier to benefit more slowly but significantly from the COVID recovery, since many have yet to make full valuation recoveries.

	Next, we have been becoming increasingly concerned with rising valuations in equities broadly. While we have great confidence in the abilities of our chosen companies to keep growing through the pandemic and beyond, we do see risks of overvaluation in equity markets broadly that have an increasing likelihood of affecting our holdings as well as the rest of the market. To combat this we have occasionally begun to hedge this risk by buying protection from broader market downturns, while maintaining large concentrated positions in our chosen equities. While we are generally bullish on the economy and equities long term, we think this strategy will provide us with the most risk adjusted returns, in effect buying us the position that our chosen companies will outperform the market broadly. We don’t have a perfect way to do this, and have had mixed results from our attempts during the quarter. We intend to iterate on and continue to implement this strategy through the rest of this year.

	Moving into the fourth quarter of this year and on into 2021, our current view is still cautious optimism. We intend to maintain our overweight position on cannabis stocks as well as our barbell strategy of cyclical and WFH winners. Government support has maintained our bullish thesis for the economy overall and is providing the impetus for us to remain fully invested despite the significant run up in asset prices through the year so far. We will keep our eye on several key indicators as things continue: rising long-term bond yields, drops in government stimulus and the health of the housing market. We don’t see day-to-day case counts as being a significant factor besides potentially setting the market’s short-term mindframe.

<!-- {BearID:1829A460-3C00-4107-B2FD-A6D1D8DED48D-37181-0003D32EB7A5F4C5} -->
# XCAP 3Q20 Meeting
<!-- #finance/xcap/meetings -->
[[071 XCAP MOC]]

## Agenda
- Discuss quarterly results
- Active trading evaluation
- Positioning review
- Ownership calculation methodology discussion
- Current + past reports

<!-- {BearID:1420DFD9-AE0D-40D3-891E-52CD3A582BBE-1228-0000B87E865E21C5} -->
# XCAP 3Q20 NMIH
[[NMIH]]
[[071 XCAP MOC]]

Current price: $23.52
Current mkt cap: $1.995bn
end price: $17.91
end mkt cap: 1.519
Top price: $35.57
mkt cap: $3.017bn
Bottom price: $9.58
mkt cap: $.813bn

*Y we likey:*
*Risk-based pricing?*
* summarize moves this quarter, performance
* Where do we want to be?
* Fundamental thesis strong - solid small cap, cyclical company
	* Most exposed to stimulus/ consumer recovery
	* Making big strides lately
* Improving macro data supports the thesis

NMI is an incredible little company that has been at the center of our COVID strategy for some time. We started the quarter with an allocation of 13.4% to NMI, saw our position grow by 10.7% and deployed a significant amount of new capital raised (representing a more than doubling of our AUM) to end the quarter with a weighting of 17.8%. We acknowledge that this is a much more aggressive allocation than most would opt for, but we stand beside NMI as our highest-conviction macro and fundamental play at the moment. We expect to slightly trim our position going into the election, but beside that to hold our position well into 2021. 

Fundamentally, we believe the business has been undervalued throughout the whole covid pandemic as a result of the gloom cast over the broader real estate sector. We see several key factors that make NMI an especially strong company and give us confidence holding it through the covid recession:
	* Fiscal discipline, lots of cash raised in March/ April that hasn’t been needed yet to cover additional loan losses. Significant increases in housing prices since then have greatly decreased the risk of defaults coming out of covid.
	* Ability to grow the top line without growing costs, really good pre-covid track record of increasing issuance each quarter without compromising on loan quality (40-50% annual growth).
	* Able to scale while hiring almost no new staff.
	* Current forward multiple of 8x shows how cheap the stock is, especially given ability to grow responsibly

From a macro perspective, we believe NMI is an excellent, pro-cyclical company that is extremely levered to a combination of housing prices and general consumer health. It is our view that the US government is going to continue to support consumers as the pandemic goes on through continued fiscal and monetary stimulus. We have already seen the impact of unprecedented government monetary intervention in record-low mortgage rates that have helped fuel the surge in demand for new mortgages, protecting the core business. We anticipate this to continue beyond Election Day.

The election does pose risks for NMI. Given their exposure to government intervention and the market’s recent propensity to move on hopes and fears, we definitely see risks of heightened volatility as we near the election for NMI. Uncertainty surrounding the outcome of the election is a particular risk, as this could significantly draw out the timeline for future stimulus.

<!-- {BearID:4A204AD4-1721-4A5F-AEEC-A2F11316AD56-37181-0003D32E3F6C9A38} -->
# XCAP 3Q20 TCNNF
<!-- #finance/xcap/3q20 -->

[[TCNNF]]

* *Positioning:* Getting ready to decrease allocation as election nears, derisk and take profits. LT aim ~15-20%


This quarter marked some of the most intense investor activity in TCNNF to date, which in our view reflects the tailwinds the cannabis industry broadly is expected to face coming out of the election season. Our existing position in TCNNF grew by *47%* and we deployed a further 9% of our total assets into the stock during the quarter, leading to a final weighting of *18.1%* in TCNNF at the end of the quarter.

Our current bullish thesis on TCNNF is based on several core factors we have identified in the company that we believe gives them an especially good chance to become a dominant player in what will likely become a $50+ billion industry within the decade. These core fundamental aspects of the bull case we see as likely are now being supported by significant tailwinds from a continually improving public and legislative outlook, on all sides of the political spectrum. Growing state deficits in conjunction with consistent approval in the 70%+ range nationwide mean we are now much more likely to see significant, progressive legislation regardless of who wins the election for the White House.

With that said, we do believe that TCNNF’s model is more likely to benefit under a Trump White House, whereas a model like that of CURLF is more likely to benefit under a democratic victory (as a result of their broader national penetration and exposure to recreational as well as medical cannabis which is TCNNF’s territory. We see a democratic victory as being more beneficial for the recreational space due to a higher likelihood of broader recreational legislation) We have grown our portfolio’s total allocation to cannabis by mostly increasing our allocation to CURLF to give us better optionality regardless of the outcome of the election, and help hedge against some of the volatility likely to occur.

The political aspect of the thesis has definitely dominated conversations around TCNNF this quarter, and we believe for good reason. TCNNF is

One core fundamental metric we emphasize in our analysis is monthly customer additions, and we have noticed a marked inflection in customer additions during this quarter, which we b


*Y tru tru cool:*
*Positioning:* from 17.7% to 18.1%, position grew by 47%, as diluted by significant inflows.
1. Rock solid fundamentals, fiscal discipline, thesis unchanged from before
2. Major covid use inflection - big uptick in monthly customer additions. May not continue at this pace once covid lockdowns lighten, but the customer additions are likely stickier than for other firms given TCNNF’s high brand value
3. Election impacts growing, starting to see increasing interest in stock as a result. Short term boost. Will likely use this as an opportunity to bring the position down as a proportion of the portfolio and move proportionally to CURLF, which is a good democratic hedge to TCNNF.

Trulieve has continued to be one of the best-performing brand-focused MSOs in the US throughout the third quarter and we maintain our fundamental bullish thesis on the company. The fund’s allocation to Trulieve started at *17.7%* and ended at *18.1%*, after growth in our existing position of *47%*. We expect Trulieve to be an ultra-long term holding for the fund, pending developments in the industry - we want to always be invested in the best players in the space, given the likely growth over the next decade.

Our fundamental position on Trulieve based on fiscal discipline and successful organic growth remains unchanged. Monthly *customer additions have continued inflecting upwards*, growing 42%, as compared with 23% the quarter prior. We see this as ongoing proof of the strong covid thesis driving the cannabis industry at large and demonstrating Trulieve’s ability to leverage it’s strong brand to come out on top. We acknowledge that customer growth probably won’t continue at the same pace as lockdowns lift, but we expect Trulieve to be *better able to retain the new customers than other companies due to their more concentrated brand presence in Florida.* Trulieve’s long term strategy is different from most competitors of the same scale (such as CURLF),

*The election*  remains a source of uncertainty for Trulieve. Given their specialization in medical cannabis, recreational legalization that may come with a democratic victory poses a significant challenge to Trulieve’s growth prospects beyond the state, without a significant shift in the meaning of the brand. Given this risk, we intend to gradually decrease the relative allocation of our portfolio in Trulieve as compared with Curaleaf, which is a definite democratic beneficiary. Given TCNNF’s exposure to republican victory, we can see our position as somewhat of a hedge for the currently likelier outcome of a Biden victory - we can assume that the likeliest outcome will be priced in, so a shock would allow us to capitalize.

That said, we still see a number of big structural drivers for medical as well as recreational cannabis regardless of the outcome of the election, namely in the passage of banking acts like SAFE that would give cannabis companies access to the US financial system. Public opinion on both sides of the aisle suggests that this legislation has a decent likelihood of being passed even under a Republican victory, decreasing the medium to long-term risk of this company.

<!-- {BearID:DB5DEADB-CF86-4BDE-9FD5-34CDCA3020F1-37181-0003D2666A80FE62} -->
[[071 XCAP MOC]]
# XCAP Branding
<!-- #finance/xcap/branding -->

Colors: 
# 16BADE
# ajklf;ailkyhg
## aasdfalk;j;l
### aasdh
#### a
##### a
###### a
#######a
* Core color: \#89B2D2
* Light Accent: \#a9d3e7
* Colorful Accent: \#6db5d7
* Deep Accent: \#3595c2
* Alt 1: \#d76d80 - **candy pink**
* Alt 2: \#d76db5 - **sky magenta**
* Alt 3: \#d7c46d - **vegas gold**
* Alt 4: \#d78f6d - **copper crayola**
* Alt 5: \#6d80d7 - **glaucous**
* Alt 6: \#6dd7c4 - **turqoise**


Fonts: PT Sans
# XCAP COVID Winner Strategy Review
#### HEAD
Annal review of our COVID winners strategy
	* ATVI
	* MTCH
	* DIS
	* MSFT

#### REF
[[071 XCAP MOC]] -> [[XCAP 2020 Annual Report]]

# Plan
	* Summarize overall environment and TAM/ opportunity
	* Summarize what we did
	* Summarize why we like the names we like
		* And why we chose the mix we chose!~
	* Brief review of performance, plan for future

# Letter

<!-- {BearID:EEECAAEB-B419-4BFF-A059-9423BBE426F8-396-00002A7688E1D1C2} -->
# XCAP Cannabis Strategy Review
#### HEAD
Annal review of our cannabis strategy
	* TCNNF
	* CURLF

#### REF
[[071 XCAP MOC]] -> [[XCAP 2020 Annual Report]]

# Plan
	* Summarize overall environment and TAM/ opportunity
	* Summarize what we did
	* Summarize why we like CURLF, TCNNF
	* Brief review of performance, plan for future

# Content
Our cannabis strategy grew out of early research on the large and rapidly expanding market opportunity facing legitimate cannabis businesses. Estimates for the total number of regular and occasional users range from 25 to more than 35 million people in the US alone. Furthermore, more than two thirds of the country’s population now support *recreational* legalization. In our initial predictions back in 2019, we estimated the timeline for federal legalization/ decriminalization to occur after the 2020 US election. As it has now happened, Biden’s Presidential victory has oppened the door for this exact event, unlocking our Bull case and sales of just over $70bn in 2025 (contrast this with $115bn for the tobacco industry and  $252bn for the alcohol industry). This rapid opportunity creation creates the foundation for our bullish strategy.

Our investment thesis is built on three core pillars:
	1. *Financially sustainable growth*
		While there are tens of companies growing at 30%+ per quarter, few of them are doing so in ways that set them up for sustainable operations in the future. The most prime example of what not to do is offered by the largest Canadian players - they raised too much capital on hype, were unable to effectively deploy it and witnessed extreme losses over the last couple years. For this reason we prefer companies that emphasize cost control and financial restraint in pursuing strategic acquisitions. 
	2. *Market leadership*
		We want to own the companies that are already at the cutting edge of product development, brand building and national CPG scale. We also require a concrete pathway to continued expansion and development with a focus on management vision. The biggest winners are going to be the ones that are able to grow into scale rapidly and efficiently.
	3. *Strength of management teams*
		Due to industry stigma, cannabis companies for a long time struggled with getting access to great leaders and managers. Now the situation is changing, and we have begun to put a large emphasis on this metric. Key areas we look for are 1. Management vision for future growth 2. Dealmaking ability/ strategic planning 3. Ability to navigate regulatory complexity.

The two main players we have identified through these investment lenses are CURLF (Curaleaf) and TCNNF (Trulieve). Two of the largest operators in the US, they have both proven their abilities to keep losses to a minimum while growing at a clip often faster than their competitors. Curaleaf dominates the national CPG space, while Trulieve has built the strongest brand in the areas in which they operate. Both companies have excellent leadership between Boris Jordan and Joe Bayern at CURLF and Kim Rivers at TCNNF, in both cases exhibiting an excellent grasp of the opportunities and risks lying ahead.

*2020 Investment Strategy*
Although we made our first cannabis purchases at the end of 2019, we only began to truly scale our strategy in the beginning of 2020. We continually increased our allocation to this strategy, with a peak allocation of nearly 40% of the fund’s overall value. Our average return on cannabis investments was about 140% throughout the year, contributing significantly to our overall results. The pandemic especially created opportunities to buy these companies at huge discounts to true value. 

After the 2Q20, we began to use cannabis as a way to play the then increasingly likely Biden victory. We initially allocated substantially more to TCNNF than CURLF thanks to their ongoing profitability, but realized that a Blue victory in congress or for the presidency would mean a drastic improvement in the legislative and regulatory situation, so put additional capital into CURLF, which we saw as a bigger winner from recreational legalization (VS TCNNF which stood to gain from continued medical adoption). 

All of what we anticipated and a whole lot more has come to pass. Federal legalization appears to be around the corner, and the cannabis market opportunity is no longer severely discounted in markets. We will likely begin to realize some gains if valuations remain extended, but will maintain an aggressive long position overall.

<!-- {BearID:67939568-21AB-4FCF-910C-0AC6726B719B-396-00002A17690B8997} -->
# XCAP China Hedge Strategy Review
#### HEAD
Thinking of buying some china stocks to further hedge our large US exposure atm

#### REF
[[071 XCAP MOC]] -> [[XCAP 2020 Annual Report]]
[[XCAP China Hedge]]
<!-- #finance/xcap/strategy -->
<!-- #finance/xcap/Investments -->

# Plan
	* Reasoning behind making a broad move
	* ETF selection
	* Review of performance, comparison to thesis
	* Plan with hedge moving forward

<!-- {BearID:BA086069-9DD8-4461-83DF-C28F657C20AD-396-00002AC816E09163} -->
# XCAP China Hedge
#### HEAD
Thinking of buying some china stocks to further hedge our large US exposure atm

#### REF
[[071 XCAP MOC]] -> [[XCAP 2020 Annual Report]]
<!-- #finance/xcap/strategy -->
<!-- #finance/xcap/Investments -->


# Plan
* 5% MCHI - nice whole china etf, v liquid, tracks MSCI China  (huge baba and tcehy exposure)
* 7% CNYA - Lots of domestic equities, 478 holdings
* 5% CQQQ - extra tech exposure, tracks FTSE China A Tech index


# Actual Trades
* CNYA - 4%
* CQQQ - 2%
* MCHI - 1%


# Strategy Note
[[XCAP China Hedge Strategy Review]]

<!-- {BearID:6ACE1A88-CC19-40D9-A405-DCD70D3B7A00-87251-00055C047DAED9C1} -->
[[071 XCAP MOC]]
# XCAP Current Positions
#projects/old/xcap


# Options Positions:
[[Bearish JKS Spread]]

# Equity Positions:
* *TCNNF:* [[XCAP 3Q20 TCNNF]]
* *CURLF:* [[XCAP 3Q20 CURLF]]
* *NMIH:* [[XCAP 3Q20 NMIH]]

<!-- {BearID:A9D0835C-B33F-42C2-8E7C-31193B06803B-37181-0003E3B431127925} -->
# XCAP Cyclical Strategy Review
#### HEAD
Annal review of our cyclical, pro-consumer COVID strategy

#### REF
[[071 XCAP MOC]] -> [[XCAP 2020 Annual Report]]

# Plan
	* Summarize overall environment and TAM/ opportunity
	* Summarize what we did
	* Summarize why we like NMIH, VRRM, PYPL, MA, JPM
		* And how the mix made sense 
	* Brief review of performance, plan for future with any ongoing positions

# Content
After the market carnage March 2020 had mostly subsided, we began to see signs of the formation of a new bull market borne out by fiscal and monetary governmental support. Key indicators we watched at the time were overall levels of capital deployed by governments, housing prices and consumer data. Although there were critical weaknesses in areas such as jobs, the net outlook appeared more bullish than the market was reflecting at the time. If the strong recovery scenario did come to pass, we reasoned the secular bull successes of early-mid 2019 would underperform new cyclical leadership that could build operating leverage and benefit from the forming bull market. 

To execute this strategy we chose a “barbell” approach, investing in a combination of small-cap financial companies and large money center institutions. The idea behind this split was to profit from both the strong outlook for small businesses as well as taking advantage of the increased level of savings among consumers. The cornerstones of this strategy were: 
* NMIH, which is a small, competitive mortgage insurer heavily levered to the performance of the housing market.
* VRRM, which is a small tech-based urban city monitoring and payment solutions provider levered to consumer activity in cities.
* PYPL and MA, both exposed to a rebound in consumer purchases as well as the COVID tailwind of increasing digitization for payments.
* JPM, occupying the far end of the barbell as a large money center. Especially effective if government support is maintained into 2021 and savings continue to increase

We executed this strategy in bursts throughout the year. In the early days of the pandemic we prioritized NMIH and VRRM, focusing on the smaller size and great operating leverage. As the rebound became more certain, we increased our emphasis on the far end of the barbell with the companies more exposed to consumer spending and saving.

Overall the strategy has proven to be highly effective throughout this year. We’ve witnessed arguably an even stronger recovery in the short term than anticipated, paired with better prospects on the crucial 1-3 year horizon. This has been reflected in markets, making the trade riskier at the end of 2020 than it was when we started. However, and especially in the context of even higher valuations in popular growth names, we believe there is a larger degree of upside remaining in this strategy than in secular names in tech and elsewhere. We intend to keep this strategy active into 2021, focusing mostly on the outlook at least a year from now.

<!-- {BearID:A9CB09BA-C09C-4D8E-A70F-B924F895D9A8-396-00002A5EE5BF2C0A} -->
[[071 XCAP MOC]]
# XCAP Meeting Minutes

[[04-24 Meeting Minutes]]
[[Unknown Meeting Minutes]]
[[Meeting plan 26-02]]
[[XCAP Meeting Minutes]]
[[05-01 Meeting Minutes]]
[[XCAP 3Q20 Meeting]]
[[Shareholder Meeting]]# XCAP Quant Update
#### HEAD
The 2020 Annual report update on the development of our quantitative trading strategy

#### REF
[[000 Life MOC]]

#### PLAN
	- Outline gist of why we want to employ algo strategies
	- Three core components
		- Asset allocation (Portfolio)
		- Active Trading (Subsystem)
		- CLI (Dataman/ stocks)

# Content
Among many other firsts, 2020 marked the beginning of our effort to create an automated investment system. While reflecting on our fund’s performance in the spring, we realized that oftentimes in the past our strategy was sound, but our execution was weak, leading to smaller returns than we should be able to achieve. This led us to begin exploring ways to systematize our strategy, so portions of it could be implemented dynamically and automatically, depending on incoming data. All of this has been an experiment designed to broaden our perspective on markets and investing.

We are currently working on four core domains as part of this project:
1. *Asset allocation system* - this system is designed to take in certain screening rules (e.g. companies that consistently grow earnings, companies that consistently beat estimates etc, all companies in a sector) and buy them all. This gives us the benefit of being able to customize very specific macro reactions depending on our outlook for the economy. We can emphasize companies with good operating leverage at times like now during early recoveries, or focus on those that have low leverage when rates are increasing. This component is designed to create a long-term backbone of somewhat stable returns, investing for the long haul.
2. *Active trading system* - this portion of the system is designed to implement specific, rules-based strategies on certain assets in a shorter timeframe. While this system is designed more to test our theories about the market than trade itself wholesale, we do believe our tiny AUM gives us an advantage in trading certain micro cap, illiquid names. In particular we are thinking of a combination of momentum, mean reversion and arbitrage strategies in this domain. 
3. *Command Line Interface* - This component allows quick terminal access to a wealth of stock information. We use it to quickly pull and compare fundamental information as well as visualizing different timelines of technical information. While initially designed to facilitate the asset allocation process, this component ended up developing useful standalone functionality.
4. *Specific Strategy Research* - Finally, we are also doing research on ideas-first strategies we have been thinking of for specific companies. Our most promising candidate is a strategy that ties in highly relevant macro data for a relatively illiquid small cap. Initial research started with running regressions on our datasets and ensuring our indicators had statistical significance. We are now in the phase of working to transform (relevant) macro data into discrete buy/ sell signals to be used by our active trading system.

So far, the project is still in the testing/ development phase. We view this as an exciting way to both assure a new degree of stability into the portfolio through our asset allocation approach, as well as potentially yielding outsize returns on our small level of capital through access to strategies that are inoperable with larger fund sizes. We intend to continue to use our proprietary code to test ideas about market structure and operation, as well as fine-tune our strategies outside of the context of quantitative finance.

<!-- {BearID:E313CAB0-1377-4D80-A29B-165BF1823CC2-2637-00005DD577C8FB5E} -->
# XCAP Scrap Hunting Strategy Review
[[XCAP 2020 Annual Report]]

<!-- {BearID:3BC7DFA7-E9E0-4DD7-886B-62B703A17A33-2637-00002E680B511739} -->
# XCAP Short Strategy Review

<!-- {BearID:49C63867-DFC2-4D4B-A0C4-E8FF78640B72-398-00004EBDB2F49E8D} -->
[[071 XCAP MOC]]
# XCAP Strategies
[[XCAP Watchlist]]

## Latest Strategy Thoughts
- Time to start buying Cannabis if it keeps falling
	- Prioritize CURLF, but may want to stay on the TCNNF train too
	- Review cannabis fundamentals - are we still in the same position?
- WTF are we still holding VXX - it's f%#%$ng short term!!! Really shouldn't be holding
	- If we ever buy it again should set a couple day cap on how long we can hold it
* Strategy where I put 1% into each of the biggest holdings of the most liquid ETFs. I should be long the names with the big money flows - or perhaps consider these as hedges?
* Hedge the tail risk! People underestimate impact of it. Look at Fulgent Genetics, FLGT for an example. Should have bought in jan when a virus was breaking out. Think of how I could apply that to future possible scenarios!! Be ready with responses!

## Watchlist
- [[Aerofarms]] - I like this kind of innovative shit# XCAP Tech Strategy Review
[[XCAP 2020 Annual Report]]

<!-- {BearID:FFDFAEAC-220C-4CE0-9254-990808758753-2637-00002E68D79E0F36} -->
# Rise Watchlist
#projects/old/xcap [[071 XCAP MOC]]

- MELI


## Screener Picks
- JCTCF?	
	- Lumber company
	- Prolly too small, lots of errors in calcing ratios on bloomby
- PKI?
	- Too big maybe, big medical tech & support provider.
	- seems like it got super profitable lately, idk wtf is up. deserves a closer look
- DXCM
	- Huge medical equipment & devices co. Turned profitable last year. Interesting, large tho.
- QDEL
	- 
- IDXX
	- Kinda like this one
	- Vet tech company - for pets and farm animals
	- ~45 billy mkt cap
- SGC
	- 
- [[071 XCAP MOC]]
# XCAP Writing
#projects/old/xcap

This is a place for me to centralize all of my "content" output
[[XCAP 2020 Annual Report]]:
	[[XCAP Cannabis Strategy Review]] 
	[[XCAP 3Q20 Cannabis - the election]]
	[[XCAPITAL FUND UPDATE 06-09]]
	[[XCAP Cyclical Strategy Review]]
	[[XCAP 2020 EOY Letter]]


*Older notes, these need to be relooked at:*
## Capital:
+ Charter investment contracts for people putting in money and for all existing shareholders [[Investment Contract Agreement]]
- Understand tax situation
- Write an investment thesis for all current investments
	- [[Paypal Investment Thesis]]
	- [[Amazon Investment Thesis]]
	- [[Nvidia Investment Thesis]]
	- [[Paypal Investment Thesis]]
- Write an investment post-mortem for accounts for each completed investment, and add to ledger
	- [[Progenics Post-Mortem]]
	- [[Tencent Post-Mortem]]
	- [[Emerging Markets ETF Post-Mortem]]
- Meeting tonight [[Meeting plan 26/02]]
	- Assign initial board seats
	- Assign analyst positions
	- Clear up roles
- Addendums to the investment contract [[
+ Create ledger of all transactions
+ Figure out equity and expected board members

## Corp:
- Marketing team
- Product team
	- Software     
	- Hardware
+ Figure out middchallenge date
- Product spec

Marketing 
What’s missing from this scene? Party sounds in darkness. 

Thats right. Its not lit
*Lights*
[[071 XCAP MOC]]
# XCAP mailing list
#projects/old/xcap

"Van Gansbeke, Frank" <fvangansbek@middlebury.edu>, Ron Basu <ron.basu@morganstanley.com>, Robert Rubin <robert.m.rubin@morganstanley.com>, "Raphael, John" <John.Raphael@morganstanleypwm.com>, Richard Parry <RParry@tjim.com>, "Mastrangelo, Joseph Demetrius (Joe)" <jmastrangelo@middlebury.edu>, "Langaker, Carl Erik" <clangaker@middlebury.edu>, jrmohling@middlebury.edu, "Hinton, Wiatt Fenner" <whinton@middlebury.edu>, Nikosha <nicholascalvey@gmail.com>, Adam Guo <aguo@middlebury.edu>, Tayo Zenger <adezenger@gmail.com>, Hasan Tuhtamishev <hasan.tuhtamishev@yale.edu>, Sean Michael Thompson <smthompson@middlebury.edu>, Sergio Bobadilla <sbobadilla@middlebury.edu>, Julia Calvey <juliacalvey@gmail.com>, Akhil Kopisetti <akoppisetti24@gmail.com>, solalmartincamus@protonmail.com, ckoster@middlebury.edu, samkorman99@gmail.com, daniyal.manekia@gmail.com, aleksey.shevchenko@morganstanley.com, TempMC <TempMC@bvcp.ru>, Molly Triana <mtriana@usc.edu>, Sean Michael Thompson <smthompson@middlebury.edu>, "Tarantino, Thomas Joseph" <ttarantino@middlebury.edu>, Omar Alsaeed <oalsaeed@middlebury.edu>, "de Wit, Loic Jean-Francois" <ldewit@middlebury.edu>, tyou@middlebury.edu, "Langaker, Carl Erik" <clangaker@middlebury.edu>

"Andy Saperstein" <andy.saperstein@morganstanley.com>

* Andy
- Frank 
- Ron 
- Nate Stein
- John Raphael
- Richard Parry

* Charlie Ryan
* 

- Joe
- Carl
- Dan
- Xav
- Sergio
- Josh
- Wiatt
- guo
- Sean T.

- Tayo
- Hasan
- Emi



"Van Gansbeke, Frank" <fvangansbek@middlebury.edu>, Ron Basu <ron.basu@morganstanley.com>, Robert Rubin <robert.m.rubin@morganstanley.com>, "Raphael, John" <John.Raphael@morganstanleypwm.com>, Richard Parry <RParry@tjim.com>, "Mastrangelo, Joseph Demetrius (Joe)" <jmastrangelo@middlebury.edu>, "Langaker, Carl Erik" <clangaker@middlebury.edu>, jrmohling@middlebury.edu, "Hinton, Wiatt Fenner" <whinton@middlebury.edu>, Nikosha <nicholascalvey@gmail.com>, Adam Guo <aguo@middlebury.edu>, Nate Stein <nate.stein@morganstanley.com>, Tayo Zenger <adezenger@gmail.com>, Hasan Tuhtamishev <hasan.tuhtamishev@yale.edu>, Sean Michael Thompson <smthompson@middlebury.edu>, Sergio Bobadilla <sbobadilla@middlebury.edu> Akhil Kopisetti <akoppisetti24@gmail.com>, solalmartincamus@protonmail.com, ckoster@middlebury.edu, samkorman99@gmail.com, 
daniyal.manekia@gmail.com

<!-- {BearID:434F76BB-4796-4FE4-BDBF-43F6592903E4-75570-0004CD379CF5720F} -->
[[071 XCAP MOC]]
# XCAPITAL FUND UPDATE 06/09
#projects/old/xcap 

Dear Friends and Colleagues,

This is the first in a hopefully ongoing series of portfolio updates we will report when we make meaningful changes to the fund. This first report addresses our moves in response to the sharp fall in markets in the latter half of the week. All told, *we deployed an additional 10% of the fund amounting to 30% of the total cash we had to deploy*. While broadly valuations are concerningly high, we see opportunities opening in individual names when markets dislocate. 

We have stuck to our predetermined strategy on when and what to buy when things start falling, which has allowed us to make investment decisions with less emotion. While we never strive to time the market more than we do to choose the right universe of stocks to own, we still acknowledge that times when prices move by upwards of 10% can present outsize opportunity for buying or selling.  We generally do not mechanistically buy the dip on all holdings, but instead focus on a shorter list of names that we want to own more than everything else, and take advantage of cheap opportunities to enter. With that, here are the major moves we made over the past several days:

*New 5% position in ATVI:* Activision Blizzard has been on our watchlist for several weeks because we like the solid combination of exposure to long term gaming trends like esports and mobile gaming, as well as the short term benefits of ongoing lockdowns paired with key upcoming product launches. Seeing the stock trade down below its 2 month low around $79 gave us the opportunity we needed to enter. Coming back to school this fall and witnessing how virtual entertainment will now have to be definitely reinforced our belief in the trend overall. Our current position represents approximately a 5% allocation of the fund overall, which is exactly where we want it to be right now. 

*Increase TCNNF position from 20% to 26%:* We have been very positive on Trulieve in the past as the most responsible of the large US cannabis MSOs, and have seen an incredibly positive inflection in the company’s growth YTD, with customer acquisitions accelerating even as lockdowns have been lifted. The market has begun to acknowledge TCNNF as a leading MSO (As we predicted for well over a year) but we now have a period of price consolidation (down 20%) after a 300% run up in the March - July period. This gives us an opportunity to add to our holdings at a price we still see to be intrinsically discounting the company’s true potential. Talk of a merger, although vague so far, is also both exciting and foreseeable, since M&A is likely to be the easiest strategy for Trulieve to pursue true expansion beyond the FL market. While there are clear risks to having a position take up over a quarter of all our assets, we truly believe that TCNNF is the most promising player in the space, and one of the only ones to have enough composure to actually survive into maturity. 

*Increase VRRM position from 9.2% to 11%:* We continue to like Verra as a cyclical, smaller cap play with significant exposure to urban government spending, an area with obvious benefits at the moment. While classified as an industrial company on the surface, Verra is really a payments platform operator with direct exposure to car travel. While travel has been hit during COVID, we expect to continue to see a solid rebound in car traffic as people resort to more private methods of travel (vs airplanes, public transport). Verra isn’t a name we’re expecting a short-term beat from, but rather a structural beneficiary of the economic environment we see flourishing in the next couple years. 

*Increasing MTCH position from 3% to 5%:* This is a great example of an accurate execution of our follow-on investment strategy. We identified MTCH as a player that benefitted from COVID lockdowns and would likely see a permanent inflection in attitudes relating to virtual romantic and sexual interactions. We see online dating and relationship building as an area with a TAM far bigger than that occupied by all current players, and MTCH is in our opinion the category-killer in the space. Tinder especially has become a cultural phenomenon in and of itself, overtaking the concept of online matchmaking itself. This is a name we expect to hold for an extended period of time. 

*Increasing PYPL position from 5% to 6.2%:* We really like PayPal because of its aggressive exposure to e-commerce and consistent ability to increase network efficiencies. PYPL is one of our oldest investments with a current unrealized gain of over 100% on our original shares, making this follow-on somewhat tricky, however we still believe in the fundamental value of the company and see a great potential runway for the future. P2P (Venmo) bears less weight than in the past in our thesis.

Yours truly,

Michael and the XCAPITAL team




Trades placed:
- *ATVI* 10 shares @ 77.07 = $770.7
	- 0 -> 10
- *TCNNF* - 30 shares @ 19.385 = $581.55
	      - 13 shares @ 19.33 = 251.29
	- 151 -> 194
- *VRRM* 25 shares @ 10.045 = 251.13
	- 143 -> 168
- *MTCH* 2 shares @ 102.885 = 205.77
	- 6 -> 8
- *PYPL* 1 share @ 186.6549
	- 4 -> 5

<!-- {BearID:0EA31A12-5AFE-481D-B1FD-855B2EFB2912-75570-00047D60E81033D3} -->
# Xav notes
[[200 Outputs MOC]]
- Shorten sentences - don’t need to prove your writing ability, want to convey max. I would even use incomplete sentences and bullet points in places to convey info more quickly.
- Think about putting select info/ data in bold or italics to emphasise, will send an example
- Make sure sentences are clear from the beginning - make it conducive for skimming (I was looking at the first sentence where you need to read the whole thing to understand what you’re trying to say. I would say something like “MATFX’s high PE relative to peers of 22.95 vs category and index PE of 4.58 and 8.19 implies ….” Its not a huge change and more of a stylistic thing, and would definitely hurt you in an essay or more formal piece but I think its important to convey key things quickly.
- The latter half of your PE paragraph is a bit redundant, I would hope everyone that reads this know what PE is and how to interpret relative figures
- Look closely at phrases that add language without adding content
- Love the meituan dianping paragraph, perfect use of numbers interspersed in key areas
- I know this is just notes but I’d think about putting a little more emphasis on structure: have paragraph headers and make it clear what each paragraph is discussing so people can pick what to read
- 

^Look at the way this writing reads. More casual and super information-dense. Look at the frequent numbering to make things clear, headers, tickers where possible
- I like BILI, exciting to see what you write about it.
- In terms of structure, I’d have it read like an outline actually - named sections like “Fund overview”, “Summary of Holdings”, and “Peer comparison” to contain relevant data
# Zettelkasten
<!-- #thoughts -->
[[Thoughts on note taking]]
* Started with some thoughts here: [[Daily Log 2020\/10\/14]]
Very interesting system of organization and note taking. It’s almost more about setting up a reference environment where accidental learning and discovery can happen through tight cognitive grouping.

* More on Zetterlkasten and Luhmann [Niklas Luhmann - Wikipedia](https://en.wikipedia.org/wiki/Niklas_Luhmann)

<!-- {BearID:A7A3D31C-3119-4602-AED2-EDA6CE892B92-37181-0002AE97704A2AAF} -->
[[052 Physics TOC]]
# rem
#concepts 
The unit of measurement for [[Radiation]]. Corresponds to each square centimeter receiving 2 billion gamma rays. If each square centimeter of the body receives a dose of 1 rem, we say the person received a whole-body dose of 1 rem.

There are 100 rem in one Sievert

### Radiation Poisoning
| Whole-body dose | Resulting illness                                     |
| --------------- | ----------------------------------------------------- |
| <100 rem        | Nothing                                               |
| 100-200 rem     | slight, short-term, nausea, hair loss                 |
| 300 rem         | LD50, 50% chance of death if untreated within 60 days | 
| >1000 rem       | Incapacitation within hours, survival unlikely        |
|                 |                                                       |
